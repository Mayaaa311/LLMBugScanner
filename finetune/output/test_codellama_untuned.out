---------------------------------------
Begin Slurm Prolog: Nov-11-2024 14:52:27
Job ID:    926285
User ID:   zyahn3
Account:   scs
Job name:  finetuningNxcode
Partition: coc-gpu
---------------------------------------
/home/hice1/zyahn3/.conda/envs/BugScanner
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:09<00:09,  9.79s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:13<00:00,  5.92s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:13<00:00,  6.50s/it]
Traceback (most recent call last):
  File "/storage/ice1/5/9/zyahn3/LLMBugScanner/finetune/test_model.py", line 31, in <module>
    prompt = pipe.tokenizer.apply_chat_template(eval_dataset[rand_idx]["messages"][:3], tokenize=False, add_generation_prompt=True)
  File "/home/hice1/zyahn3/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 1803, in apply_chat_template
    chat_template = self.get_chat_template(chat_template, tools)
  File "/home/hice1/zyahn3/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 1967, in get_chat_template
    raise ValueError(
ValueError: Cannot use chat template functions because tokenizer.chat_template is not set and no template argument was passed! For information about writing templates and setting the tokenizer.chat_template attribute, please see the documentation at https://huggingface.co/docs/transformers/main/en/chat_templating
---------------------------------------
Begin Slurm Epilog: Nov-11-2024 14:52:54
Job ID:        926285
Array Job ID:  _4294967294
User ID:       zyahn3
Account:       scs
Job name:      finetuningNxcode
Resources:     cpu=1,gres/gpu:a100=1,mem=80G,node=1
Rsrc Used:     cput=00:00:28,vmem=0,walltime=00:00:28,mem=948676K,energy_used=0
Partition:     coc-gpu
Nodes:         atl1-1-03-007-35-0
---------------------------------------
