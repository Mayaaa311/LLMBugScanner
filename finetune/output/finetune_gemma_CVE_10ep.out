---------------------------------------
Begin Slurm Prolog: Nov-23-2024 15:24:51
Job ID:    971271
User ID:   zyahn3
Account:   scs
Job name:  finetuningNxcode
Partition: ice-gpu
---------------------------------------
no change     /usr/local/pace-apps/manual/packages/anaconda3/2023.03/condabin/conda
no change     /usr/local/pace-apps/manual/packages/anaconda3/2023.03/bin/conda
no change     /usr/local/pace-apps/manual/packages/anaconda3/2023.03/bin/conda-env
no change     /usr/local/pace-apps/manual/packages/anaconda3/2023.03/bin/activate
no change     /usr/local/pace-apps/manual/packages/anaconda3/2023.03/bin/deactivate
no change     /usr/local/pace-apps/manual/packages/anaconda3/2023.03/etc/profile.d/conda.sh
no change     /usr/local/pace-apps/manual/packages/anaconda3/2023.03/etc/fish/conf.d/conda.fish
no change     /usr/local/pace-apps/manual/packages/anaconda3/2023.03/shell/condabin/Conda.psm1
no change     /usr/local/pace-apps/manual/packages/anaconda3/2023.03/shell/condabin/conda-hook.ps1
no change     /usr/local/pace-apps/manual/packages/anaconda3/2023.03/lib/python3.10/site-packages/xontrib/conda.xsh
no change     /usr/local/pace-apps/manual/packages/anaconda3/2023.03/etc/profile.d/conda.csh
no change     /home/hice1/zyahn3/.bashrc
No action taken.
/home/hice1/zyahn3/.conda/envs/BugScanner
`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.
Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use
`config.hidden_activation` if you want to override this behaviour.
See https://github.com/huggingface/transformers/pull/29402 for more details.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:06,  2.23s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:04<00:04,  2.17s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:06<00:02,  2.13s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:07<00:00,  1.66s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:07<00:00,  1.84s/it]
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
/home/hice1/zyahn3/.local/lib/python3.10/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length, dataset_kwargs. Will not be supported from version '0.13.0'.

Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
  warnings.warn(message, FutureWarning)
/home/hice1/zyahn3/.local/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:300: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
  warnings.warn(
/home/hice1/zyahn3/.local/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:334: UserWarning: You passed a `dataset_kwargs` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
  warnings.warn(
  0%|          | 0/65 [00:00<?, ?it/s]  2%|▏         | 1/65 [00:00<00:49,  1.29it/s]  3%|▎         | 2/65 [00:01<00:39,  1.60it/s]  5%|▍         | 3/65 [00:01<00:36,  1.71it/s]  6%|▌         | 4/65 [00:02<00:37,  1.64it/s]  8%|▊         | 5/65 [00:03<00:35,  1.70it/s]  9%|▉         | 6/65 [00:03<00:41,  1.41it/s] 11%|█         | 7/65 [00:04<00:39,  1.47it/s] 12%|█▏        | 8/65 [00:05<00:38,  1.49it/s] 14%|█▍        | 9/65 [00:05<00:34,  1.63it/s] 15%|█▌        | 10/65 [00:06<00:33,  1.63it/s] 17%|█▋        | 11/65 [00:06<00:30,  1.78it/s] 18%|█▊        | 12/65 [00:07<00:33,  1.58it/s] 20%|██        | 13/65 [00:08<00:31,  1.68it/s] 22%|██▏       | 14/65 [00:08<00:29,  1.75it/s] 23%|██▎       | 15/65 [00:09<00:29,  1.71it/s] 25%|██▍       | 16/65 [00:09<00:28,  1.70it/s] 26%|██▌       | 17/65 [00:10<00:26,  1.82it/s] 28%|██▊       | 18/65 [00:10<00:25,  1.83it/s] 29%|██▉       | 19/65 [00:11<00:24,  1.84it/s] 31%|███       | 20/65 [00:11<00:24,  1.86it/s] 32%|███▏      | 21/65 [00:12<00:27,  1.62it/s] 34%|███▍      | 22/65 [00:13<00:27,  1.57it/s] 35%|███▌      | 23/65 [00:13<00:25,  1.62it/s] 37%|███▋      | 24/65 [00:14<00:28,  1.45it/s] 38%|███▊      | 25/65 [00:15<00:24,  1.61it/s] 40%|████      | 26/65 [00:15<00:23,  1.63it/s] 42%|████▏     | 27/65 [00:16<00:22,  1.66it/s] 43%|████▎     | 28/65 [00:16<00:21,  1.69it/s] 45%|████▍     | 29/65 [00:17<00:19,  1.83it/s] 46%|████▌     | 30/65 [00:17<00:18,  1.87it/s] 48%|████▊     | 31/65 [00:18<00:19,  1.75it/s] 49%|████▉     | 32/65 [00:19<00:20,  1.61it/s] 51%|█████     | 33/65 [00:20<00:20,  1.54it/s] 52%|█████▏    | 34/65 [00:20<00:19,  1.62it/s] 54%|█████▍    | 35/65 [00:21<00:18,  1.64it/s] 55%|█████▌    | 36/65 [00:21<00:17,  1.69it/s] 57%|█████▋    | 37/65 [00:22<00:16,  1.67it/s] 58%|█████▊    | 38/65 [00:22<00:15,  1.80it/s] 60%|██████    | 39/65 [00:23<00:16,  1.59it/s] 62%|██████▏   | 40/65 [00:24<00:16,  1.55it/s] 63%|██████▎   | 41/65 [00:24<00:15,  1.59it/s] 65%|██████▍   | 42/65 [00:25<00:13,  1.66it/s] 66%|██████▌   | 43/65 [00:26<00:14,  1.54it/s] 68%|██████▊   | 44/65 [00:26<00:13,  1.61it/s] 69%|██████▉   | 45/65 [00:27<00:12,  1.65it/s] 71%|███████   | 46/65 [00:27<00:10,  1.77it/s] 72%|███████▏  | 47/65 [00:28<00:10,  1.75it/s] 74%|███████▍  | 48/65 [00:28<00:09,  1.79it/s] 75%|███████▌  | 49/65 [00:29<00:09,  1.68it/s] 77%|███████▋  | 50/65 [00:30<00:09,  1.51it/s] 78%|███████▊  | 51/65 [00:30<00:08,  1.63it/s] 80%|████████  | 52/65 [00:31<00:07,  1.72it/s] 82%|████████▏ | 53/65 [00:32<00:07,  1.60it/s] 83%|████████▎ | 54/65 [00:32<00:06,  1.72it/s] 85%|████████▍ | 55/65 [00:33<00:05,  1.69it/s] 86%|████████▌ | 56/65 [00:33<00:05,  1.62it/s] 88%|████████▊ | 57/65 [00:34<00:04,  1.70it/s] 89%|████████▉ | 58/65 [00:34<00:04,  1.71it/s] 91%|█████████ | 59/65 [00:35<00:03,  1.73it/s] 92%|█████████▏| 60/65 [00:36<00:02,  1.77it/s] 94%|█████████▍| 61/65 [00:36<00:02,  1.88it/s] 95%|█████████▌| 62/65 [00:36<00:01,  2.01it/s] 97%|█████████▋| 63/65 [00:37<00:01,  1.96it/s] 98%|█████████▊| 64/65 [00:38<00:00,  1.81it/s]100%|██████████| 65/65 [00:38<00:00,  1.75it/s]                                               100%|██████████| 65/65 [00:38<00:00,  1.75it/s]100%|██████████| 65/65 [00:38<00:00,  1.68it/s]
/home/hice1/zyahn3/.local/lib/python3.10/site-packages/peft/utils/save_and_load.py:257: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
{'train_runtime': 38.7743, 'train_samples_per_second': 3.482, 'train_steps_per_second': 1.676, 'train_loss': 0.6987461970402644, 'epoch': 4.81}
EOS token is 256001 for tokenizer GemmaTokenizerFast(name_or_path='TechxGenus/CodeGemma-7b', vocab_size=256000, model_max_length=2048, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|im_start|>', 'eos_token': '<|im_end|>', 'unk_token': '<unk>', 'pad_token': '<|im_end|>', 'additional_special_tokens': ['<|im_start|>', '<|im_end|>']}, clean_up_tokenization_spaces=False),  added_tokens_decoder={
	0: AddedToken("<pad>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	1: AddedToken("<eos>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	2: AddedToken("<bos>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	3: AddedToken("<unk>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	256000: AddedToken("<|im_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	256001: AddedToken("<|im_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
}
---------------------------------------
Begin Slurm Epilog: Nov-23-2024 15:26:04
Job ID:        971271
Array Job ID:  _4294967294
User ID:       zyahn3
Account:       scs
Job name:      finetuningNxcode
Resources:     cpu=1,gres/gpu:h100=1,mem=128G,node=1
Rsrc Used:     cput=00:01:14,vmem=0,walltime=00:01:14,mem=6604K,energy_used=0
Partition:     ice-gpu
Nodes:         atl1-1-03-012-23-0
---------------------------------------
