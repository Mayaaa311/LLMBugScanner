---------------------------------------
Begin Slurm Prolog: Nov-11-2024 14:29:01
Job ID:    926254
User ID:   zyahn3
Account:   scs
Job name:  finetuningNxcode
Partition: coc-gpu
---------------------------------------
/home/hice1/zyahn3/.conda/envs/BugScanner
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [01:34<01:34, 94.17s/it]Loading checkpoint shards: 100%|██████████| 2/2 [01:55<00:00, 51.25s/it]Loading checkpoint shards: 100%|██████████| 2/2 [01:55<00:00, 57.69s/it]
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
/home/hice1/zyahn3/.local/lib/python3.10/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length, packing, dataset_kwargs. Will not be supported from version '0.13.0'.

Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
  warnings.warn(message, FutureWarning)
/home/hice1/zyahn3/.local/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:212: UserWarning: You passed a `packing` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
  warnings.warn(
/home/hice1/zyahn3/.local/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:300: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
  warnings.warn(
/home/hice1/zyahn3/.local/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:334: UserWarning: You passed a `dataset_kwargs` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
  warnings.warn(
  0%|          | 0/60 [00:00<?, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
/home/hice1/zyahn3/.local/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
  2%|▏         | 1/60 [00:09<09:16,  9.43s/it]  3%|▎         | 2/60 [00:18<08:59,  9.31s/it]  5%|▌         | 3/60 [00:27<08:50,  9.31s/it]  7%|▋         | 4/60 [00:37<08:40,  9.30s/it]  8%|▊         | 5/60 [00:46<08:31,  9.30s/it] 10%|█         | 6/60 [00:55<08:22,  9.31s/it] 12%|█▏        | 7/60 [01:05<08:14,  9.33s/it] 13%|█▎        | 8/60 [01:14<08:05,  9.34s/it] 15%|█▌        | 9/60 [01:23<07:56,  9.35s/it] 17%|█▋        | 10/60 [01:33<07:48,  9.37s/it]                                                17%|█▋        | 10/60 [01:33<07:48,  9.37s/it] 18%|█▊        | 11/60 [01:42<07:39,  9.38s/it] 20%|██        | 12/60 [01:49<06:48,  8.51s/it]/home/hice1/zyahn3/.local/lib/python3.10/site-packages/peft/utils/save_and_load.py:257: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
/home/hice1/zyahn3/.local/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
 22%|██▏       | 13/60 [02:05<08:24, 10.73s/it] 23%|██▎       | 14/60 [02:14<07:55, 10.33s/it] 25%|██▌       | 15/60 [02:23<07:32, 10.06s/it] 27%|██▋       | 16/60 [02:33<07:14,  9.87s/it] 28%|██▊       | 17/60 [02:42<06:58,  9.74s/it] 30%|███       | 18/60 [02:52<06:45,  9.66s/it] 32%|███▏      | 19/60 [03:01<06:33,  9.60s/it] 33%|███▎      | 20/60 [03:11<06:22,  9.56s/it]                                                33%|███▎      | 20/60 [03:11<06:22,  9.56s/it] 35%|███▌      | 21/60 [03:20<06:11,  9.53s/it] 37%|███▋      | 22/60 [03:30<06:01,  9.52s/it] 38%|███▊      | 23/60 [03:39<05:51,  9.50s/it] 40%|████      | 24/60 [03:46<05:10,  8.63s/it]/home/hice1/zyahn3/.local/lib/python3.10/site-packages/peft/utils/save_and_load.py:257: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
/home/hice1/zyahn3/.local/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
 42%|████▏     | 25/60 [04:02<06:18, 10.80s/it] 43%|████▎     | 26/60 [04:11<05:53, 10.39s/it] 45%|████▌     | 27/60 [04:20<05:33, 10.10s/it] 47%|████▋     | 28/60 [04:30<05:16,  9.90s/it] 48%|████▊     | 29/60 [04:39<05:02,  9.76s/it] 50%|█████     | 30/60 [04:49<04:50,  9.67s/it]                                                50%|█████     | 30/60 [04:49<04:50,  9.67s/it] 52%|█████▏    | 31/60 [04:58<04:38,  9.60s/it] 53%|█████▎    | 32/60 [05:08<04:27,  9.55s/it] 55%|█████▌    | 33/60 [05:17<04:17,  9.52s/it] 57%|█████▋    | 34/60 [05:27<04:06,  9.50s/it] 58%|█████▊    | 35/60 [05:36<03:56,  9.48s/it] 60%|██████    | 36/60 [05:43<03:26,  8.60s/it]/home/hice1/zyahn3/.local/lib/python3.10/site-packages/peft/utils/save_and_load.py:257: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
/home/hice1/zyahn3/.local/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
 62%|██████▏   | 37/60 [05:58<04:07, 10.77s/it] 63%|██████▎   | 38/60 [06:08<03:48, 10.36s/it] 65%|██████▌   | 39/60 [06:17<03:31, 10.08s/it] 67%|██████▋   | 40/60 [06:27<03:17,  9.88s/it]                                                67%|██████▋   | 40/60 [06:27<03:17,  9.88s/it] 68%|██████▊   | 41/60 [06:36<03:05,  9.75s/it] 70%|███████   | 42/60 [06:45<02:53,  9.65s/it] 72%|███████▏  | 43/60 [06:55<02:42,  9.59s/it] 73%|███████▎  | 44/60 [07:04<02:32,  9.54s/it] 75%|███████▌  | 45/60 [07:14<02:22,  9.51s/it] 77%|███████▋  | 46/60 [07:23<02:12,  9.49s/it] 78%|███████▊  | 47/60 [07:33<02:03,  9.48s/it] 80%|████████  | 48/60 [07:39<01:43,  8.59s/it]/home/hice1/zyahn3/.local/lib/python3.10/site-packages/peft/utils/save_and_load.py:257: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
/home/hice1/zyahn3/.local/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
 82%|████████▏ | 49/60 [07:55<01:58, 10.74s/it] 83%|████████▎ | 50/60 [08:04<01:43, 10.34s/it]                                                83%|████████▎ | 50/60 [08:04<01:43, 10.34s/it] 85%|████████▌ | 51/60 [08:14<01:30, 10.06s/it] 87%|████████▋ | 52/60 [08:23<01:18,  9.87s/it] 88%|████████▊ | 53/60 [08:33<01:08,  9.73s/it] 90%|█████████ | 54/60 [08:42<00:57,  9.64s/it] 92%|█████████▏| 55/60 [08:51<00:47,  9.58s/it] 93%|█████████▎| 56/60 [09:01<00:38,  9.53s/it] 95%|█████████▌| 57/60 [09:10<00:28,  9.51s/it] 97%|█████████▋| 58/60 [09:20<00:18,  9.49s/it] 98%|█████████▊| 59/60 [09:29<00:09,  9.47s/it]100%|██████████| 60/60 [09:36<00:00,  8.59s/it]                                               100%|██████████| 60/60 [09:36<00:00,  8.59s/it]/home/hice1/zyahn3/.local/lib/python3.10/site-packages/peft/utils/save_and_load.py:257: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
/home/hice1/zyahn3/.local/lib/python3.10/site-packages/peft/utils/save_and_load.py:257: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
                                               100%|██████████| 60/60 [09:51<00:00,  8.59s/it]100%|██████████| 60/60 [09:51<00:00,  9.86s/it]
/home/hice1/zyahn3/.local/lib/python3.10/site-packages/peft/utils/save_and_load.py:257: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
{'loss': 0.8225, 'grad_norm': 0.13901743292808533, 'learning_rate': 0.0002, 'epoch': 0.83}
{'loss': 0.2913, 'grad_norm': 0.06355924904346466, 'learning_rate': 0.0002, 'epoch': 1.67}
{'loss': 0.2426, 'grad_norm': 0.04828565940260887, 'learning_rate': 0.0002, 'epoch': 2.5}
{'loss': 0.2714, 'grad_norm': 0.060318123549222946, 'learning_rate': 0.0002, 'epoch': 3.33}
{'loss': 0.1679, 'grad_norm': 0.06612151116132736, 'learning_rate': 0.0002, 'epoch': 4.17}
{'loss': 0.1645, 'grad_norm': 0.08988053351640701, 'learning_rate': 0.0002, 'epoch': 5.0}
{'train_runtime': 591.4684, 'train_samples_per_second': 0.592, 'train_steps_per_second': 0.101, 'train_loss': 0.32669352293014525, 'epoch': 5.0}
---------------------------------------
Begin Slurm Epilog: Nov-11-2024 14:41:34
Job ID:        926254
Array Job ID:  _4294967294
User ID:       zyahn3
Account:       scs
Job name:      finetuningNxcode
Resources:     cpu=1,gres/gpu:a100=1,mem=80G,node=1
Rsrc Used:     cput=00:12:33,vmem=0,walltime=00:12:33,mem=7188K,energy_used=0
Partition:     coc-gpu
Nodes:         atl1-1-03-007-35-0
---------------------------------------
