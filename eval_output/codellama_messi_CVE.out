---------------------------------------
Begin Slurm Prolog: Nov-22-2024 19:00:55
Job ID:    965819
User ID:   zyahn3
Account:   scs
Job name:  eval_bugscanner
Partition: ice-gpu
---------------------------------------
no change     /usr/local/pace-apps/manual/packages/anaconda3/2023.03/condabin/conda
no change     /usr/local/pace-apps/manual/packages/anaconda3/2023.03/bin/conda
no change     /usr/local/pace-apps/manual/packages/anaconda3/2023.03/bin/conda-env
no change     /usr/local/pace-apps/manual/packages/anaconda3/2023.03/bin/activate
no change     /usr/local/pace-apps/manual/packages/anaconda3/2023.03/bin/deactivate
no change     /usr/local/pace-apps/manual/packages/anaconda3/2023.03/etc/profile.d/conda.sh
no change     /usr/local/pace-apps/manual/packages/anaconda3/2023.03/etc/fish/conf.d/conda.fish
no change     /usr/local/pace-apps/manual/packages/anaconda3/2023.03/shell/condabin/Conda.psm1
no change     /usr/local/pace-apps/manual/packages/anaconda3/2023.03/shell/condabin/conda-hook.ps1
no change     /usr/local/pace-apps/manual/packages/anaconda3/2023.03/lib/python3.10/site-packages/xontrib/conda.xsh
no change     /usr/local/pace-apps/manual/packages/anaconda3/2023.03/etc/profile.d/conda.csh
no change     /home/hice1/zyahn3/.bashrc
No action taken.
/home/hice1/zyahn3/.conda/envs/BugScanner
2024-11-22 19:01:08,827 - INFO - Initializing auditor models...
2024-11-22 19:01:08,828 - INFO - Auditor models: finetune/model/codellama/codellama_messi_10ep/
2024-11-22 19:01:08,828 - INFO - Initializing critic model: finetune/model/codellama/codellama_messi_10ep/
2024-11-22 19:01:08,828 - INFO - Initializing ranker model: finetune/model/codellama/codellama_messi_10ep/
2024-11-22 19:01:08,828 - INFO - Initializing parser model: finetune/model/codellama/codellama_messi_10ep/
2024-11-22 19:01:08,828 - INFO - Initializing BugScanner with the given models...
2024-11-22 19:01:08,829 - INFO - BugScanner initialization completed.
2024-11-22 19:01:08,829 - INFO - Starting the bug scanning pipeline with Top-K = 5...
2024-11-22 19:01:08,830 - INFO - Found 109 .sol files in the folder: data_full/0.8splitCVE_clean
2024-11-22 19:01:08,830 - INFO - Output will be saved in: eval_result/codellama_messi_CVE
Unused kwargs: ['load_in_8bit_fp32_cpu_offload']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.
`low_cpu_mem_usage` was None, now set to True since model is quantized.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:12<00:12, 12.34s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:16<00:00,  9.95s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:16<00:00,  8.35s/it]
Unused kwargs: ['load_in_8bit_fp32_cpu_offload']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.
`low_cpu_mem_usage` was None, now set to True since model is quantized.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:12<00:12, 12.01s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:16<00:00,  9.68s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:16<00:00,  8.13s/it]
  0%|          | 0/109 [00:00<?, ?it/s]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
This is a friendly reminder - the current text generation call will exceed the model's predefined maximum length (16384). Depending on the model, you may observe exceptions, performance degradation, or nothing at all.
  1%|          | 1/109 [19:41<35:27:01, 1181.68s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
  2%|▏         | 2/109 [39:34<35:13:02, 1184.88s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
  3%|▎         | 3/109 [59:05<34:46:08, 1180.83s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
  4%|▎         | 4/109 [1:18:14<34:09:33, 1171.17s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
  5%|▍         | 5/109 [1:36:10<33:00:45, 1142.75s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
  6%|▌         | 6/109 [1:54:06<32:07:12, 1122.65s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
  6%|▋         | 7/109 [2:12:10<31:28:43, 1111.01s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
  7%|▋         | 8/109 [2:30:05<30:52:23, 1100.43s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
  8%|▊         | 9/109 [2:48:03<30:22:42, 1093.63s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
  9%|▉         | 10/109 [3:06:03<29:57:30, 1089.40s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 10%|█         | 11/109 [3:24:01<29:33:46, 1085.98s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 11%|█         | 12/109 [3:41:57<29:11:04, 1083.14s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 12%|█▏        | 13/109 [4:00:00<28:52:52, 1083.04s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 13%|█▎        | 14/109 [4:17:57<28:32:00, 1081.27s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 14%|█▍        | 15/109 [4:35:53<28:11:37, 1079.76s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 15%|█▍        | 16/109 [4:53:52<27:52:58, 1079.34s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 16%|█▌        | 17/109 [5:11:57<27:37:40, 1081.10s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 17%|█▋        | 18/109 [5:30:01<27:20:56, 1081.94s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
srun: Job step aborted: Waiting up to 32 seconds for job step to finish.
slurmstepd: error: *** JOB 965819 ON atl1-1-03-013-3-0 CANCELLED AT 2024-11-23T00:37:47 ***
slurmstepd: error: *** STEP 965819.0 ON atl1-1-03-013-3-0 CANCELLED AT 2024-11-23T00:37:47 ***
---------------------------------------
Begin Slurm Epilog: Nov-23-2024 00:37:49
Job ID:        965819
Array Job ID:  _4294967294
User ID:       zyahn3
Account:       scs
Job name:      eval_bugscanner
Resources:     cpu=1,gres/gpu:h100=1,mem=80G,node=1
Rsrc Used:     cput=05:36:51,vmem=0,walltime=05:36:51,mem=6584K,energy_used=0
Partition:     ice-gpu
Nodes:         atl1-1-03-013-3-0
---------------------------------------
