---------------------------------------
Begin Slurm Prolog: Nov-23-2024 13:06:49
Job ID:    970183
User ID:   zyahn3
Account:   scs
Job name:  eval_bugscanner
Partition: ice-gpu
---------------------------------------
no change     /usr/local/pace-apps/manual/packages/anaconda3/2023.03/condabin/conda
no change     /usr/local/pace-apps/manual/packages/anaconda3/2023.03/bin/conda
no change     /usr/local/pace-apps/manual/packages/anaconda3/2023.03/bin/conda-env
no change     /usr/local/pace-apps/manual/packages/anaconda3/2023.03/bin/activate
no change     /usr/local/pace-apps/manual/packages/anaconda3/2023.03/bin/deactivate
no change     /usr/local/pace-apps/manual/packages/anaconda3/2023.03/etc/profile.d/conda.sh
no change     /usr/local/pace-apps/manual/packages/anaconda3/2023.03/etc/fish/conf.d/conda.fish
no change     /usr/local/pace-apps/manual/packages/anaconda3/2023.03/shell/condabin/Conda.psm1
no change     /usr/local/pace-apps/manual/packages/anaconda3/2023.03/shell/condabin/conda-hook.ps1
no change     /usr/local/pace-apps/manual/packages/anaconda3/2023.03/lib/python3.10/site-packages/xontrib/conda.xsh
no change     /usr/local/pace-apps/manual/packages/anaconda3/2023.03/etc/profile.d/conda.csh
no change     /home/hice1/zyahn3/.bashrc
No action taken.
/home/hice1/zyahn3/.conda/envs/BugScanner
2024-11-23 13:06:59,180 - INFO - Initializing auditor models...
2024-11-23 13:06:59,180 - INFO - Auditor models: finetune/model/gemma/gemma_CVE_10ep/
2024-11-23 13:06:59,181 - INFO - Initializing critic model: finetune/model/gemma/gemma_CVE_10ep/
2024-11-23 13:06:59,181 - INFO - Initializing ranker model: finetune/model/gemma/gemma_CVE_10ep/
2024-11-23 13:06:59,181 - INFO - Initializing parser model: finetune/model/gemma/gemma_CVE_10ep/
2024-11-23 13:06:59,181 - INFO - Initializing BugScanner with the given models...
2024-11-23 13:06:59,181 - INFO - BugScanner initialization completed.
2024-11-23 13:06:59,181 - INFO - Starting the bug scanning pipeline with Top-K = 5...
2024-11-23 13:06:59,181 - INFO - Found 109 .sol files in the folder: data_full/0.8splitCVE_clean
2024-11-23 13:06:59,182 - INFO - Output will be saved in: eval_result/gemma_CVE_10ep
Unused kwargs: ['load_in_8bit_fp32_cpu_offload']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.
`low_cpu_mem_usage` was None, now set to True since model is quantized.
`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.
Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use
`config.hidden_activation` if you want to override this behaviour.
See https://github.com/huggingface/transformers/pull/29402 for more details.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:07,  2.62s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:04<00:05,  2.54s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:07<00:02,  2.47s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  2.04s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  2.07s/it]
Unused kwargs: ['load_in_8bit_fp32_cpu_offload']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.
`low_cpu_mem_usage` was None, now set to True since model is quantized.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:07,  2.37s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:04<00:04,  2.38s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:07<00:02,  2.36s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  1.97s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:08<00:00,  2.04s/it]
  0%|          | 0/109 [00:00<?, ?it/s]/home/hice1/zyahn3/.local/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:601: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.5` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
srun: Job step aborted: Waiting up to 32 seconds for job step to finish.
slurmstepd: error: *** STEP 970183.0 ON atl1-1-03-012-18-0 CANCELLED AT 2024-11-23T13:15:29 ***
slurmstepd: error: *** JOB 970183 ON atl1-1-03-012-18-0 CANCELLED AT 2024-11-23T13:15:29 ***
---------------------------------------
Begin Slurm Epilog: Nov-23-2024 13:15:31
Job ID:        970183
Array Job ID:  _4294967294
User ID:       zyahn3
Account:       scs
Job name:      eval_bugscanner
Resources:     cpu=1,gres/gpu:h100=1,mem=80G,node=1
Rsrc Used:     cput=00:08:40,vmem=0,walltime=00:08:40,mem=6576K,energy_used=0
Partition:     ice-gpu
Nodes:         atl1-1-03-012-18-0
---------------------------------------
