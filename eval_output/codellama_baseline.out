---------------------------------------
Begin Slurm Prolog: Nov-22-2024 14:37:46
Job ID:    965238
User ID:   zyahn3
Account:   scs
Job name:  eval_bugscanner
Partition: ice-gpu
---------------------------------------
no change     /usr/local/pace-apps/manual/packages/anaconda3/2023.03/condabin/conda
no change     /usr/local/pace-apps/manual/packages/anaconda3/2023.03/bin/conda
no change     /usr/local/pace-apps/manual/packages/anaconda3/2023.03/bin/conda-env
no change     /usr/local/pace-apps/manual/packages/anaconda3/2023.03/bin/activate
no change     /usr/local/pace-apps/manual/packages/anaconda3/2023.03/bin/deactivate
no change     /usr/local/pace-apps/manual/packages/anaconda3/2023.03/etc/profile.d/conda.sh
no change     /usr/local/pace-apps/manual/packages/anaconda3/2023.03/etc/fish/conf.d/conda.fish
no change     /usr/local/pace-apps/manual/packages/anaconda3/2023.03/shell/condabin/Conda.psm1
no change     /usr/local/pace-apps/manual/packages/anaconda3/2023.03/shell/condabin/conda-hook.ps1
no change     /usr/local/pace-apps/manual/packages/anaconda3/2023.03/lib/python3.10/site-packages/xontrib/conda.xsh
no change     /usr/local/pace-apps/manual/packages/anaconda3/2023.03/etc/profile.d/conda.csh
no change     /home/hice1/zyahn3/.bashrc
No action taken.
/home/hice1/zyahn3/.conda/envs/BugScanner
2024-11-22 14:37:58,534 - INFO - Initializing auditor models...
2024-11-22 14:37:58,543 - INFO - Auditor models: AlfredPros/CodeLlama-7b-Instruct-Solidity
2024-11-22 14:37:58,543 - INFO - Initializing critic model: AlfredPros/CodeLlama-7b-Instruct-Solidity
2024-11-22 14:37:58,543 - INFO - Initializing ranker model: AlfredPros/CodeLlama-7b-Instruct-Solidity
2024-11-22 14:37:58,543 - INFO - Initializing parser model: AlfredPros/CodeLlama-7b-Instruct-Solidity
2024-11-22 14:37:58,543 - INFO - Initializing BugScanner with the given models...
2024-11-22 14:37:58,544 - INFO - BugScanner initialization completed.
2024-11-22 14:37:58,544 - INFO - Starting the bug scanning pipeline with Top-K = 5...
2024-11-22 14:37:58,545 - INFO - Found 109 .sol files in the folder: data_full/0.8splitCVE_clean
2024-11-22 14:37:58,545 - INFO - Output will be saved in: eval_result/codellama_baseline
Unused kwargs: ['load_in_8bit_fp32_cpu_offload']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.
2024-11-22 14:38:00,723 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:47<00:47, 47.60s/it]Loading checkpoint shards: 100%|██████████| 2/2 [01:04<00:00, 38.41s/it]Loading checkpoint shards: 100%|██████████| 2/2 [01:04<00:00, 32.28s/it]
Unused kwargs: ['load_in_8bit_fp32_cpu_offload']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.
2024-11-22 14:39:05,788 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:04<00:04,  4.29s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  3.46s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.90s/it]
  0%|          | 0/109 [00:00<?, ?it/s]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
  1%|          | 1/109 [00:26<48:29, 26.94s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
  2%|▏         | 2/109 [02:16<1:32:15, 51.73s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
  3%|▎         | 3/109 [02:49<1:21:20, 46.04s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
  4%|▎         | 4/109 [04:36<1:52:51, 64.49s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
  5%|▍         | 5/109 [05:07<1:34:08, 54.31s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
  6%|▌         | 6/109 [05:41<1:22:47, 48.23s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
This is a friendly reminder - the current text generation call will exceed the model's predefined maximum length (16384). Depending on the model, you may observe exceptions, performance degradation, or nothing at all.
  6%|▋         | 7/109 [17:22<6:54:52, 244.05s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
  7%|▋         | 8/109 [17:42<4:57:54, 176.98s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
  8%|▊         | 9/109 [17:52<3:31:11, 126.71s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
  9%|▉         | 10/109 [18:20<2:40:10, 97.07s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 10%|█         | 11/109 [18:52<2:06:54, 77.70s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 11%|█         | 12/109 [19:19<1:40:59, 62.47s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 12%|█▏        | 13/109 [19:47<1:23:24, 52.13s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 13%|█▎        | 14/109 [20:19<1:13:08, 46.20s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 14%|█▍        | 15/109 [22:48<2:00:28, 76.90s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 15%|█▍        | 16/109 [23:16<1:36:35, 62.31s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 16%|█▌        | 17/109 [23:49<1:21:57, 53.46s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 17%|█▋        | 18/109 [24:17<1:09:38, 45.91s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 17%|█▋        | 19/109 [24:43<59:44, 39.82s/it]  The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 18%|█▊        | 20/109 [25:15<55:42, 37.56s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 19%|█▉        | 21/109 [26:16<1:05:06, 44.39s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 20%|██        | 22/109 [26:44<57:19, 39.53s/it]  The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 21%|██        | 23/109 [27:36<1:02:12, 43.41s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 22%|██▏       | 24/109 [27:56<51:29, 36.35s/it]  The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 23%|██▎       | 25/109 [28:51<58:29, 41.78s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 24%|██▍       | 26/109 [28:58<43:34, 31.50s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 25%|██▍       | 27/109 [29:27<41:55, 30.67s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 26%|██▌       | 28/109 [29:48<37:44, 27.95s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 27%|██▋       | 29/109 [30:15<36:36, 27.46s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 28%|██▊       | 30/109 [30:35<33:15, 25.25s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 28%|██▊       | 31/109 [31:08<36:03, 27.74s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 29%|██▉       | 32/109 [31:36<35:39, 27.78s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 30%|███       | 33/109 [32:03<34:45, 27.44s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 31%|███       | 34/109 [32:55<43:31, 34.81s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 32%|███▏      | 35/109 [33:34<44:20, 35.95s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 33%|███▎      | 36/109 [34:13<45:11, 37.14s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 34%|███▍      | 37/109 [35:00<48:06, 40.09s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 35%|███▍      | 38/109 [35:51<51:03, 43.14s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 36%|███▌      | 39/109 [36:09<41:40, 35.72s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 37%|███▋      | 40/109 [47:15<4:18:19, 224.63s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 38%|███▊      | 41/109 [58:23<6:45:27, 357.76s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 39%|███▊      | 42/109 [58:43<4:46:23, 256.47s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 39%|███▉      | 43/109 [59:15<3:28:10, 189.24s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 40%|████      | 44/109 [59:48<2:34:00, 142.17s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 41%|████▏     | 45/109 [59:59<1:49:38, 102.79s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 42%|████▏     | 46/109 [1:00:29<1:25:12, 81.15s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 43%|████▎     | 47/109 [1:01:02<1:08:40, 66.47s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 44%|████▍     | 48/109 [1:02:22<1:11:47, 70.62s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
using cuda
Attempted to get_model_instance for name AlfredPros/CodeLlama-7b-Instruct-Solidity and got model_class <class 'lens.Huggingface.Huggingface_LLM'>
Attempted to get_model_instance for name AlfredPros/CodeLlama-7b-Instruct-Solidity and got model_class <class 'lens.Huggingface.Huggingface_LLM'>
Attempted to get_model_instance for name AlfredPros/CodeLlama-7b-Instruct-Solidity and got model_class <class 'lens.Huggingface.Huggingface_LLM'>
Attempted to get_model_instance for name AlfredPros/CodeLlama-7b-Instruct-Solidity and got model_class <class 'lens.Huggingface.Huggingface_LLM'>
---- Running Auditor(s) ----
num tokens input:  1
Auditor response written to :  eval_result/codellama_baseline/2018-12080/auditor/AlfredPros_CodeLlama-7b-Instruct-Solidity_auditor.json
num tokens input:  1
Auditor response written to :  eval_result/codellama_baseline/2018-13703/auditor/AlfredPros_CodeLlama-7b-Instruct-Solidity_auditor.json
num tokens input:  1
Auditor response written to :  eval_result/codellama_baseline/2018-13083/auditor/AlfredPros_CodeLlama-7b-Instruct-Solidity_auditor.json
num tokens input:  1
Auditor response written to :  eval_result/codellama_baseline/2018-12885/auditor/AlfredPros_CodeLlama-7b-Instruct-Solidity_auditor.json
num tokens input:  1
Auditor response written to :  eval_result/codellama_baseline/2018-13079/auditor/AlfredPros_CodeLlama-7b-Instruct-Solidity_auditor.json
num tokens input:  1
Auditor response written to :  eval_result/codellama_baseline/2018-14003/auditor/AlfredPros_CodeLlama-7b-Instruct-Solidity_auditor.json
num tokens input:  1
Auditor response written to :  eval_result/codellama_baseline/2018-12975/auditor/AlfredPros_CodeLlama-7b-Instruct-Solidity_auditor.json
num tokens input:  1
Auditor response written to :  eval_result/codellama_baseline/2018-13041/auditor/AlfredPros_CodeLlama-7b-Instruct-Solidity_auditor.json
num tokens input:  1
Auditor response written to :  eval_result/codellama_baseline/2018-13625/auditor/AlfredPros_CodeLlama-7b-Instruct-Solidity_auditor.json
num tokens input:  1
Auditor response written to :  eval_result/codellama_baseline/2018-13230/auditor/AlfredPros_CodeLlama-7b-Instruct-Solidity_auditor.json
num tokens input:  1
Auditor response written to :  eval_result/codellama_baseline/2018-13088/auditor/AlfredPros_CodeLlama-7b-Instruct-Solidity_auditor.json
num tokens input:  1
Auditor response written to :  eval_result/codellama_baseline/2018-13493/auditor/AlfredPros_CodeLlama-7b-Instruct-Solidity_auditor.json
num tokens input:  1
Auditor response written to :  eval_result/codellama_baseline/2018-12079/auditor/AlfredPros_CodeLlama-7b-Instruct-Solidity_auditor.json
num tokens input:  1
Auditor response written to :  eval_result/codellama_baseline/2018-17987/auditor/AlfredPros_CodeLlama-7b-Instruct-Solidity_auditor.json
num tokens input:  1
Auditor response written to :  eval_result/codellama_baseline/2018-17877/auditor/AlfredPros_CodeLlama-7b-Instruct-Solidity_auditor.json
num tokens input:  1
Auditor response written to :  eval_result/codellama_baseline/2018-13208/auditor/AlfredPros_CodeLlama-7b-Instruct-Solidity_auditor.json
num tokens input:  1
Auditor response written to :  eval_result/codellama_baseline/2018-12070/auditor/AlfredPros_CodeLlama-7b-Instruct-Solidity_auditor.json
num tokens input:  1
Auditor response written to :  eval_result/codellama_baseline/2018-12081/auditor/AlfredPros_CodeLlama-7b-Instruct-Solidity_auditor.json
num tokens input:  1
Auditor response written to :  eval_result/codellama_baseline/2018-12083/auditor/AlfredPros_CodeLlama-7b-Instruct-Solidity_auditor.json
num tokens input:  1
Auditor response written to :  eval_result/codellama_baseline/2018-14087/auditor/AlfredPros_CodeLlama-7b-Instruct-Solidity_auditor.json
num tokens input:  1
Auditor response written to :  eval_result/codellama_baseline/2018-14089/auditor/AlfredPros_CodeLlama-7b-Instruct-Solidity_auditor.json
num tokens input:  1
Auditor response written to :  eval_result/codellama_baseline/2018-13113/auditor/AlfredPros_CodeLlama-7b-Instruct-Solidity_auditor.json
num tokens input:  1
Auditor response written to :  eval_result/codellama_baseline/2018-13533/auditor/AlfredPros_CodeLlama-7b-Instruct-Solidity_auditor.json
num tokens input:  1
Auditor response written to :  eval_result/codellama_baseline/2018-17111/auditor/AlfredPros_CodeLlama-7b-Instruct-Solidity_auditor.json
num tokens input:  1
Auditor response written to :  eval_result/codellama_baseline/2018-10666/auditor/AlfredPros_CodeLlama-7b-Instruct-Solidity_auditor.json
num tokens input:  1
Auditor response written to :  eval_result/codellama_baseline/2018-19833/auditor/AlfredPros_CodeLlama-7b-Instruct-Solidity_auditor.json
num tokens input:  1
Auditor response written to :  eval_result/codellama_baseline/2018-11411/auditor/AlfredPros_CodeLlama-7b-Instruct-Solidity_auditor.json
num tokens input:  1
Auditor response written to :  eval_result/codellama_baseline/2018-12082/auditor/AlfredPros_CodeLlama-7b-Instruct-Solidity_auditor.json
num tokens input:  1
Auditor response written to :  eval_result/codellama_baseline/2018-12063/auditor/AlfredPros_CodeLlama-7b-Instruct-Solidity_auditor.json
num tokens input:  1
Auditor response written to :  eval_result/codellama_baseline/2019-15079/auditor/AlfredPros_CodeLlama-7b-Instruct-Solidity_auditor.json
num tokens input:  1
Auditor response written to :  eval_result/codellama_baseline/2018-12702/auditor/AlfredPros_CodeLlama-7b-Instruct-Solidity_auditor.json
num tokens input:  1
Auditor response written to :  eval_result/codellama_baseline/2018-12511/auditor/AlfredPros_CodeLlama-7b-Instruct-Solidity_auditor.json
num tokens input:  1
Auditor response written to :  eval_result/codellama_baseline/2020-17752/auditor/AlfredPros_CodeLlama-7b-Instruct-Solidity_auditor.json
num tokens input:  1
Auditor response written to :  eval_result/codellama_baseline/2018-13070/auditor/AlfredPros_CodeLlama-7b-Instruct-Solidity_auditor.json
num tokens input:  1
Auditor response written to :  eval_result/codellama_baseline/2018-13127/auditor/AlfredPros_CodeLlama-7b-Instruct-Solidity_auditor.json
num tokens input:  1
Auditor response written to :  eval_result/codellama_baseline/2018-11335/auditor/AlfredPros_CodeLlama-7b-Instruct-Solidity_auditor.json
num tokens input:  1
Auditor response written to :  eval_result/codellama_baseline/2018-14576/auditor/AlfredPros_CodeLlama-7b-Instruct-Solidity_auditor.json
num tokens input:  1
Auditor response written to :  eval_result/codellama_baseline/2018-12025/auditor/AlfredPros_CodeLlama-7b-Instruct-Solidity_auditor.json
num tokens input:  1
Auditor response written to :  eval_result/codellama_baseline/2018-17071/auditor/AlfredPros_CodeLlama-7b-Instruct-Solidity_auditor.json
num tokens input:  1
Auditor response written to :  eval_result/codellama_baseline/2018-13221/auditor/AlfredPros_CodeLlama-7b-Instruct-Solidity_auditor.json
num tokens input:  1
Auditor response written to :  eval_result/codellama_baseline/2019-15078/auditor/AlfredPros_CodeLlama-7b-Instruct-Solidity_auditor.json
num tokens input:  1
Auditor response written to :  eval_result/codellama_baseline/2018-14063/auditor/AlfredPros_CodeLlama-7b-Instruct-Solidity_auditor.json
num tokens input:  1
Auditor response written to :  eval_result/codellama_baseline/2018-13089/auditor/AlfredPros_CodeLlama-7b-Instruct-Solidity_auditor.json
num tokens input:  1
Auditor response written to :  eval_result/codellama_baseline/2018-13087/auditor/AlfredPros_CodeLlama-7b-Instruct-Solidity_auditor.json
num tokens input:  1
Auditor response written to :  eval_result/codellama_baseline/2018-10944/auditor/AlfredPros_CodeLlama-7b-Instruct-Solidity_auditor.json
num tokens input:  1
Auditor response written to :  eval_result/codellama_baseline/2018-12959/auditor/AlfredPros_CodeLlama-7b-Instruct-Solidity_auditor.json
num tokens input:  1
Auditor response written to :  eval_result/codellama_baseline/2018-13075/auditor/AlfredPros_CodeLlama-7b-Instruct-Solidity_auditor.json
num tokens input:  1
Auditor response written to :  eval_result/codellama_baseline/2018-19834/auditor/AlfredPros_CodeLlama-7b-Instruct-Solidity_auditor.json
num tokens input:  1
 45%|████▍     | 49/109 [1:03:06<1:02:40, 62.67s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 46%|████▌     | 50/109 [1:03:22<47:59, 48.81s/it]  The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 47%|████▋     | 51/109 [1:03:36<37:05, 38.38s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 48%|████▊     | 52/109 [1:04:09<34:44, 36.57s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 49%|████▊     | 53/109 [1:04:46<34:19, 36.77s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 50%|████▉     | 54/109 [1:05:09<29:58, 32.70s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 50%|█████     | 55/109 [1:05:41<29:16, 32.52s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 51%|█████▏    | 56/109 [1:06:15<29:01, 32.86s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 52%|█████▏    | 57/109 [1:06:53<29:51, 34.45s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 53%|█████▎    | 58/109 [1:07:23<28:01, 32.98s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 54%|█████▍    | 59/109 [1:07:31<21:12, 25.46s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 55%|█████▌    | 60/109 [1:07:58<21:22, 26.17s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 56%|█████▌    | 61/109 [1:08:34<23:15, 29.07s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 57%|█████▋    | 62/109 [1:09:15<25:26, 32.47s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 58%|█████▊    | 63/109 [1:09:54<26:32, 34.63s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 59%|█████▊    | 64/109 [1:10:43<29:01, 38.71s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 60%|█████▉    | 65/109 [1:11:12<26:22, 35.97s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 61%|██████    | 66/109 [1:11:49<25:59, 36.27s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 61%|██████▏   | 67/109 [1:13:22<37:20, 53.34s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 62%|██████▏   | 68/109 [1:13:53<31:49, 46.57s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 63%|██████▎   | 69/109 [1:14:10<25:09, 37.74s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 64%|██████▍   | 70/109 [1:25:14<2:26:41, 225.69s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 65%|██████▌   | 71/109 [1:25:47<1:46:10, 167.64s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 66%|██████▌   | 72/109 [1:27:39<1:33:04, 150.92s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 67%|██████▋   | 73/109 [1:39:03<3:06:37, 311.05s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 68%|██████▊   | 74/109 [1:39:42<2:13:42, 229.22s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 69%|██████▉   | 75/109 [1:40:14<1:36:26, 170.18s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 70%|██████▉   | 76/109 [1:41:10<1:14:48, 136.00s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 71%|███████   | 77/109 [1:41:31<54:03, 101.36s/it]  The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 72%|███████▏  | 78/109 [1:41:53<40:09, 77.73s/it] The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 72%|███████▏  | 79/109 [1:42:13<30:13, 60.45s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 73%|███████▎  | 80/109 [1:42:41<24:28, 50.65s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 74%|███████▍  | 81/109 [1:43:20<21:57, 47.06s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 75%|███████▌  | 82/109 [1:43:39<17:22, 38.62s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 76%|███████▌  | 83/109 [1:43:52<13:28, 31.10s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 77%|███████▋  | 84/109 [1:44:26<13:17, 31.89s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 78%|███████▊  | 85/109 [1:44:39<10:27, 26.14s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 79%|███████▉  | 86/109 [1:45:02<09:43, 25.38s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 80%|███████▉  | 87/109 [1:45:15<07:54, 21.59s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 81%|████████  | 88/109 [1:45:45<08:23, 23.99s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 82%|████████▏ | 89/109 [1:46:13<08:23, 25.17s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 83%|████████▎ | 90/109 [1:46:41<08:14, 26.03s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 83%|████████▎ | 91/109 [1:47:29<09:47, 32.62s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 84%|████████▍ | 92/109 [1:47:48<08:08, 28.71s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 85%|████████▌ | 93/109 [1:48:21<07:58, 29.92s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 86%|████████▌ | 94/109 [1:48:44<06:57, 27.81s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 87%|████████▋ | 95/109 [1:49:26<07:28, 32.06s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 88%|████████▊ | 96/109 [1:50:29<08:55, 41.20s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 89%|████████▉ | 97/109 [1:50:57<07:27, 37.29s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 90%|████████▉ | 98/109 [1:51:15<05:46, 31.54s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 91%|█████████ | 99/109 [1:51:31<04:29, 26.98s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 92%|█████████▏| 100/109 [1:51:59<04:05, 27.27s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
Auditor response written to :  eval_result/codellama_baseline/2018-13085/auditor/AlfredPros_CodeLlama-7b-Instruct-Solidity_auditor.json
num tokens input:  1
Auditor response written to :  eval_result/codellama_baseline/2018-17050/auditor/AlfredPros_CodeLlama-7b-Instruct-Solidity_auditor.json
num tokens input:  1
Auditor response written to :  eval_result/codellama_baseline/2018-13670/auditor/AlfredPros_CodeLlama-7b-Instruct-Solidity_auditor.json
num tokens input:  1
Auditor response written to :  eval_result/codellama_baseline/2018-13069/auditor/AlfredPros_CodeLlama-7b-Instruct-Solidity_auditor.json
num tokens input:  1
Auditor response written to :  eval_result/codellama_baseline/2018-13836/auditor/AlfredPros_CodeLlama-7b-Instruct-Solidity_auditor.json
num tokens input:  1
Auditor response written to :  eval_result/codellama_baseline/2018-13132/auditor/AlfredPros_CodeLlama-7b-Instruct-Solidity_auditor.json
num tokens input:  1
Auditor response written to :  eval_result/codellama_baseline/2018-13722/auditor/AlfredPros_CodeLlama-7b-Instruct-Solidity_auditor.json
num tokens input:  1
Auditor response written to :  eval_result/codellama_baseline/2019-15080/auditor/AlfredPros_CodeLlama-7b-Instruct-Solidity_auditor.json
num tokens input:  1
Auditor response written to :  eval_result/codellama_baseline/2018-13325/auditor/AlfredPros_CodeLlama-7b-Instruct-Solidity_auditor.json
num tokens input:  1
Auditor response written to :  eval_result/codellama_baseline/2018-13202/auditor/AlfredPros_CodeLlama-7b-Instruct-Solidity_auditor.json
num tokens input:  1
Auditor response written to :  eval_result/codellama_baseline/2018-11687/auditor/AlfredPros_CodeLlama-7b-Instruct-Solidity_auditor.json
num tokens input:  1
Auditor response written to :  eval_result/codellama_baseline/2018-13227/auditor/AlfredPros_CodeLlama-7b-Instruct-Solidity_auditor.json
num tokens input:  1
Auditor response written to :  eval_result/codellama_baseline/2018-14001/auditor/AlfredPros_CodeLlama-7b-Instruct-Solidity_auditor.json
num tokens input:  1
Auditor response written to :  eval_result/codellama_baseline/2018-13091/auditor/AlfredPros_CodeLlama-7b-Instruct-Solidity_auditor.json
num tokens input:  1
Auditor response written to :  eval_result/codellama_baseline/2018-13129/auditor/AlfredPros_CodeLlama-7b-Instruct-Solidity_auditor.json
num tokens input:  1
Auditor response written to :  eval_result/codellama_baseline/2018-10706/auditor/AlfredPros_CodeLlama-7b-Instruct-Solidity_auditor.json
num tokens input:  1
Auditor response written to :  eval_result/codellama_baseline/2021-34273/auditor/AlfredPros_CodeLlama-7b-Instruct-Solidity_auditor.json
num tokens input:  1
Auditor response written to :  eval_result/codellama_baseline/2018-14004/auditor/AlfredPros_CodeLlama-7b-Instruct-Solidity_auditor.json
num tokens input:  1
Auditor response written to :  eval_result/codellama_baseline/2020-17753/auditor/AlfredPros_CodeLlama-7b-Instruct-Solidity_auditor.json
num tokens input:  1
Auditor response written to :  eval_result/codellama_baseline/2018-13327/auditor/AlfredPros_CodeLlama-7b-Instruct-Solidity_auditor.json
num tokens input:  1
Auditor response written to :  eval_result/codellama_baseline/2018-11239/auditor/AlfredPros_CodeLlama-7b-Instruct-Solidity_auditor.json
num tokens input:  1
Auditor response written to :  eval_result/codellama_baseline/2018-12454/auditor/AlfredPros_CodeLlama-7b-Instruct-Solidity_auditor.json
num tokens input:  1
Auditor response written to :  eval_result/codellama_baseline/2018-13086/auditor/AlfredPros_CodeLlama-7b-Instruct-Solidity_auditor.json
num tokens input:  1
Auditor response written to :  eval_result/codellama_baseline/2018-14715/auditor/AlfredPros_CodeLlama-7b-Instruct-Solidity_auditor.json
num tokens input:  1
Auditor response written to :  eval_result/codellama_baseline/2021-3004/auditor/AlfredPros_CodeLlama-7b-Instruct-Solidity_auditor.json
num tokens input:  1
Auditor response written to :  eval_result/codellama_baseline/2018-13128/auditor/AlfredPros_CodeLlama-7b-Instruct-Solidity_auditor.json
num tokens input:  1
Auditor response written to :  eval_result/codellama_baseline/2018-14084/auditor/AlfredPros_CodeLlama-7b-Instruct-Solidity_auditor.json
num tokens input:  1
Auditor response written to :  eval_result/codellama_baseline/2018-14006/auditor/AlfredPros_CodeLlama-7b-Instruct-Solidity_auditor.json
num tokens input:  1
Auditor response written to :  eval_result/codellama_baseline/2018-12230/auditor/AlfredPros_CodeLlama-7b-Instruct-Solidity_auditor.json
num tokens input:  1
Auditor response written to :  eval_result/codellama_baseline/2018-13777/auditor/AlfredPros_CodeLlama-7b-Instruct-Solidity_auditor.json
num tokens input:  1
Auditor response written to :  eval_result/codellama_baseline/2018-14085/auditor/AlfredPros_CodeLlama-7b-Instruct-Solidity_auditor.json
num tokens input:  1
Auditor response written to :  eval_result/codellama_baseline/2018-13228/auditor/AlfredPros_CodeLlama-7b-Instruct-Solidity_auditor.json
num tokens input:  1
Auditor response written to :  eval_result/codellama_baseline/2018-13225/auditor/AlfredPros_CodeLlama-7b-Instruct-Solidity_auditor.json
num tokens input:  1
Auditor response written to :  eval_result/codellama_baseline/2018-13189/auditor/AlfredPros_CodeLlama-7b-Instruct-Solidity_auditor.json
num tokens input:  1
Auditor response written to :  eval_result/codellama_baseline/2018-13126/auditor/AlfredPros_CodeLlama-7b-Instruct-Solidity_auditor.json
num tokens input:  1
Auditor response written to :  eval_result/codellama_baseline/2018-14005/auditor/AlfredPros_CodeLlama-7b-Instruct-Solidity_auditor.json
num tokens input:  1
Auditor response written to :  eval_result/codellama_baseline/2021-3006/auditor/AlfredPros_CodeLlama-7b-Instruct-Solidity_auditor.json
num tokens input:  1
Auditor response written to :  eval_result/codellama_baseline/2021-34270/auditor/AlfredPros_CodeLlama-7b-Instruct-Solidity_auditor.json
num tokens input:  1
Auditor response written to :  eval_result/codellama_baseline/2020-35962/auditor/AlfredPros_CodeLlama-7b-Instruct-Solidity_auditor.json
num tokens input:  1
Auditor response written to :  eval_result/codellama_baseline/2018-10299/auditor/AlfredPros_CodeLlama-7b-Instruct-Solidity_auditor.json
num tokens input:  1
Auditor response written to :  eval_result/codellama_baseline/2018-12068/auditor/AlfredPros_CodeLlama-7b-Instruct-Solidity_auditor.json
num tokens input:  1
Auditor response written to :  eval_result/codellama_baseline/2018-12067/auditor/AlfredPros_CodeLlama-7b-Instruct-Solidity_auditor.json
num tokens input:  1
Auditor response written to :  eval_result/codellama_baseline/2018-13071/auditor/AlfredPros_CodeLlama-7b-Instruct-Solidity_auditor.json
num tokens input:  1
Auditor response written to :  eval_result/codellama_baseline/2018-19830/auditor/AlfredPros_CodeLlama-7b-Instruct-Solidity_auditor.json
num tokens input:  1
Auditor response written to :  eval_result/codellama_baseline/2018-13778/auditor/AlfredPros_CodeLlama-7b-Instruct-Solidity_auditor.json
num tokens input:  1
Auditor response written to :  eval_result/codellama_baseline/2018-11429/auditor/AlfredPros_CodeLlama-7b-Instruct-Solidity_auditor.json
num tokens input:  1
Auditor response written to :  eval_result/codellama_baseline/2018-18425/auditor/AlfredPros_CodeLlama-7b-Instruct-Solidity_auditor.json
num tokens input:  1
Auditor response written to :  eval_result/codellama_baseline/2018-17968/auditor/AlfredPros_CodeLlama-7b-Instruct-Solidity_auditor.json
num tokens input:  1
Auditor response written to :  eval_result/codellama_baseline/2018-13074/auditor/AlfredPros_CodeLlama-7b-Instruct-Solidity_auditor.json
num tokens input:  1
Auditor response written to :  eval_result/codellama_baseline/2018-13092/auditor/AlfredPros_CodeLlama-7b-Instruct-Solidity_auditor.json
num tokens input:  1
Auditor response written to :  eval_result/codellama_baseline/2018-12078/auditor/AlfredPros_CodeLlama-7b-Instruct-Solidity_auditor.json
num tokens input:  1
Auditor response written to :  eval_result/codellama_baseline/2018-12062/auditor/AlfredPros_CodeLlama-7b-Instruct-Solidity_auditor.json
num tokens input:  1
 93%|█████████▎| 101/109 [1:52:23<03:29, 26.17s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 94%|█████████▎| 102/109 [1:52:44<02:52, 24.71s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 94%|█████████▍| 103/109 [1:53:15<02:39, 26.63s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 95%|█████████▌| 104/109 [1:53:44<02:16, 27.33s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 96%|█████████▋| 105/109 [1:54:12<01:50, 27.54s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 97%|█████████▋| 106/109 [1:54:40<01:22, 27.58s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 98%|█████████▊| 107/109 [1:55:14<00:59, 29.55s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 99%|█████████▉| 108/109 [1:55:36<00:27, 27.16s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
100%|██████████| 109/109 [1:56:22<00:00, 32.87s/it]100%|██████████| 109/109 [1:56:22<00:00, 64.06s/it]
  0%|          | 0/109 [00:00<?, ?it/s]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
  1%|          | 1/109 [00:26<48:32, 26.97s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
  2%|▏         | 2/109 [02:18<1:33:08, 52.23s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
  3%|▎         | 3/109 [02:51<1:22:07, 46.49s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
  4%|▎         | 4/109 [04:37<1:52:39, 64.38s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
  5%|▍         | 5/109 [05:08<1:34:15, 54.38s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
  6%|▌         | 6/109 [05:42<1:22:51, 48.27s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
Auditor response written to :  eval_result/codellama_baseline/2018-13068/auditor/AlfredPros_CodeLlama-7b-Instruct-Solidity_auditor.json
num tokens input:  1
Auditor response written to :  eval_result/codellama_baseline/2018-10376/auditor/AlfredPros_CodeLlama-7b-Instruct-Solidity_auditor.json
num tokens input:  1
Auditor response written to :  eval_result/codellama_baseline/2018-13073/auditor/AlfredPros_CodeLlama-7b-Instruct-Solidity_auditor.json
num tokens input:  1
Auditor response written to :  eval_result/codellama_baseline/2018-13783/auditor/AlfredPros_CodeLlama-7b-Instruct-Solidity_auditor.json
num tokens input:  1
Auditor response written to :  eval_result/codellama_baseline/2018-13782/auditor/AlfredPros_CodeLlama-7b-Instruct-Solidity_auditor.json
num tokens input:  1
Auditor response written to :  eval_result/codellama_baseline/2021-34272/auditor/AlfredPros_CodeLlama-7b-Instruct-Solidity_auditor.json
num tokens input:  1
Auditor response written to :  eval_result/codellama_baseline/2018-12703/auditor/AlfredPros_CodeLlama-7b-Instruct-Solidity_auditor.json
num tokens input:  1
Auditor response written to :  eval_result/codellama_baseline/2018-13076/auditor/AlfredPros_CodeLlama-7b-Instruct-Solidity_auditor.json
num tokens input:  1
Auditor response written to :  eval_result/codellama_baseline/2018-19832/auditor/AlfredPros_CodeLlama-7b-Instruct-Solidity_auditor.json
all auditor output write to :  ['eval_result/codellama_baseline/2018-12080/auditor', 'eval_result/codellama_baseline/2018-13703/auditor', 'eval_result/codellama_baseline/2018-13083/auditor', 'eval_result/codellama_baseline/2018-12885/auditor', 'eval_result/codellama_baseline/2018-13079/auditor', 'eval_result/codellama_baseline/2018-14003/auditor', 'eval_result/codellama_baseline/2018-12975/auditor', 'eval_result/codellama_baseline/2018-13041/auditor', 'eval_result/codellama_baseline/2018-13625/auditor', 'eval_result/codellama_baseline/2018-13230/auditor', 'eval_result/codellama_baseline/2018-13088/auditor', 'eval_result/codellama_baseline/2018-13493/auditor', 'eval_result/codellama_baseline/2018-12079/auditor', 'eval_result/codellama_baseline/2018-17987/auditor', 'eval_result/codellama_baseline/2018-17877/auditor', 'eval_result/codellama_baseline/2018-13208/auditor', 'eval_result/codellama_baseline/2018-12070/auditor', 'eval_result/codellama_baseline/2018-12081/auditor', 'eval_result/codellama_baseline/2018-12083/auditor', 'eval_result/codellama_baseline/2018-14087/auditor', 'eval_result/codellama_baseline/2018-14089/auditor', 'eval_result/codellama_baseline/2018-13113/auditor', 'eval_result/codellama_baseline/2018-13533/auditor', 'eval_result/codellama_baseline/2018-17111/auditor', 'eval_result/codellama_baseline/2018-10666/auditor', 'eval_result/codellama_baseline/2018-19833/auditor', 'eval_result/codellama_baseline/2018-11411/auditor', 'eval_result/codellama_baseline/2018-12082/auditor', 'eval_result/codellama_baseline/2018-12063/auditor', 'eval_result/codellama_baseline/2019-15079/auditor', 'eval_result/codellama_baseline/2018-12702/auditor', 'eval_result/codellama_baseline/2018-12511/auditor', 'eval_result/codellama_baseline/2020-17752/auditor', 'eval_result/codellama_baseline/2018-13070/auditor', 'eval_result/codellama_baseline/2018-13127/auditor', 'eval_result/codellama_baseline/2018-11335/auditor', 'eval_result/codellama_baseline/2018-14576/auditor', 'eval_result/codellama_baseline/2018-12025/auditor', 'eval_result/codellama_baseline/2018-17071/auditor', 'eval_result/codellama_baseline/2018-13221/auditor', 'eval_result/codellama_baseline/2019-15078/auditor', 'eval_result/codellama_baseline/2018-14063/auditor', 'eval_result/codellama_baseline/2018-13089/auditor', 'eval_result/codellama_baseline/2018-13087/auditor', 'eval_result/codellama_baseline/2018-10944/auditor', 'eval_result/codellama_baseline/2018-12959/auditor', 'eval_result/codellama_baseline/2018-13075/auditor', 'eval_result/codellama_baseline/2018-19834/auditor', 'eval_result/codellama_baseline/2018-13085/auditor', 'eval_result/codellama_baseline/2018-17050/auditor', 'eval_result/codellama_baseline/2018-13670/auditor', 'eval_result/codellama_baseline/2018-13069/auditor', 'eval_result/codellama_baseline/2018-13836/auditor', 'eval_result/codellama_baseline/2018-13132/auditor', 'eval_result/codellama_baseline/2018-13722/auditor', 'eval_result/codellama_baseline/2019-15080/auditor', 'eval_result/codellama_baseline/2018-13325/auditor', 'eval_result/codellama_baseline/2018-13202/auditor', 'eval_result/codellama_baseline/2018-11687/auditor', 'eval_result/codellama_baseline/2018-13227/auditor', 'eval_result/codellama_baseline/2018-14001/auditor', 'eval_result/codellama_baseline/2018-13091/auditor', 'eval_result/codellama_baseline/2018-13129/auditor', 'eval_result/codellama_baseline/2018-10706/auditor', 'eval_result/codellama_baseline/2021-34273/auditor', 'eval_result/codellama_baseline/2018-14004/auditor', 'eval_result/codellama_baseline/2020-17753/auditor', 'eval_result/codellama_baseline/2018-13327/auditor', 'eval_result/codellama_baseline/2018-11239/auditor', 'eval_result/codellama_baseline/2018-12454/auditor', 'eval_result/codellama_baseline/2018-13086/auditor', 'eval_result/codellama_baseline/2018-14715/auditor', 'eval_result/codellama_baseline/2021-3004/auditor', 'eval_result/codellama_baseline/2018-13128/auditor', 'eval_result/codellama_baseline/2018-14084/auditor', 'eval_result/codellama_baseline/2018-14006/auditor', 'eval_result/codellama_baseline/2018-12230/auditor', 'eval_result/codellama_baseline/2018-13777/auditor', 'eval_result/codellama_baseline/2018-14085/auditor', 'eval_result/codellama_baseline/2018-13228/auditor', 'eval_result/codellama_baseline/2018-13225/auditor', 'eval_result/codellama_baseline/2018-13189/auditor', 'eval_result/codellama_baseline/2018-13126/auditor', 'eval_result/codellama_baseline/2018-14005/auditor', 'eval_result/codellama_baseline/2021-3006/auditor', 'eval_result/codellama_baseline/2021-34270/auditor', 'eval_result/codellama_baseline/2020-35962/auditor', 'eval_result/codellama_baseline/2018-10299/auditor', 'eval_result/codellama_baseline/2018-12068/auditor', 'eval_result/codellama_baseline/2018-12067/auditor', 'eval_result/codellama_baseline/2018-13071/auditor', 'eval_result/codellama_baseline/2018-19830/auditor', 'eval_result/codellama_baseline/2018-13778/auditor', 'eval_result/codellama_baseline/2018-11429/auditor', 'eval_result/codellama_baseline/2018-18425/auditor', 'eval_result/codellama_baseline/2018-17968/auditor', 'eval_result/codellama_baseline/2018-13074/auditor', 'eval_result/codellama_baseline/2018-13092/auditor', 'eval_result/codellama_baseline/2018-12078/auditor', 'eval_result/codellama_baseline/2018-12062/auditor', 'eval_result/codellama_baseline/2018-13068/auditor', 'eval_result/codellama_baseline/2018-10376/auditor', 'eval_result/codellama_baseline/2018-13073/auditor', 'eval_result/codellama_baseline/2018-13783/auditor', 'eval_result/codellama_baseline/2018-13782/auditor', 'eval_result/codellama_baseline/2021-34272/auditor', 'eval_result/codellama_baseline/2018-12703/auditor', 'eval_result/codellama_baseline/2018-13076/auditor', 'eval_result/codellama_baseline/2018-19832/auditor']
---- Running step: auditor_summary ----
FILEPATH:  eval_result/codellama_baseline/2018-12080/auditor/AlfredPros_CodeLlama-7b-Instruct-Solidity_auditor_0.json
num tokens input:  1
FILEPATH:  eval_result/codellama_baseline/2018-13703/auditor/AlfredPros_CodeLlama-7b-Instruct-Solidity_auditor_0.json
num tokens input:  1
FILEPATH:  eval_result/codellama_baseline/2018-13083/auditor/AlfredPros_CodeLlama-7b-Instruct-Solidity_auditor_0.json
num tokens input:  1
FILEPATH:  eval_result/codellama_baseline/2018-12885/auditor/AlfredPros_CodeLlama-7b-Instruct-Solidity_auditor_0.json
num tokens input:  1
FILEPATH:  eval_result/codellama_baseline/2018-13079/auditor/AlfredPros_CodeLlama-7b-Instruct-Solidity_auditor_0.json
num tokens input:  1
FILEPATH:  eval_result/codellama_baseline/2018-14003/auditor/AlfredPros_CodeLlama-7b-Instruct-Solidity_auditor_0.json
num tokens input:  1
FILEPATH:  eval_result/codellama_baseline/2018-12975/auditor/AlfredPros_CodeLlama-7b-Instruct-Solidity_auditor_0.json
num tokens input:  1
  6%|▋         | 7/109 [20:22<8:26:11, 297.76s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
  7%|▋         | 8/109 [20:42<6:01:02, 214.48s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
  8%|▊         | 9/109 [20:51<4:14:45, 152.85s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
  9%|▉         | 10/109 [21:19<3:10:19, 115.35s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 10%|█         | 11/109 [21:51<2:27:37, 90.39s/it] The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 11%|█         | 12/109 [22:17<1:55:05, 71.19s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 12%|█▏        | 13/109 [22:45<1:33:03, 58.16s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 13%|█▎        | 14/109 [23:19<1:20:31, 50.86s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 14%|█▍        | 15/109 [25:43<2:03:16, 78.69s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 15%|█▍        | 16/109 [26:10<1:38:16, 63.40s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 16%|█▌        | 17/109 [26:42<1:22:48, 54.01s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 17%|█▋        | 18/109 [27:10<1:09:55, 46.11s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 17%|█▋        | 19/109 [27:35<59:44, 39.83s/it]  The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 18%|█▊        | 20/109 [28:07<55:36, 37.49s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 19%|█▉        | 21/109 [29:06<1:04:12, 43.78s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 20%|██        | 22/109 [29:33<56:16, 38.81s/it]  The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 21%|██        | 23/109 [30:25<1:01:09, 42.67s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 22%|██▏       | 24/109 [30:45<50:44, 35.82s/it]  The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 23%|██▎       | 25/109 [31:33<55:37, 39.74s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 24%|██▍       | 26/109 [31:41<41:32, 30.04s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 25%|██▍       | 27/109 [32:09<40:14, 29.44s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 26%|██▌       | 28/109 [32:30<36:29, 27.03s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 27%|██▋       | 29/109 [32:56<35:39, 26.74s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 28%|██▊       | 30/109 [33:16<32:32, 24.71s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 28%|██▊       | 31/109 [33:47<34:17, 26.38s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 29%|██▉       | 32/109 [34:14<34:22, 26.79s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 30%|███       | 33/109 [34:40<33:27, 26.42s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 31%|███       | 34/109 [35:31<42:08, 33.72s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 32%|███▏      | 35/109 [36:08<42:58, 34.84s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 33%|███▎      | 36/109 [36:47<43:58, 36.14s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 34%|███▍      | 37/109 [37:33<46:43, 38.94s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 35%|███▍      | 38/109 [38:19<48:47, 41.23s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 36%|███▌      | 39/109 [38:37<40:01, 34.31s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 37%|███▋      | 40/109 [53:17<5:30:55, 287.76s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 38%|███▊      | 41/109 [1:08:09<8:51:49, 469.25s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 39%|███▊      | 42/109 [1:08:29<6:13:21, 334.35s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 39%|███▉      | 43/109 [1:09:01<4:27:57, 243.60s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 40%|████      | 44/109 [1:09:33<3:15:05, 180.09s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 41%|████▏     | 45/109 [1:09:43<2:17:39, 129.06s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 42%|████▏     | 46/109 [1:10:12<1:44:13, 99.27s/it] The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 43%|████▎     | 47/109 [1:10:44<1:21:43, 79.08s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 44%|████▍     | 48/109 [1:12:02<1:19:59, 78.68s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 45%|████▍     | 49/109 [1:12:45<1:07:53, 67.89s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 46%|████▌     | 50/109 [1:13:01<51:28, 52.35s/it]  The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 47%|████▋     | 51/109 [1:13:15<39:23, 40.75s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 48%|████▊     | 52/109 [1:13:47<36:11, 38.10s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 49%|████▊     | 53/109 [1:14:23<35:08, 37.65s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 50%|████▉     | 54/109 [1:14:46<30:31, 33.30s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 50%|█████     | 55/109 [1:15:17<29:22, 32.64s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 51%|█████▏    | 56/109 [1:15:50<28:47, 32.60s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 52%|█████▏    | 57/109 [1:16:27<29:24, 33.93s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 53%|█████▎    | 58/109 [1:16:56<27:29, 32.35s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 54%|█████▍    | 59/109 [1:17:03<20:47, 24.94s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 55%|█████▌    | 60/109 [1:17:31<21:02, 25.76s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 56%|█████▌    | 61/109 [1:18:06<22:52, 28.60s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 57%|█████▋    | 62/109 [1:18:46<25:03, 31.99s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 58%|█████▊    | 63/109 [1:19:25<26:07, 34.07s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 59%|█████▊    | 64/109 [1:20:12<28:23, 37.85s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 60%|█████▉    | 65/109 [1:20:40<25:40, 35.02s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
FILEPATH:  eval_result/codellama_baseline/2018-13041/auditor/AlfredPros_CodeLlama-7b-Instruct-Solidity_auditor_0.json
num tokens input:  1
FILEPATH:  eval_result/codellama_baseline/2018-13625/auditor/AlfredPros_CodeLlama-7b-Instruct-Solidity_auditor_0.json
num tokens input:  1
FILEPATH:  eval_result/codellama_baseline/2018-13230/auditor/AlfredPros_CodeLlama-7b-Instruct-Solidity_auditor_0.json
num tokens input:  1
FILEPATH:  eval_result/codellama_baseline/2018-13088/auditor/AlfredPros_CodeLlama-7b-Instruct-Solidity_auditor_0.json
num tokens input:  1
FILEPATH:  eval_result/codellama_baseline/2018-13493/auditor/AlfredPros_CodeLlama-7b-Instruct-Solidity_auditor_0.json
num tokens input:  1
FILEPATH:  eval_result/codellama_baseline/2018-12079/auditor/AlfredPros_CodeLlama-7b-Instruct-Solidity_auditor_0.json
num tokens input:  1
FILEPATH:  eval_result/codellama_baseline/2018-17987/auditor/AlfredPros_CodeLlama-7b-Instruct-Solidity_auditor_0.json
num tokens input:  1
FILEPATH:  eval_result/codellama_baseline/2018-17877/auditor/AlfredPros_CodeLlama-7b-Instruct-Solidity_auditor_0.json
num tokens input:  1
FILEPATH:  eval_result/codellama_baseline/2018-13208/auditor/AlfredPros_CodeLlama-7b-Instruct-Solidity_auditor_0.json
num tokens input:  1
FILEPATH:  eval_result/codellama_baseline/2018-12070/auditor/AlfredPros_CodeLlama-7b-Instruct-Solidity_auditor_0.json
num tokens input:  1
FILEPATH:  eval_result/codellama_baseline/2018-12081/auditor/AlfredPros_CodeLlama-7b-Instruct-Solidity_auditor_0.json
num tokens input:  1
FILEPATH:  eval_result/codellama_baseline/2018-12083/auditor/AlfredPros_CodeLlama-7b-Instruct-Solidity_auditor_0.json
num tokens input:  1
FILEPATH:  eval_result/codellama_baseline/2018-14087/auditor/AlfredPros_CodeLlama-7b-Instruct-Solidity_auditor_0.json
num tokens input:  1
FILEPATH:  eval_result/codellama_baseline/2018-14089/auditor/AlfredPros_CodeLlama-7b-Instruct-Solidity_auditor_0.json
num tokens input:  1
FILEPATH:  eval_result/codellama_baseline/2018-13113/auditor/AlfredPros_CodeLlama-7b-Instruct-Solidity_auditor_0.json
num tokens input:  1
FILEPATH:  eval_result/codellama_baseline/2018-13533/auditor/AlfredPros_CodeLlama-7b-Instruct-Solidity_auditor_0.json
num tokens input:  1
FILEPATH:  eval_result/codellama_baseline/2018-17111/auditor/AlfredPros_CodeLlama-7b-Instruct-Solidity_auditor_0.json
num tokens input:  1
FILEPATH:  eval_result/codellama_baseline/2018-10666/auditor/AlfredPros_CodeLlama-7b-Instruct-Solidity_auditor_0.json
num tokens input:  1
FILEPATH:  eval_result/codellama_baseline/2018-19833/auditor/AlfredPros_CodeLlama-7b-Instruct-Solidity_auditor_0.json
num tokens input:  1
FILEPATH:  eval_result/codellama_baseline/2018-11411/auditor/AlfredPros_CodeLlama-7b-Instruct-Solidity_auditor_0.json
num tokens input:  1
FILEPATH:  eval_result/codellama_baseline/2018-12082/auditor/AlfredPros_CodeLlama-7b-Instruct-Solidity_auditor_0.json
num tokens input:  1
FILEPATH:  eval_result/codellama_baseline/2018-12063/auditor/AlfredPros_CodeLlama-7b-Instruct-Solidity_auditor_0.json
num tokens input:  1
FILEPATH:  eval_result/codellama_baseline/2019-15079/auditor/AlfredPros_CodeLlama-7b-Instruct-Solidity_auditor_0.json
num tokens input:  1
FILEPATH:  eval_result/codellama_baseline/2018-12702/auditor/AlfredPros_CodeLlama-7b-Instruct-Solidity_auditor_0.json
num tokens input:  1
FILEPATH:  eval_result/codellama_baseline/2018-12511/auditor/AlfredPros_CodeLlama-7b-Instruct-Solidity_auditor_0.json
num tokens input:  1
FILEPATH:  eval_result/codellama_baseline/2020-17752/auditor/AlfredPros_CodeLlama-7b-Instruct-Solidity_auditor_0.json
num tokens input:  1
FILEPATH:  eval_result/codellama_baseline/2018-13070/auditor/AlfredPros_CodeLlama-7b-Instruct-Solidity_auditor_0.json
num tokens input:  1
FILEPATH:  eval_result/codellama_baseline/2018-13127/auditor/AlfredPros_CodeLlama-7b-Instruct-Solidity_auditor_0.json
num tokens input:  1
FILEPATH:  eval_result/codellama_baseline/2018-11335/auditor/AlfredPros_CodeLlama-7b-Instruct-Solidity_auditor_0.json
num tokens input:  1
FILEPATH:  eval_result/codellama_baseline/2018-14576/auditor/AlfredPros_CodeLlama-7b-Instruct-Solidity_auditor_0.json
num tokens input:  1
FILEPATH:  eval_result/codellama_baseline/2018-12025/auditor/AlfredPros_CodeLlama-7b-Instruct-Solidity_auditor_0.json
num tokens input:  1
FILEPATH:  eval_result/codellama_baseline/2018-17071/auditor/AlfredPros_CodeLlama-7b-Instruct-Solidity_auditor_0.json
num tokens input:  1
FILEPATH:  eval_result/codellama_baseline/2018-13221/auditor/AlfredPros_CodeLlama-7b-Instruct-Solidity_auditor_0.json
num tokens input:  1
FILEPATH:  eval_result/codellama_baseline/2019-15078/auditor/AlfredPros_CodeLlama-7b-Instruct-Solidity_auditor_0.json
num tokens input:  1
FILEPATH:  eval_result/codellama_baseline/2018-14063/auditor/AlfredPros_CodeLlama-7b-Instruct-Solidity_auditor_0.json
num tokens input:  1
FILEPATH:  eval_result/codellama_baseline/2018-13089/auditor/AlfredPros_CodeLlama-7b-Instruct-Solidity_auditor_0.json
num tokens input:  1
FILEPATH:  eval_result/codellama_baseline/2018-13087/auditor/AlfredPros_CodeLlama-7b-Instruct-Solidity_auditor_0.json
num tokens input:  1
FILEPATH:  eval_result/codellama_baseline/2018-10944/auditor/AlfredPros_CodeLlama-7b-Instruct-Solidity_auditor_0.json
num tokens input:  1
FILEPATH:  eval_result/codellama_baseline/2018-12959/auditor/AlfredPros_CodeLlama-7b-Instruct-Solidity_auditor_0.json
num tokens input:  1
FILEPATH:  eval_result/codellama_baseline/2018-13075/auditor/AlfredPros_CodeLlama-7b-Instruct-Solidity_auditor_0.json
num tokens input:  1
FILEPATH:  eval_result/codellama_baseline/2018-19834/auditor/AlfredPros_CodeLlama-7b-Instruct-Solidity_auditor_0.json
num tokens input:  1
FILEPATH:  eval_result/codellama_baseline/2018-13085/auditor/AlfredPros_CodeLlama-7b-Instruct-Solidity_auditor_0.json
num tokens input:  1
FILEPATH:  eval_result/codellama_baseline/2018-17050/auditor/AlfredPros_CodeLlama-7b-Instruct-Solidity_auditor_0.json
num tokens input:  1
FILEPATH:  eval_result/codellama_baseline/2018-13670/auditor/AlfredPros_CodeLlama-7b-Instruct-Solidity_auditor_0.json
num tokens input:  1
FILEPATH:  eval_result/codellama_baseline/2018-13069/auditor/AlfredPros_CodeLlama-7b-Instruct-Solidity_auditor_0.json
num tokens input:  1
FILEPATH:  eval_result/codellama_baseline/2018-13836/auditor/AlfredPros_CodeLlama-7b-Instruct-Solidity_auditor_0.json
num tokens input:  1
FILEPATH:  eval_result/codellama_baseline/2018-13132/auditor/AlfredPros_CodeLlama-7b-Instruct-Solidity_auditor_0.json
num tokens input:  1
FILEPATH:  eval_result/codellama_baseline/2018-13722/auditor/AlfredPros_CodeLlama-7b-Instruct-Solidity_auditor_0.json
num tokens input:  1
FILEPATH:  eval_result/codellama_baseline/2019-15080/auditor/AlfredPros_CodeLlama-7b-Instruct-Solidity_auditor_0.json
num tokens input:  1
FILEPATH:  eval_result/codellama_baseline/2018-13325/auditor/AlfredPros_CodeLlama-7b-Instruct-Solidity_auditor_0.json
num tokens input:  1
FILEPATH:  eval_result/codellama_baseline/2018-13202/auditor/AlfredPros_CodeLlama-7b-Instruct-Solidity_auditor_0.json
num tokens input:  1
FILEPATH:  eval_result/codellama_baseline/2018-11687/auditor/AlfredPros_CodeLlama-7b-Instruct-Solidity_auditor_0.json
num tokens input:  1
FILEPATH:  eval_result/codellama_baseline/2018-13227/auditor/AlfredPros_CodeLlama-7b-Instruct-Solidity_auditor_0.json
num tokens input:  1
FILEPATH:  eval_result/codellama_baseline/2018-14001/auditor/AlfredPros_CodeLlama-7b-Instruct-Solidity_auditor_0.json
num tokens input:  1
FILEPATH:  eval_result/codellama_baseline/2018-13091/auditor/AlfredPros_CodeLlama-7b-Instruct-Solidity_auditor_0.json
num tokens input:  1
FILEPATH:  eval_result/codellama_baseline/2018-13129/auditor/AlfredPros_CodeLlama-7b-Instruct-Solidity_auditor_0.json
num tokens input:  1
FILEPATH:  eval_result/codellama_baseline/2018-10706/auditor/AlfredPros_CodeLlama-7b-Instruct-Solidity_auditor_0.json
num tokens input:  1
FILEPATH:  eval_result/codellama_baseline/2021-34273/auditor/AlfredPros_CodeLlama-7b-Instruct-Solidity_auditor_0.json
num tokens input:  1
FILEPATH:  eval_result/codellama_baseline/2018-14004/auditor/AlfredPros_CodeLlama-7b-Instruct-Solidity_auditor_0.json
num tokens input:  1
 61%|██████    | 66/109 [1:21:16<25:18, 35.32s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 61%|██████▏   | 67/109 [1:22:47<36:23, 52.00s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 62%|██████▏   | 68/109 [1:23:17<31:02, 45.43s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 63%|██████▎   | 69/109 [1:23:34<24:32, 36.82s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 64%|██████▍   | 70/109 [1:24:19<25:35, 39.37s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 65%|██████▌   | 71/109 [1:24:51<23:32, 37.18s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 66%|██████▌   | 72/109 [1:26:43<36:46, 59.62s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 67%|██████▋   | 73/109 [1:41:40<3:06:25, 310.70s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 68%|██████▊   | 74/109 [1:42:17<2:13:28, 228.81s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 69%|██████▉   | 75/109 [1:42:49<1:36:12, 169.77s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 70%|██████▉   | 76/109 [1:43:42<1:13:58, 134.50s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 71%|███████   | 77/109 [1:44:00<53:12, 99.76s/it]   The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 72%|███████▏  | 78/109 [1:44:23<39:31, 76.49s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 72%|███████▏  | 79/109 [1:44:42<29:44, 59.48s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 73%|███████▎  | 80/109 [1:45:10<24:09, 49.98s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 74%|███████▍  | 81/109 [1:45:48<21:41, 46.47s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 75%|███████▌  | 82/109 [1:46:06<17:03, 37.90s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 76%|███████▌  | 83/109 [1:46:20<13:15, 30.60s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 77%|███████▋  | 84/109 [1:46:53<13:03, 31.32s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 78%|███████▊  | 85/109 [1:47:05<10:15, 25.63s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 79%|███████▉  | 86/109 [1:47:29<09:34, 24.98s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 80%|███████▉  | 87/109 [1:47:41<07:46, 21.21s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 81%|████████  | 88/109 [1:48:11<08:16, 23.66s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 82%|████████▏ | 89/109 [1:48:38<08:18, 24.93s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 83%|████████▎ | 90/109 [1:49:06<08:10, 25.83s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 83%|████████▎ | 91/109 [1:49:54<09:40, 32.23s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 84%|████████▍ | 92/109 [1:50:13<08:03, 28.41s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 85%|████████▌ | 93/109 [1:50:45<07:53, 29.58s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 86%|████████▌ | 94/109 [1:51:08<06:51, 27.44s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 87%|████████▋ | 95/109 [1:51:48<07:19, 31.43s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 88%|████████▊ | 96/109 [1:52:50<08:46, 40.54s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 89%|████████▉ | 97/109 [1:53:18<07:20, 36.68s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 90%|████████▉ | 98/109 [1:53:36<05:41, 31.03s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 91%|█████████ | 99/109 [1:53:52<04:25, 26.55s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 92%|█████████▏| 100/109 [1:54:20<04:02, 26.91s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 93%|█████████▎| 101/109 [1:54:43<03:26, 25.77s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 94%|█████████▎| 102/109 [1:55:04<02:49, 24.27s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 94%|█████████▍| 103/109 [1:55:34<02:37, 26.17s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 95%|█████████▌| 104/109 [1:56:03<02:14, 26.92s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 96%|█████████▋| 105/109 [1:56:30<01:48, 27.15s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 97%|█████████▋| 106/109 [1:56:58<01:21, 27.16s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 98%|█████████▊| 107/109 [1:57:29<00:56, 28.31s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 99%|█████████▉| 108/109 [1:57:50<00:26, 26.22s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
100%|██████████| 109/109 [1:58:35<00:00, 31.88s/it]100%|██████████| 109/109 [1:58:35<00:00, 65.28s/it]
FILEPATH:  eval_result/codellama_baseline/2020-17753/auditor/AlfredPros_CodeLlama-7b-Instruct-Solidity_auditor_0.json
num tokens input:  1
FILEPATH:  eval_result/codellama_baseline/2018-13327/auditor/AlfredPros_CodeLlama-7b-Instruct-Solidity_auditor_0.json
num tokens input:  1
FILEPATH:  eval_result/codellama_baseline/2018-11239/auditor/AlfredPros_CodeLlama-7b-Instruct-Solidity_auditor_0.json
num tokens input:  1
FILEPATH:  eval_result/codellama_baseline/2018-12454/auditor/AlfredPros_CodeLlama-7b-Instruct-Solidity_auditor_0.json
num tokens input:  1
FILEPATH:  eval_result/codellama_baseline/2018-13086/auditor/AlfredPros_CodeLlama-7b-Instruct-Solidity_auditor_0.json
num tokens input:  1
FILEPATH:  eval_result/codellama_baseline/2018-14715/auditor/AlfredPros_CodeLlama-7b-Instruct-Solidity_auditor_0.json
num tokens input:  1
FILEPATH:  eval_result/codellama_baseline/2021-3004/auditor/AlfredPros_CodeLlama-7b-Instruct-Solidity_auditor_0.json
num tokens input:  1
FILEPATH:  eval_result/codellama_baseline/2018-13128/auditor/AlfredPros_CodeLlama-7b-Instruct-Solidity_auditor_0.json
num tokens input:  1
FILEPATH:  eval_result/codellama_baseline/2018-14084/auditor/AlfredPros_CodeLlama-7b-Instruct-Solidity_auditor_0.json
num tokens input:  1
FILEPATH:  eval_result/codellama_baseline/2018-14006/auditor/AlfredPros_CodeLlama-7b-Instruct-Solidity_auditor_0.json
num tokens input:  1
FILEPATH:  eval_result/codellama_baseline/2018-12230/auditor/AlfredPros_CodeLlama-7b-Instruct-Solidity_auditor_0.json
num tokens input:  1
FILEPATH:  eval_result/codellama_baseline/2018-13777/auditor/AlfredPros_CodeLlama-7b-Instruct-Solidity_auditor_0.json
num tokens input:  1
FILEPATH:  eval_result/codellama_baseline/2018-14085/auditor/AlfredPros_CodeLlama-7b-Instruct-Solidity_auditor_0.json
num tokens input:  1
FILEPATH:  eval_result/codellama_baseline/2018-13228/auditor/AlfredPros_CodeLlama-7b-Instruct-Solidity_auditor_0.json
num tokens input:  1
FILEPATH:  eval_result/codellama_baseline/2018-13225/auditor/AlfredPros_CodeLlama-7b-Instruct-Solidity_auditor_0.json
num tokens input:  1
FILEPATH:  eval_result/codellama_baseline/2018-13189/auditor/AlfredPros_CodeLlama-7b-Instruct-Solidity_auditor_0.json
num tokens input:  1
FILEPATH:  eval_result/codellama_baseline/2018-13126/auditor/AlfredPros_CodeLlama-7b-Instruct-Solidity_auditor_0.json
num tokens input:  1
FILEPATH:  eval_result/codellama_baseline/2018-14005/auditor/AlfredPros_CodeLlama-7b-Instruct-Solidity_auditor_0.json
num tokens input:  1
FILEPATH:  eval_result/codellama_baseline/2021-3006/auditor/AlfredPros_CodeLlama-7b-Instruct-Solidity_auditor_0.json
num tokens input:  1
FILEPATH:  eval_result/codellama_baseline/2021-34270/auditor/AlfredPros_CodeLlama-7b-Instruct-Solidity_auditor_0.json
num tokens input:  1
FILEPATH:  eval_result/codellama_baseline/2020-35962/auditor/AlfredPros_CodeLlama-7b-Instruct-Solidity_auditor_0.json
num tokens input:  1
FILEPATH:  eval_result/codellama_baseline/2018-10299/auditor/AlfredPros_CodeLlama-7b-Instruct-Solidity_auditor_0.json
num tokens input:  1
FILEPATH:  eval_result/codellama_baseline/2018-12068/auditor/AlfredPros_CodeLlama-7b-Instruct-Solidity_auditor_0.json
num tokens input:  1
FILEPATH:  eval_result/codellama_baseline/2018-12067/auditor/AlfredPros_CodeLlama-7b-Instruct-Solidity_auditor_0.json
num tokens input:  1
FILEPATH:  eval_result/codellama_baseline/2018-13071/auditor/AlfredPros_CodeLlama-7b-Instruct-Solidity_auditor_0.json
num tokens input:  1
FILEPATH:  eval_result/codellama_baseline/2018-19830/auditor/AlfredPros_CodeLlama-7b-Instruct-Solidity_auditor_0.json
num tokens input:  1
FILEPATH:  eval_result/codellama_baseline/2018-13778/auditor/AlfredPros_CodeLlama-7b-Instruct-Solidity_auditor_0.json
num tokens input:  1
FILEPATH:  eval_result/codellama_baseline/2018-11429/auditor/AlfredPros_CodeLlama-7b-Instruct-Solidity_auditor_0.json
num tokens input:  1
FILEPATH:  eval_result/codellama_baseline/2018-18425/auditor/AlfredPros_CodeLlama-7b-Instruct-Solidity_auditor_0.json
num tokens input:  1
FILEPATH:  eval_result/codellama_baseline/2018-17968/auditor/AlfredPros_CodeLlama-7b-Instruct-Solidity_auditor_0.json
num tokens input:  1
FILEPATH:  eval_result/codellama_baseline/2018-13074/auditor/AlfredPros_CodeLlama-7b-Instruct-Solidity_auditor_0.json
num tokens input:  1
FILEPATH:  eval_result/codellama_baseline/2018-13092/auditor/AlfredPros_CodeLlama-7b-Instruct-Solidity_auditor_0.json
num tokens input:  1
FILEPATH:  eval_result/codellama_baseline/2018-12078/auditor/AlfredPros_CodeLlama-7b-Instruct-Solidity_auditor_0.json
num tokens input:  1
FILEPATH:  eval_result/codellama_baseline/2018-12062/auditor/AlfredPros_CodeLlama-7b-Instruct-Solidity_auditor_0.json
num tokens input:  1
FILEPATH:  eval_result/codellama_baseline/2018-13068/auditor/AlfredPros_CodeLlama-7b-Instruct-Solidity_auditor_0.json
num tokens input:  1
FILEPATH:  eval_result/codellama_baseline/2018-10376/auditor/AlfredPros_CodeLlama-7b-Instruct-Solidity_auditor_0.json
num tokens input:  1
FILEPATH:  eval_result/codellama_baseline/2018-13073/auditor/AlfredPros_CodeLlama-7b-Instruct-Solidity_auditor_0.json
num tokens input:  1
FILEPATH:  eval_result/codellama_baseline/2018-13783/auditor/AlfredPros_CodeLlama-7b-Instruct-Solidity_auditor_0.json
num tokens input:  1
FILEPATH:  eval_result/codellama_baseline/2018-13782/auditor/AlfredPros_CodeLlama-7b-Instruct-Solidity_auditor_0.json
num tokens input:  1
FILEPATH:  eval_result/codellama_baseline/2021-34272/auditor/AlfredPros_CodeLlama-7b-Instruct-Solidity_auditor_0.json
num tokens input:  1
FILEPATH:  eval_result/codellama_baseline/2018-12703/auditor/AlfredPros_CodeLlama-7b-Instruct-Solidity_auditor_0.json
num tokens input:  1
FILEPATH:  eval_result/codellama_baseline/2018-13076/auditor/AlfredPros_CodeLlama-7b-Instruct-Solidity_auditor_0.json
num tokens input:  1
FILEPATH:  eval_result/codellama_baseline/2018-19832/auditor/AlfredPros_CodeLlama-7b-Instruct-Solidity_auditor_0.json
num tokens input:  1
Unused kwargs: ['load_in_8bit_fp32_cpu_offload']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.
2024-11-22 18:34:10,191 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:45<00:45, 45.98s/it]Loading checkpoint shards: 100%|██████████| 2/2 [01:02<00:00, 37.04s/it]Loading checkpoint shards: 100%|██████████| 2/2 [01:02<00:00, 31.08s/it]
  0%|          | 0/109 [00:00<?, ?it/s]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
  1%|          | 1/109 [00:15<27:38, 15.36s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
  2%|▏         | 2/109 [01:39<1:04:10, 35.98s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
  3%|▎         | 3/109 [02:15<1:03:46, 36.09s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
  4%|▎         | 4/109 [03:11<1:13:29, 42.00s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
all auditor summary write to :  ['eval_result/codellama_baseline/2018-12080/auditor_summary', 'eval_result/codellama_baseline/2018-13703/auditor_summary', 'eval_result/codellama_baseline/2018-13083/auditor_summary', 'eval_result/codellama_baseline/2018-12885/auditor_summary', 'eval_result/codellama_baseline/2018-13079/auditor_summary', 'eval_result/codellama_baseline/2018-14003/auditor_summary', 'eval_result/codellama_baseline/2018-12975/auditor_summary', 'eval_result/codellama_baseline/2018-13041/auditor_summary', 'eval_result/codellama_baseline/2018-13625/auditor_summary', 'eval_result/codellama_baseline/2018-13230/auditor_summary', 'eval_result/codellama_baseline/2018-13088/auditor_summary', 'eval_result/codellama_baseline/2018-13493/auditor_summary', 'eval_result/codellama_baseline/2018-12079/auditor_summary', 'eval_result/codellama_baseline/2018-17987/auditor_summary', 'eval_result/codellama_baseline/2018-17877/auditor_summary', 'eval_result/codellama_baseline/2018-13208/auditor_summary', 'eval_result/codellama_baseline/2018-12070/auditor_summary', 'eval_result/codellama_baseline/2018-12081/auditor_summary', 'eval_result/codellama_baseline/2018-12083/auditor_summary', 'eval_result/codellama_baseline/2018-14087/auditor_summary', 'eval_result/codellama_baseline/2018-14089/auditor_summary', 'eval_result/codellama_baseline/2018-13113/auditor_summary', 'eval_result/codellama_baseline/2018-13533/auditor_summary', 'eval_result/codellama_baseline/2018-17111/auditor_summary', 'eval_result/codellama_baseline/2018-10666/auditor_summary', 'eval_result/codellama_baseline/2018-19833/auditor_summary', 'eval_result/codellama_baseline/2018-11411/auditor_summary', 'eval_result/codellama_baseline/2018-12082/auditor_summary', 'eval_result/codellama_baseline/2018-12063/auditor_summary', 'eval_result/codellama_baseline/2019-15079/auditor_summary', 'eval_result/codellama_baseline/2018-12702/auditor_summary', 'eval_result/codellama_baseline/2018-12511/auditor_summary', 'eval_result/codellama_baseline/2020-17752/auditor_summary', 'eval_result/codellama_baseline/2018-13070/auditor_summary', 'eval_result/codellama_baseline/2018-13127/auditor_summary', 'eval_result/codellama_baseline/2018-11335/auditor_summary', 'eval_result/codellama_baseline/2018-14576/auditor_summary', 'eval_result/codellama_baseline/2018-12025/auditor_summary', 'eval_result/codellama_baseline/2018-17071/auditor_summary', 'eval_result/codellama_baseline/2018-13221/auditor_summary', 'eval_result/codellama_baseline/2019-15078/auditor_summary', 'eval_result/codellama_baseline/2018-14063/auditor_summary', 'eval_result/codellama_baseline/2018-13089/auditor_summary', 'eval_result/codellama_baseline/2018-13087/auditor_summary', 'eval_result/codellama_baseline/2018-10944/auditor_summary', 'eval_result/codellama_baseline/2018-12959/auditor_summary', 'eval_result/codellama_baseline/2018-13075/auditor_summary', 'eval_result/codellama_baseline/2018-19834/auditor_summary', 'eval_result/codellama_baseline/2018-13085/auditor_summary', 'eval_result/codellama_baseline/2018-17050/auditor_summary', 'eval_result/codellama_baseline/2018-13670/auditor_summary', 'eval_result/codellama_baseline/2018-13069/auditor_summary', 'eval_result/codellama_baseline/2018-13836/auditor_summary', 'eval_result/codellama_baseline/2018-13132/auditor_summary', 'eval_result/codellama_baseline/2018-13722/auditor_summary', 'eval_result/codellama_baseline/2019-15080/auditor_summary', 'eval_result/codellama_baseline/2018-13325/auditor_summary', 'eval_result/codellama_baseline/2018-13202/auditor_summary', 'eval_result/codellama_baseline/2018-11687/auditor_summary', 'eval_result/codellama_baseline/2018-13227/auditor_summary', 'eval_result/codellama_baseline/2018-14001/auditor_summary', 'eval_result/codellama_baseline/2018-13091/auditor_summary', 'eval_result/codellama_baseline/2018-13129/auditor_summary', 'eval_result/codellama_baseline/2018-10706/auditor_summary', 'eval_result/codellama_baseline/2021-34273/auditor_summary', 'eval_result/codellama_baseline/2018-14004/auditor_summary', 'eval_result/codellama_baseline/2020-17753/auditor_summary', 'eval_result/codellama_baseline/2018-13327/auditor_summary', 'eval_result/codellama_baseline/2018-11239/auditor_summary', 'eval_result/codellama_baseline/2018-12454/auditor_summary', 'eval_result/codellama_baseline/2018-13086/auditor_summary', 'eval_result/codellama_baseline/2018-14715/auditor_summary', 'eval_result/codellama_baseline/2021-3004/auditor_summary', 'eval_result/codellama_baseline/2018-13128/auditor_summary', 'eval_result/codellama_baseline/2018-14084/auditor_summary', 'eval_result/codellama_baseline/2018-14006/auditor_summary', 'eval_result/codellama_baseline/2018-12230/auditor_summary', 'eval_result/codellama_baseline/2018-13777/auditor_summary', 'eval_result/codellama_baseline/2018-14085/auditor_summary', 'eval_result/codellama_baseline/2018-13228/auditor_summary', 'eval_result/codellama_baseline/2018-13225/auditor_summary', 'eval_result/codellama_baseline/2018-13189/auditor_summary', 'eval_result/codellama_baseline/2018-13126/auditor_summary', 'eval_result/codellama_baseline/2018-14005/auditor_summary', 'eval_result/codellama_baseline/2021-3006/auditor_summary', 'eval_result/codellama_baseline/2021-34270/auditor_summary', 'eval_result/codellama_baseline/2020-35962/auditor_summary', 'eval_result/codellama_baseline/2018-10299/auditor_summary', 'eval_result/codellama_baseline/2018-12068/auditor_summary', 'eval_result/codellama_baseline/2018-12067/auditor_summary', 'eval_result/codellama_baseline/2018-13071/auditor_summary', 'eval_result/codellama_baseline/2018-19830/auditor_summary', 'eval_result/codellama_baseline/2018-13778/auditor_summary', 'eval_result/codellama_baseline/2018-11429/auditor_summary', 'eval_result/codellama_baseline/2018-18425/auditor_summary', 'eval_result/codellama_baseline/2018-17968/auditor_summary', 'eval_result/codellama_baseline/2018-13074/auditor_summary', 'eval_result/codellama_baseline/2018-13092/auditor_summary', 'eval_result/codellama_baseline/2018-12078/auditor_summary', 'eval_result/codellama_baseline/2018-12062/auditor_summary', 'eval_result/codellama_baseline/2018-13068/auditor_summary', 'eval_result/codellama_baseline/2018-10376/auditor_summary', 'eval_result/codellama_baseline/2018-13073/auditor_summary', 'eval_result/codellama_baseline/2018-13783/auditor_summary', 'eval_result/codellama_baseline/2018-13782/auditor_summary', 'eval_result/codellama_baseline/2021-34272/auditor_summary', 'eval_result/codellama_baseline/2018-12703/auditor_summary', 'eval_result/codellama_baseline/2018-13076/auditor_summary', 'eval_result/codellama_baseline/2018-19832/auditor_summary']
---- Running step: critic ----
FILEPATH:  eval_result/codellama_baseline/2018-12080/auditor_summary/AlfredPros_CodeLlama-7b-Instruct-Solidity_summarized.json
num tokens input:  1
Critic response written to :  eval_result/codellama_baseline/2018-12080/critic/AlfredPros_CodeLlama-7b-Instruct-Solidity_critic_asummarized.json
FILEPATH:  eval_result/codellama_baseline/2018-13703/auditor_summary/AlfredPros_CodeLlama-7b-Instruct-Solidity_summarized.json
num tokens input:  1
Critic response written to :  eval_result/codellama_baseline/2018-13703/critic/AlfredPros_CodeLlama-7b-Instruct-Solidity_critic_asummarized.json
FILEPATH:  eval_result/codellama_baseline/2018-13083/auditor_summary/AlfredPros_CodeLlama-7b-Instruct-Solidity_summarized.json
num tokens input:  1
Critic response written to :  eval_result/codellama_baseline/2018-13083/critic/AlfredPros_CodeLlama-7b-Instruct-Solidity_critic_asummarized.json
FILEPATH:  eval_result/codellama_baseline/2018-12885/auditor_summary/AlfredPros_CodeLlama-7b-Instruct-Solidity_summarized.json
num tokens input:  1
Critic response written to :  eval_result/codellama_baseline/2018-12885/critic/AlfredPros_CodeLlama-7b-Instruct-Solidity_critic_asummarized.json
FILEPATH:  eval_result/codellama_baseline/2018-13079/auditor_summary/AlfredPros_CodeLlama-7b-Instruct-Solidity_summarized.json
num tokens input:  1
Critic response written to :  eval_result/codellama_baseline/2018-13079/critic/AlfredPros_CodeLlama-7b-Instruct-Solidity_critic_asummarized.json
  5%|▍         | 5/109 [03:28<59:30, 34.33s/it]  The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
  6%|▌         | 6/109 [04:00<58:11, 33.90s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
  6%|▋         | 7/109 [18:44<8:11:10, 288.93s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
  7%|▋         | 8/109 [19:10<5:53:15, 209.86s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
  8%|▊         | 9/109 [19:21<4:10:33, 150.34s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
  9%|▉         | 10/109 [19:37<3:01:19, 109.89s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 10%|█         | 11/109 [20:16<2:24:56, 88.74s/it] The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 11%|█         | 12/109 [20:50<1:56:48, 72.25s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 12%|█▏        | 13/109 [21:05<1:28:21, 55.22s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 13%|█▎        | 14/109 [21:27<1:11:21, 45.07s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 14%|█▍        | 15/109 [21:55<1:02:43, 40.04s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 15%|█▍        | 16/109 [22:11<50:40, 32.69s/it]  The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 16%|█▌        | 17/109 [22:50<53:13, 34.71s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 17%|█▋        | 18/109 [23:06<43:59, 29.00s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 17%|█▋        | 19/109 [23:38<45:01, 30.02s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 18%|█▊        | 20/109 [24:18<48:45, 32.87s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 19%|█▉        | 21/109 [25:23<1:02:42, 42.75s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 20%|██        | 22/109 [25:44<52:18, 36.08s/it]  The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 21%|██        | 23/109 [26:22<52:28, 36.61s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 22%|██▏       | 24/109 [26:33<40:59, 28.93s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 23%|██▎       | 25/109 [27:04<41:33, 29.68s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 24%|██▍       | 26/109 [38:23<5:10:36, 224.54s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 25%|██▍       | 27/109 [38:56<3:47:58, 166.81s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 26%|██▌       | 28/109 [39:08<2:42:44, 120.54s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 27%|██▋       | 29/109 [39:24<1:58:57, 89.21s/it] The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 28%|██▊       | 30/109 [39:51<1:32:39, 70.37s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 28%|██▊       | 31/109 [40:38<1:22:39, 63.58s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 29%|██▉       | 32/109 [40:54<1:03:16, 49.30s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
FILEPATH:  eval_result/codellama_baseline/2018-14003/auditor_summary/AlfredPros_CodeLlama-7b-Instruct-Solidity_summarized.json
num tokens input:  1
Critic response written to :  eval_result/codellama_baseline/2018-14003/critic/AlfredPros_CodeLlama-7b-Instruct-Solidity_critic_asummarized.json
FILEPATH:  eval_result/codellama_baseline/2018-12975/auditor_summary/AlfredPros_CodeLlama-7b-Instruct-Solidity_summarized.json
num tokens input:  1
Critic response written to :  eval_result/codellama_baseline/2018-12975/critic/AlfredPros_CodeLlama-7b-Instruct-Solidity_critic_asummarized.json
FILEPATH:  eval_result/codellama_baseline/2018-13041/auditor_summary/AlfredPros_CodeLlama-7b-Instruct-Solidity_summarized.json
num tokens input:  1
Critic response written to :  eval_result/codellama_baseline/2018-13041/critic/AlfredPros_CodeLlama-7b-Instruct-Solidity_critic_asummarized.json
FILEPATH:  eval_result/codellama_baseline/2018-13625/auditor_summary/AlfredPros_CodeLlama-7b-Instruct-Solidity_summarized.json
num tokens input:  1
Critic response written to :  eval_result/codellama_baseline/2018-13625/critic/AlfredPros_CodeLlama-7b-Instruct-Solidity_critic_asummarized.json
FILEPATH:  eval_result/codellama_baseline/2018-13230/auditor_summary/AlfredPros_CodeLlama-7b-Instruct-Solidity_summarized.json
num tokens input:  1
Critic response written to :  eval_result/codellama_baseline/2018-13230/critic/AlfredPros_CodeLlama-7b-Instruct-Solidity_critic_asummarized.json
FILEPATH:  eval_result/codellama_baseline/2018-13088/auditor_summary/AlfredPros_CodeLlama-7b-Instruct-Solidity_summarized.json
num tokens input:  1
Critic response written to :  eval_result/codellama_baseline/2018-13088/critic/AlfredPros_CodeLlama-7b-Instruct-Solidity_critic_asummarized.json
FILEPATH:  eval_result/codellama_baseline/2018-13493/auditor_summary/AlfredPros_CodeLlama-7b-Instruct-Solidity_summarized.json
num tokens input:  1
Critic response written to :  eval_result/codellama_baseline/2018-13493/critic/AlfredPros_CodeLlama-7b-Instruct-Solidity_critic_asummarized.json
FILEPATH:  eval_result/codellama_baseline/2018-12079/auditor_summary/AlfredPros_CodeLlama-7b-Instruct-Solidity_summarized.json
num tokens input:  1
Critic response written to :  eval_result/codellama_baseline/2018-12079/critic/AlfredPros_CodeLlama-7b-Instruct-Solidity_critic_asummarized.json
FILEPATH:  eval_result/codellama_baseline/2018-17987/auditor_summary/AlfredPros_CodeLlama-7b-Instruct-Solidity_summarized.json
num tokens input:  1
Critic response written to :  eval_result/codellama_baseline/2018-17987/critic/AlfredPros_CodeLlama-7b-Instruct-Solidity_critic_asummarized.json
FILEPATH:  eval_result/codellama_baseline/2018-17877/auditor_summary/AlfredPros_CodeLlama-7b-Instruct-Solidity_summarized.json
num tokens input:  1
Critic response written to :  eval_result/codellama_baseline/2018-17877/critic/AlfredPros_CodeLlama-7b-Instruct-Solidity_critic_asummarized.json
FILEPATH:  eval_result/codellama_baseline/2018-13208/auditor_summary/AlfredPros_CodeLlama-7b-Instruct-Solidity_summarized.json
num tokens input:  1
Critic response written to :  eval_result/codellama_baseline/2018-13208/critic/AlfredPros_CodeLlama-7b-Instruct-Solidity_critic_asummarized.json
FILEPATH:  eval_result/codellama_baseline/2018-12070/auditor_summary/AlfredPros_CodeLlama-7b-Instruct-Solidity_summarized.json
num tokens input:  1
Critic response written to :  eval_result/codellama_baseline/2018-12070/critic/AlfredPros_CodeLlama-7b-Instruct-Solidity_critic_asummarized.json
FILEPATH:  eval_result/codellama_baseline/2018-12081/auditor_summary/AlfredPros_CodeLlama-7b-Instruct-Solidity_summarized.json
num tokens input:  1
Critic response written to :  eval_result/codellama_baseline/2018-12081/critic/AlfredPros_CodeLlama-7b-Instruct-Solidity_critic_asummarized.json
FILEPATH:  eval_result/codellama_baseline/2018-12083/auditor_summary/AlfredPros_CodeLlama-7b-Instruct-Solidity_summarized.json
num tokens input:  1
Critic response written to :  eval_result/codellama_baseline/2018-12083/critic/AlfredPros_CodeLlama-7b-Instruct-Solidity_critic_asummarized.json
FILEPATH:  eval_result/codellama_baseline/2018-14087/auditor_summary/AlfredPros_CodeLlama-7b-Instruct-Solidity_summarized.json
num tokens input:  1
Critic response written to :  eval_result/codellama_baseline/2018-14087/critic/AlfredPros_CodeLlama-7b-Instruct-Solidity_critic_asummarized.json
FILEPATH:  eval_result/codellama_baseline/2018-14089/auditor_summary/AlfredPros_CodeLlama-7b-Instruct-Solidity_summarized.json
num tokens input:  1
Critic response written to :  eval_result/codellama_baseline/2018-14089/critic/AlfredPros_CodeLlama-7b-Instruct-Solidity_critic_asummarized.json
FILEPATH:  eval_result/codellama_baseline/2018-13113/auditor_summary/AlfredPros_CodeLlama-7b-Instruct-Solidity_summarized.json
num tokens input:  1
Critic response written to :  eval_result/codellama_baseline/2018-13113/critic/AlfredPros_CodeLlama-7b-Instruct-Solidity_critic_asummarized.json
FILEPATH:  eval_result/codellama_baseline/2018-13533/auditor_summary/AlfredPros_CodeLlama-7b-Instruct-Solidity_summarized.json
num tokens input:  1
Critic response written to :  eval_result/codellama_baseline/2018-13533/critic/AlfredPros_CodeLlama-7b-Instruct-Solidity_critic_asummarized.json
FILEPATH:  eval_result/codellama_baseline/2018-17111/auditor_summary/AlfredPros_CodeLlama-7b-Instruct-Solidity_summarized.json
num tokens input:  1
Critic response written to :  eval_result/codellama_baseline/2018-17111/critic/AlfredPros_CodeLlama-7b-Instruct-Solidity_critic_asummarized.json
FILEPATH:  eval_result/codellama_baseline/2018-10666/auditor_summary/AlfredPros_CodeLlama-7b-Instruct-Solidity_summarized.json
num tokens input:  1
Critic response written to :  eval_result/codellama_baseline/2018-10666/critic/AlfredPros_CodeLlama-7b-Instruct-Solidity_critic_asummarized.json
FILEPATH:  eval_result/codellama_baseline/2018-19833/auditor_summary/AlfredPros_CodeLlama-7b-Instruct-Solidity_summarized.json
num tokens input:  1
Critic response written to :  eval_result/codellama_baseline/2018-19833/critic/AlfredPros_CodeLlama-7b-Instruct-Solidity_critic_asummarized.json
FILEPATH:  eval_result/codellama_baseline/2018-11411/auditor_summary/AlfredPros_CodeLlama-7b-Instruct-Solidity_summarized.json
num tokens input:  1
Critic response written to :  eval_result/codellama_baseline/2018-11411/critic/AlfredPros_CodeLlama-7b-Instruct-Solidity_critic_asummarized.json
FILEPATH:  eval_result/codellama_baseline/2018-12082/auditor_summary/AlfredPros_CodeLlama-7b-Instruct-Solidity_summarized.json
num tokens input:  1
Critic response written to :  eval_result/codellama_baseline/2018-12082/critic/AlfredPros_CodeLlama-7b-Instruct-Solidity_critic_asummarized.json
FILEPATH:  eval_result/codellama_baseline/2018-12063/auditor_summary/AlfredPros_CodeLlama-7b-Instruct-Solidity_summarized.json
num tokens input:  1
Critic response written to :  eval_result/codellama_baseline/2018-12063/critic/AlfredPros_CodeLlama-7b-Instruct-Solidity_critic_asummarized.json
FILEPATH:  eval_result/codellama_baseline/2019-15079/auditor_summary/AlfredPros_CodeLlama-7b-Instruct-Solidity_summarized.json
num tokens input:  1
Critic response written to :  eval_result/codellama_baseline/2019-15079/critic/AlfredPros_CodeLlama-7b-Instruct-Solidity_critic_asummarized.json
FILEPATH:  eval_result/codellama_baseline/2018-12702/auditor_summary/AlfredPros_CodeLlama-7b-Instruct-Solidity_summarized.json
num tokens input:  1
Critic response written to :  eval_result/codellama_baseline/2018-12702/critic/AlfredPros_CodeLlama-7b-Instruct-Solidity_critic_asummarized.json
FILEPATH:  eval_result/codellama_baseline/2018-12511/auditor_summary/AlfredPros_CodeLlama-7b-Instruct-Solidity_summarized.json
num tokens input:  1
Critic response written to :  eval_result/codellama_baseline/2018-12511/critic/AlfredPros_CodeLlama-7b-Instruct-Solidity_critic_asummarized.json
FILEPATH:  eval_result/codellama_baseline/2020-17752/auditor_summary/AlfredPros_CodeLlama-7b-Instruct-Solidity_summarized.json
num tokens input:  1
 30%|███       | 33/109 [41:09<49:25, 39.03s/it]  The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 31%|███       | 34/109 [41:49<49:06, 39.28s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 32%|███▏      | 35/109 [42:15<43:31, 35.29s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 33%|███▎      | 36/109 [43:02<47:09, 38.75s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 34%|███▍      | 37/109 [43:32<43:13, 36.03s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 35%|███▍      | 38/109 [44:11<43:44, 36.96s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 36%|███▌      | 39/109 [44:31<37:12, 31.89s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 37%|███▋      | 40/109 [59:18<5:31:45, 288.48s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 38%|███▊      | 41/109 [1:14:17<8:54:30, 471.63s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 39%|███▊      | 42/109 [1:14:42<6:16:55, 337.54s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 39%|███▉      | 43/109 [1:15:21<4:32:54, 248.11s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 40%|████      | 44/109 [1:16:01<3:20:57, 185.50s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 41%|████▏     | 45/109 [1:16:10<2:21:26, 132.59s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 42%|████▏     | 46/109 [1:16:32<1:44:18, 99.34s/it] The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 43%|████▎     | 47/109 [1:17:11<1:24:09, 81.44s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 44%|████▍     | 48/109 [1:18:36<1:23:51, 82.48s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 45%|████▍     | 49/109 [1:19:29<1:13:28, 73.48s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 46%|████▌     | 50/109 [1:19:37<53:06, 54.01s/it]  The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 47%|████▋     | 51/109 [1:19:52<40:41, 42.09s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 48%|████▊     | 52/109 [1:20:31<39:08, 41.21s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 49%|████▊     | 53/109 [1:21:15<39:27, 42.28s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 50%|████▉     | 54/109 [1:21:28<30:34, 33.35s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 50%|█████     | 55/109 [1:22:07<31:25, 34.92s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 51%|█████▏    | 56/109 [1:22:26<26:40, 30.19s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 52%|█████▏    | 57/109 [1:22:53<25:17, 29.19s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 53%|█████▎    | 58/109 [1:23:11<22:00, 25.90s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 54%|█████▍    | 59/109 [1:23:16<16:25, 19.71s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 55%|█████▌    | 60/109 [1:23:32<15:04, 18.47s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
Critic response written to :  eval_result/codellama_baseline/2020-17752/critic/AlfredPros_CodeLlama-7b-Instruct-Solidity_critic_asummarized.json
FILEPATH:  eval_result/codellama_baseline/2018-13070/auditor_summary/AlfredPros_CodeLlama-7b-Instruct-Solidity_summarized.json
num tokens input:  1
Critic response written to :  eval_result/codellama_baseline/2018-13070/critic/AlfredPros_CodeLlama-7b-Instruct-Solidity_critic_asummarized.json
FILEPATH:  eval_result/codellama_baseline/2018-13127/auditor_summary/AlfredPros_CodeLlama-7b-Instruct-Solidity_summarized.json
num tokens input:  1
Critic response written to :  eval_result/codellama_baseline/2018-13127/critic/AlfredPros_CodeLlama-7b-Instruct-Solidity_critic_asummarized.json
FILEPATH:  eval_result/codellama_baseline/2018-11335/auditor_summary/AlfredPros_CodeLlama-7b-Instruct-Solidity_summarized.json
num tokens input:  1
Critic response written to :  eval_result/codellama_baseline/2018-11335/critic/AlfredPros_CodeLlama-7b-Instruct-Solidity_critic_asummarized.json
FILEPATH:  eval_result/codellama_baseline/2018-14576/auditor_summary/AlfredPros_CodeLlama-7b-Instruct-Solidity_summarized.json
num tokens input:  1
Critic response written to :  eval_result/codellama_baseline/2018-14576/critic/AlfredPros_CodeLlama-7b-Instruct-Solidity_critic_asummarized.json
FILEPATH:  eval_result/codellama_baseline/2018-12025/auditor_summary/AlfredPros_CodeLlama-7b-Instruct-Solidity_summarized.json
num tokens input:  1
Critic response written to :  eval_result/codellama_baseline/2018-12025/critic/AlfredPros_CodeLlama-7b-Instruct-Solidity_critic_asummarized.json
FILEPATH:  eval_result/codellama_baseline/2018-17071/auditor_summary/AlfredPros_CodeLlama-7b-Instruct-Solidity_summarized.json
num tokens input:  1
Critic response written to :  eval_result/codellama_baseline/2018-17071/critic/AlfredPros_CodeLlama-7b-Instruct-Solidity_critic_asummarized.json
FILEPATH:  eval_result/codellama_baseline/2018-13221/auditor_summary/AlfredPros_CodeLlama-7b-Instruct-Solidity_summarized.json
num tokens input:  1
Critic response written to :  eval_result/codellama_baseline/2018-13221/critic/AlfredPros_CodeLlama-7b-Instruct-Solidity_critic_asummarized.json
FILEPATH:  eval_result/codellama_baseline/2019-15078/auditor_summary/AlfredPros_CodeLlama-7b-Instruct-Solidity_summarized.json
num tokens input:  1
Critic response written to :  eval_result/codellama_baseline/2019-15078/critic/AlfredPros_CodeLlama-7b-Instruct-Solidity_critic_asummarized.json
FILEPATH:  eval_result/codellama_baseline/2018-14063/auditor_summary/AlfredPros_CodeLlama-7b-Instruct-Solidity_summarized.json
num tokens input:  1
Critic response written to :  eval_result/codellama_baseline/2018-14063/critic/AlfredPros_CodeLlama-7b-Instruct-Solidity_critic_asummarized.json
FILEPATH:  eval_result/codellama_baseline/2018-13089/auditor_summary/AlfredPros_CodeLlama-7b-Instruct-Solidity_summarized.json
num tokens input:  1
Critic response written to :  eval_result/codellama_baseline/2018-13089/critic/AlfredPros_CodeLlama-7b-Instruct-Solidity_critic_asummarized.json
FILEPATH:  eval_result/codellama_baseline/2018-13087/auditor_summary/AlfredPros_CodeLlama-7b-Instruct-Solidity_summarized.json
num tokens input:  1
Critic response written to :  eval_result/codellama_baseline/2018-13087/critic/AlfredPros_CodeLlama-7b-Instruct-Solidity_critic_asummarized.json
FILEPATH:  eval_result/codellama_baseline/2018-10944/auditor_summary/AlfredPros_CodeLlama-7b-Instruct-Solidity_summarized.json
num tokens input:  1
Critic response written to :  eval_result/codellama_baseline/2018-10944/critic/AlfredPros_CodeLlama-7b-Instruct-Solidity_critic_asummarized.json
FILEPATH:  eval_result/codellama_baseline/2018-12959/auditor_summary/AlfredPros_CodeLlama-7b-Instruct-Solidity_summarized.json
num tokens input:  1
Critic response written to :  eval_result/codellama_baseline/2018-12959/critic/AlfredPros_CodeLlama-7b-Instruct-Solidity_critic_asummarized.json
FILEPATH:  eval_result/codellama_baseline/2018-13075/auditor_summary/AlfredPros_CodeLlama-7b-Instruct-Solidity_summarized.json
num tokens input:  1
Critic response written to :  eval_result/codellama_baseline/2018-13075/critic/AlfredPros_CodeLlama-7b-Instruct-Solidity_critic_asummarized.json
FILEPATH:  eval_result/codellama_baseline/2018-19834/auditor_summary/AlfredPros_CodeLlama-7b-Instruct-Solidity_summarized.json
num tokens input:  1
Critic response written to :  eval_result/codellama_baseline/2018-19834/critic/AlfredPros_CodeLlama-7b-Instruct-Solidity_critic_asummarized.json
FILEPATH:  eval_result/codellama_baseline/2018-13085/auditor_summary/AlfredPros_CodeLlama-7b-Instruct-Solidity_summarized.json
num tokens input:  1
Critic response written to :  eval_result/codellama_baseline/2018-13085/critic/AlfredPros_CodeLlama-7b-Instruct-Solidity_critic_asummarized.json
FILEPATH:  eval_result/codellama_baseline/2018-17050/auditor_summary/AlfredPros_CodeLlama-7b-Instruct-Solidity_summarized.json
num tokens input:  1
Critic response written to :  eval_result/codellama_baseline/2018-17050/critic/AlfredPros_CodeLlama-7b-Instruct-Solidity_critic_asummarized.json
FILEPATH:  eval_result/codellama_baseline/2018-13670/auditor_summary/AlfredPros_CodeLlama-7b-Instruct-Solidity_summarized.json
num tokens input:  1
Critic response written to :  eval_result/codellama_baseline/2018-13670/critic/AlfredPros_CodeLlama-7b-Instruct-Solidity_critic_asummarized.json
FILEPATH:  eval_result/codellama_baseline/2018-13069/auditor_summary/AlfredPros_CodeLlama-7b-Instruct-Solidity_summarized.json
num tokens input:  1
Critic response written to :  eval_result/codellama_baseline/2018-13069/critic/AlfredPros_CodeLlama-7b-Instruct-Solidity_critic_asummarized.json
FILEPATH:  eval_result/codellama_baseline/2018-13836/auditor_summary/AlfredPros_CodeLlama-7b-Instruct-Solidity_summarized.json
num tokens input:  1
Critic response written to :  eval_result/codellama_baseline/2018-13836/critic/AlfredPros_CodeLlama-7b-Instruct-Solidity_critic_asummarized.json
FILEPATH:  eval_result/codellama_baseline/2018-13132/auditor_summary/AlfredPros_CodeLlama-7b-Instruct-Solidity_summarized.json
num tokens input:  1
Critic response written to :  eval_result/codellama_baseline/2018-13132/critic/AlfredPros_CodeLlama-7b-Instruct-Solidity_critic_asummarized.json
FILEPATH:  eval_result/codellama_baseline/2018-13722/auditor_summary/AlfredPros_CodeLlama-7b-Instruct-Solidity_summarized.json
num tokens input:  1
Critic response written to :  eval_result/codellama_baseline/2018-13722/critic/AlfredPros_CodeLlama-7b-Instruct-Solidity_critic_asummarized.json
FILEPATH:  eval_result/codellama_baseline/2019-15080/auditor_summary/AlfredPros_CodeLlama-7b-Instruct-Solidity_summarized.json
num tokens input:  1
Critic response written to :  eval_result/codellama_baseline/2019-15080/critic/AlfredPros_CodeLlama-7b-Instruct-Solidity_critic_asummarized.json
FILEPATH:  eval_result/codellama_baseline/2018-13325/auditor_summary/AlfredPros_CodeLlama-7b-Instruct-Solidity_summarized.json
num tokens input:  1
Critic response written to :  eval_result/codellama_baseline/2018-13325/critic/AlfredPros_CodeLlama-7b-Instruct-Solidity_critic_asummarized.json
FILEPATH:  eval_result/codellama_baseline/2018-13202/auditor_summary/AlfredPros_CodeLlama-7b-Instruct-Solidity_summarized.json
num tokens input:  1
Critic response written to :  eval_result/codellama_baseline/2018-13202/critic/AlfredPros_CodeLlama-7b-Instruct-Solidity_critic_asummarized.json
FILEPATH:  eval_result/codellama_baseline/2018-11687/auditor_summary/AlfredPros_CodeLlama-7b-Instruct-Solidity_summarized.json
num tokens input:  1
Critic response written to :  eval_result/codellama_baseline/2018-11687/critic/AlfredPros_CodeLlama-7b-Instruct-Solidity_critic_asummarized.json
FILEPATH:  eval_result/codellama_baseline/2018-13227/auditor_summary/AlfredPros_CodeLlama-7b-Instruct-Solidity_summarized.json
num tokens input:  1
Critic response written to :  eval_result/codellama_baseline/2018-13227/critic/AlfredPros_CodeLlama-7b-Instruct-Solidity_critic_asummarized.json
FILEPATH:  eval_result/codellama_baseline/2018-14001/auditor_summary/AlfredPros_CodeLlama-7b-Instruct-Solidity_summarized.json
num tokens input:  1
 56%|█████▌    | 61/109 [1:23:54<15:49, 19.78s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 57%|█████▋    | 62/109 [1:24:43<22:11, 28.32s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 58%|█████▊    | 63/109 [1:25:28<25:39, 33.46s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 59%|█████▊    | 64/109 [1:26:26<30:28, 40.64s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 60%|█████▉    | 65/109 [1:26:47<25:36, 34.91s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 61%|██████    | 66/109 [1:27:30<26:50, 37.45s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 61%|██████▏   | 67/109 [1:28:18<28:14, 40.35s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 62%|██████▏   | 68/109 [1:28:55<26:58, 39.47s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 63%|██████▎   | 69/109 [1:29:16<22:37, 33.93s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 64%|██████▍   | 70/109 [1:29:21<16:23, 25.21s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 65%|██████▌   | 71/109 [1:30:00<18:41, 29.51s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 66%|██████▌   | 72/109 [1:30:40<20:03, 32.53s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 67%|██████▋   | 73/109 [1:46:21<3:03:03, 305.09s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 68%|██████▊   | 74/109 [1:47:05<2:12:15, 226.72s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 69%|██████▉   | 75/109 [1:47:43<1:36:21, 170.05s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 70%|██████▉   | 76/109 [1:48:11<1:10:11, 127.63s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 71%|███████   | 77/109 [1:48:32<50:55, 95.49s/it]   The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 72%|███████▏  | 78/109 [1:48:59<38:44, 74.98s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 72%|███████▏  | 79/109 [1:49:12<28:12, 56.40s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 73%|███████▎  | 80/109 [1:49:27<21:16, 44.01s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 74%|███████▍  | 81/109 [1:49:51<17:46, 38.08s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 75%|███████▌  | 82/109 [1:50:12<14:42, 32.69s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 76%|███████▌  | 83/109 [1:50:21<11:05, 25.59s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 77%|███████▋  | 84/109 [1:50:56<11:53, 28.53s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 78%|███████▊  | 85/109 [1:51:00<08:30, 21.26s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 79%|███████▉  | 86/109 [1:51:27<08:48, 22.98s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 80%|███████▉  | 87/109 [1:51:32<06:22, 17.37s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 81%|████████  | 88/109 [1:52:06<07:53, 22.53s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
Critic response written to :  eval_result/codellama_baseline/2018-14001/critic/AlfredPros_CodeLlama-7b-Instruct-Solidity_critic_asummarized.json
FILEPATH:  eval_result/codellama_baseline/2018-13091/auditor_summary/AlfredPros_CodeLlama-7b-Instruct-Solidity_summarized.json
num tokens input:  1
Critic response written to :  eval_result/codellama_baseline/2018-13091/critic/AlfredPros_CodeLlama-7b-Instruct-Solidity_critic_asummarized.json
FILEPATH:  eval_result/codellama_baseline/2018-13129/auditor_summary/AlfredPros_CodeLlama-7b-Instruct-Solidity_summarized.json
num tokens input:  1
Critic response written to :  eval_result/codellama_baseline/2018-13129/critic/AlfredPros_CodeLlama-7b-Instruct-Solidity_critic_asummarized.json
FILEPATH:  eval_result/codellama_baseline/2018-10706/auditor_summary/AlfredPros_CodeLlama-7b-Instruct-Solidity_summarized.json
num tokens input:  1
Critic response written to :  eval_result/codellama_baseline/2018-10706/critic/AlfredPros_CodeLlama-7b-Instruct-Solidity_critic_asummarized.json
FILEPATH:  eval_result/codellama_baseline/2021-34273/auditor_summary/AlfredPros_CodeLlama-7b-Instruct-Solidity_summarized.json
num tokens input:  1
Critic response written to :  eval_result/codellama_baseline/2021-34273/critic/AlfredPros_CodeLlama-7b-Instruct-Solidity_critic_asummarized.json
FILEPATH:  eval_result/codellama_baseline/2018-14004/auditor_summary/AlfredPros_CodeLlama-7b-Instruct-Solidity_summarized.json
num tokens input:  1
Critic response written to :  eval_result/codellama_baseline/2018-14004/critic/AlfredPros_CodeLlama-7b-Instruct-Solidity_critic_asummarized.json
FILEPATH:  eval_result/codellama_baseline/2020-17753/auditor_summary/AlfredPros_CodeLlama-7b-Instruct-Solidity_summarized.json
num tokens input:  1
Critic response written to :  eval_result/codellama_baseline/2020-17753/critic/AlfredPros_CodeLlama-7b-Instruct-Solidity_critic_asummarized.json
FILEPATH:  eval_result/codellama_baseline/2018-13327/auditor_summary/AlfredPros_CodeLlama-7b-Instruct-Solidity_summarized.json
num tokens input:  1
Critic response written to :  eval_result/codellama_baseline/2018-13327/critic/AlfredPros_CodeLlama-7b-Instruct-Solidity_critic_asummarized.json
FILEPATH:  eval_result/codellama_baseline/2018-11239/auditor_summary/AlfredPros_CodeLlama-7b-Instruct-Solidity_summarized.json
num tokens input:  1
Critic response written to :  eval_result/codellama_baseline/2018-11239/critic/AlfredPros_CodeLlama-7b-Instruct-Solidity_critic_asummarized.json
FILEPATH:  eval_result/codellama_baseline/2018-12454/auditor_summary/AlfredPros_CodeLlama-7b-Instruct-Solidity_summarized.json
num tokens input:  1
Critic response written to :  eval_result/codellama_baseline/2018-12454/critic/AlfredPros_CodeLlama-7b-Instruct-Solidity_critic_asummarized.json
FILEPATH:  eval_result/codellama_baseline/2018-13086/auditor_summary/AlfredPros_CodeLlama-7b-Instruct-Solidity_summarized.json
num tokens input:  1
Critic response written to :  eval_result/codellama_baseline/2018-13086/critic/AlfredPros_CodeLlama-7b-Instruct-Solidity_critic_asummarized.json
FILEPATH:  eval_result/codellama_baseline/2018-14715/auditor_summary/AlfredPros_CodeLlama-7b-Instruct-Solidity_summarized.json
num tokens input:  1
Critic response written to :  eval_result/codellama_baseline/2018-14715/critic/AlfredPros_CodeLlama-7b-Instruct-Solidity_critic_asummarized.json
FILEPATH:  eval_result/codellama_baseline/2021-3004/auditor_summary/AlfredPros_CodeLlama-7b-Instruct-Solidity_summarized.json
num tokens input:  1
Critic response written to :  eval_result/codellama_baseline/2021-3004/critic/AlfredPros_CodeLlama-7b-Instruct-Solidity_critic_asummarized.json
FILEPATH:  eval_result/codellama_baseline/2018-13128/auditor_summary/AlfredPros_CodeLlama-7b-Instruct-Solidity_summarized.json
num tokens input:  1
Critic response written to :  eval_result/codellama_baseline/2018-13128/critic/AlfredPros_CodeLlama-7b-Instruct-Solidity_critic_asummarized.json
FILEPATH:  eval_result/codellama_baseline/2018-14084/auditor_summary/AlfredPros_CodeLlama-7b-Instruct-Solidity_summarized.json
num tokens input:  1
Critic response written to :  eval_result/codellama_baseline/2018-14084/critic/AlfredPros_CodeLlama-7b-Instruct-Solidity_critic_asummarized.json
FILEPATH:  eval_result/codellama_baseline/2018-14006/auditor_summary/AlfredPros_CodeLlama-7b-Instruct-Solidity_summarized.json
num tokens input:  1
Critic response written to :  eval_result/codellama_baseline/2018-14006/critic/AlfredPros_CodeLlama-7b-Instruct-Solidity_critic_asummarized.json
FILEPATH:  eval_result/codellama_baseline/2018-12230/auditor_summary/AlfredPros_CodeLlama-7b-Instruct-Solidity_summarized.json
num tokens input:  1
Critic response written to :  eval_result/codellama_baseline/2018-12230/critic/AlfredPros_CodeLlama-7b-Instruct-Solidity_critic_asummarized.json
FILEPATH:  eval_result/codellama_baseline/2018-13777/auditor_summary/AlfredPros_CodeLlama-7b-Instruct-Solidity_summarized.json
num tokens input:  1
Critic response written to :  eval_result/codellama_baseline/2018-13777/critic/AlfredPros_CodeLlama-7b-Instruct-Solidity_critic_asummarized.json
FILEPATH:  eval_result/codellama_baseline/2018-14085/auditor_summary/AlfredPros_CodeLlama-7b-Instruct-Solidity_summarized.json
num tokens input:  1
Critic response written to :  eval_result/codellama_baseline/2018-14085/critic/AlfredPros_CodeLlama-7b-Instruct-Solidity_critic_asummarized.json
FILEPATH:  eval_result/codellama_baseline/2018-13228/auditor_summary/AlfredPros_CodeLlama-7b-Instruct-Solidity_summarized.json
num tokens input:  1
Critic response written to :  eval_result/codellama_baseline/2018-13228/critic/AlfredPros_CodeLlama-7b-Instruct-Solidity_critic_asummarized.json
FILEPATH:  eval_result/codellama_baseline/2018-13225/auditor_summary/AlfredPros_CodeLlama-7b-Instruct-Solidity_summarized.json
num tokens input:  1
Critic response written to :  eval_result/codellama_baseline/2018-13225/critic/AlfredPros_CodeLlama-7b-Instruct-Solidity_critic_asummarized.json
FILEPATH:  eval_result/codellama_baseline/2018-13189/auditor_summary/AlfredPros_CodeLlama-7b-Instruct-Solidity_summarized.json
num tokens input:  1
Critic response written to :  eval_result/codellama_baseline/2018-13189/critic/AlfredPros_CodeLlama-7b-Instruct-Solidity_critic_asummarized.json
FILEPATH:  eval_result/codellama_baseline/2018-13126/auditor_summary/AlfredPros_CodeLlama-7b-Instruct-Solidity_summarized.json
num tokens input:  1
Critic response written to :  eval_result/codellama_baseline/2018-13126/critic/AlfredPros_CodeLlama-7b-Instruct-Solidity_critic_asummarized.json
FILEPATH:  eval_result/codellama_baseline/2018-14005/auditor_summary/AlfredPros_CodeLlama-7b-Instruct-Solidity_summarized.json
num tokens input:  1
Critic response written to :  eval_result/codellama_baseline/2018-14005/critic/AlfredPros_CodeLlama-7b-Instruct-Solidity_critic_asummarized.json
FILEPATH:  eval_result/codellama_baseline/2021-3006/auditor_summary/AlfredPros_CodeLlama-7b-Instruct-Solidity_summarized.json
num tokens input:  1
Critic response written to :  eval_result/codellama_baseline/2021-3006/critic/AlfredPros_CodeLlama-7b-Instruct-Solidity_critic_asummarized.json
FILEPATH:  eval_result/codellama_baseline/2021-34270/auditor_summary/AlfredPros_CodeLlama-7b-Instruct-Solidity_summarized.json
num tokens input:  1
Critic response written to :  eval_result/codellama_baseline/2021-34270/critic/AlfredPros_CodeLlama-7b-Instruct-Solidity_critic_asummarized.json
FILEPATH:  eval_result/codellama_baseline/2020-35962/auditor_summary/AlfredPros_CodeLlama-7b-Instruct-Solidity_summarized.json
num tokens input:  1
Critic response written to :  eval_result/codellama_baseline/2020-35962/critic/AlfredPros_CodeLlama-7b-Instruct-Solidity_critic_asummarized.json
FILEPATH:  eval_result/codellama_baseline/2018-10299/auditor_summary/AlfredPros_CodeLlama-7b-Instruct-Solidity_summarized.json
num tokens input:  1
Critic response written to :  eval_result/codellama_baseline/2018-10299/critic/AlfredPros_CodeLlama-7b-Instruct-Solidity_critic_asummarized.json
FILEPATH:  eval_result/codellama_baseline/2018-12068/auditor_summary/AlfredPros_CodeLlama-7b-Instruct-Solidity_summarized.json
num tokens input:  1
 82%|████████▏ | 89/109 [1:52:21<06:45, 20.28s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 83%|████████▎ | 90/109 [1:52:36<05:55, 18.72s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 83%|████████▎ | 91/109 [1:53:05<06:29, 21.64s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 84%|████████▍ | 92/109 [1:53:28<06:16, 22.14s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 85%|████████▌ | 93/109 [1:54:04<07:01, 26.36s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 86%|████████▌ | 94/109 [1:54:31<06:37, 26.51s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 87%|████████▋ | 95/109 [1:55:03<06:35, 28.23s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 88%|████████▊ | 96/109 [1:55:37<06:30, 30.01s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 89%|████████▉ | 97/109 [1:55:53<05:09, 25.83s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 90%|████████▉ | 98/109 [1:56:04<03:54, 21.31s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 91%|█████████ | 99/109 [1:56:13<02:54, 17.41s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 92%|█████████▏| 100/109 [1:56:27<02:30, 16.67s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 93%|█████████▎| 101/109 [1:56:55<02:40, 20.01s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 94%|█████████▎| 102/109 [1:57:03<01:54, 16.33s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 94%|█████████▍| 103/109 [1:57:21<01:41, 16.90s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 95%|█████████▌| 104/109 [1:57:56<01:51, 22.39s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 96%|█████████▋| 105/109 [1:58:11<01:20, 20.17s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 97%|█████████▋| 106/109 [1:58:43<01:10, 23.46s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 98%|█████████▊| 107/109 [1:59:29<01:00, 30.26s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 99%|█████████▉| 108/109 [1:59:41<00:24, 24.73s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
100%|██████████| 109/109 [2:00:04<00:00, 24.42s/it]100%|██████████| 109/109 [2:00:04<00:00, 66.10s/it]
Critic response written to :  eval_result/codellama_baseline/2018-12068/critic/AlfredPros_CodeLlama-7b-Instruct-Solidity_critic_asummarized.json
FILEPATH:  eval_result/codellama_baseline/2018-12067/auditor_summary/AlfredPros_CodeLlama-7b-Instruct-Solidity_summarized.json
num tokens input:  1
Critic response written to :  eval_result/codellama_baseline/2018-12067/critic/AlfredPros_CodeLlama-7b-Instruct-Solidity_critic_asummarized.json
FILEPATH:  eval_result/codellama_baseline/2018-13071/auditor_summary/AlfredPros_CodeLlama-7b-Instruct-Solidity_summarized.json
num tokens input:  1
Critic response written to :  eval_result/codellama_baseline/2018-13071/critic/AlfredPros_CodeLlama-7b-Instruct-Solidity_critic_asummarized.json
FILEPATH:  eval_result/codellama_baseline/2018-19830/auditor_summary/AlfredPros_CodeLlama-7b-Instruct-Solidity_summarized.json
num tokens input:  1
Critic response written to :  eval_result/codellama_baseline/2018-19830/critic/AlfredPros_CodeLlama-7b-Instruct-Solidity_critic_asummarized.json
FILEPATH:  eval_result/codellama_baseline/2018-13778/auditor_summary/AlfredPros_CodeLlama-7b-Instruct-Solidity_summarized.json
num tokens input:  1
Critic response written to :  eval_result/codellama_baseline/2018-13778/critic/AlfredPros_CodeLlama-7b-Instruct-Solidity_critic_asummarized.json
FILEPATH:  eval_result/codellama_baseline/2018-11429/auditor_summary/AlfredPros_CodeLlama-7b-Instruct-Solidity_summarized.json
num tokens input:  1
Critic response written to :  eval_result/codellama_baseline/2018-11429/critic/AlfredPros_CodeLlama-7b-Instruct-Solidity_critic_asummarized.json
FILEPATH:  eval_result/codellama_baseline/2018-18425/auditor_summary/AlfredPros_CodeLlama-7b-Instruct-Solidity_summarized.json
num tokens input:  1
Critic response written to :  eval_result/codellama_baseline/2018-18425/critic/AlfredPros_CodeLlama-7b-Instruct-Solidity_critic_asummarized.json
FILEPATH:  eval_result/codellama_baseline/2018-17968/auditor_summary/AlfredPros_CodeLlama-7b-Instruct-Solidity_summarized.json
num tokens input:  1
Critic response written to :  eval_result/codellama_baseline/2018-17968/critic/AlfredPros_CodeLlama-7b-Instruct-Solidity_critic_asummarized.json
FILEPATH:  eval_result/codellama_baseline/2018-13074/auditor_summary/AlfredPros_CodeLlama-7b-Instruct-Solidity_summarized.json
num tokens input:  1
Critic response written to :  eval_result/codellama_baseline/2018-13074/critic/AlfredPros_CodeLlama-7b-Instruct-Solidity_critic_asummarized.json
FILEPATH:  eval_result/codellama_baseline/2018-13092/auditor_summary/AlfredPros_CodeLlama-7b-Instruct-Solidity_summarized.json
num tokens input:  1
Critic response written to :  eval_result/codellama_baseline/2018-13092/critic/AlfredPros_CodeLlama-7b-Instruct-Solidity_critic_asummarized.json
FILEPATH:  eval_result/codellama_baseline/2018-12078/auditor_summary/AlfredPros_CodeLlama-7b-Instruct-Solidity_summarized.json
num tokens input:  1
Critic response written to :  eval_result/codellama_baseline/2018-12078/critic/AlfredPros_CodeLlama-7b-Instruct-Solidity_critic_asummarized.json
FILEPATH:  eval_result/codellama_baseline/2018-12062/auditor_summary/AlfredPros_CodeLlama-7b-Instruct-Solidity_summarized.json
num tokens input:  1
Critic response written to :  eval_result/codellama_baseline/2018-12062/critic/AlfredPros_CodeLlama-7b-Instruct-Solidity_critic_asummarized.json
FILEPATH:  eval_result/codellama_baseline/2018-13068/auditor_summary/AlfredPros_CodeLlama-7b-Instruct-Solidity_summarized.json
num tokens input:  1
Critic response written to :  eval_result/codellama_baseline/2018-13068/critic/AlfredPros_CodeLlama-7b-Instruct-Solidity_critic_asummarized.json
FILEPATH:  eval_result/codellama_baseline/2018-10376/auditor_summary/AlfredPros_CodeLlama-7b-Instruct-Solidity_summarized.json
num tokens input:  1
Critic response written to :  eval_result/codellama_baseline/2018-10376/critic/AlfredPros_CodeLlama-7b-Instruct-Solidity_critic_asummarized.json
FILEPATH:  eval_result/codellama_baseline/2018-13073/auditor_summary/AlfredPros_CodeLlama-7b-Instruct-Solidity_summarized.json
num tokens input:  1
Critic response written to :  eval_result/codellama_baseline/2018-13073/critic/AlfredPros_CodeLlama-7b-Instruct-Solidity_critic_asummarized.json
FILEPATH:  eval_result/codellama_baseline/2018-13783/auditor_summary/AlfredPros_CodeLlama-7b-Instruct-Solidity_summarized.json
num tokens input:  1
Critic response written to :  eval_result/codellama_baseline/2018-13783/critic/AlfredPros_CodeLlama-7b-Instruct-Solidity_critic_asummarized.json
FILEPATH:  eval_result/codellama_baseline/2018-13782/auditor_summary/AlfredPros_CodeLlama-7b-Instruct-Solidity_summarized.json
num tokens input:  1
Critic response written to :  eval_result/codellama_baseline/2018-13782/critic/AlfredPros_CodeLlama-7b-Instruct-Solidity_critic_asummarized.json
FILEPATH:  eval_result/codellama_baseline/2021-34272/auditor_summary/AlfredPros_CodeLlama-7b-Instruct-Solidity_summarized.json
num tokens input:  1
Critic response written to :  eval_result/codellama_baseline/2021-34272/critic/AlfredPros_CodeLlama-7b-Instruct-Solidity_critic_asummarized.json
FILEPATH:  eval_result/codellama_baseline/2018-12703/auditor_summary/AlfredPros_CodeLlama-7b-Instruct-Solidity_summarized.json
num tokens input:  1
Critic response written to :  eval_result/codellama_baseline/2018-12703/critic/AlfredPros_CodeLlama-7b-Instruct-Solidity_critic_asummarized.json
FILEPATH:  eval_result/codellama_baseline/2018-13076/auditor_summary/AlfredPros_CodeLlama-7b-Instruct-Solidity_summarized.json
num tokens input:  1
Critic response written to :  eval_result/codellama_baseline/2018-13076/critic/AlfredPros_CodeLlama-7b-Instruct-Solidity_critic_asummarized.json
FILEPATH:  eval_result/codellama_baseline/2018-19832/auditor_summary/AlfredPros_CodeLlama-7b-Instruct-Solidity_summarized.json
num tokens input:  1
Critic response written to :  eval_result/codellama_baseline/2018-19832/critic/AlfredPros_CodeLlama-7b-Instruct-Solidity_critic_asummarized.json
  0%|          | 0/109 [00:00<?, ?it/s]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
  1%|          | 1/109 [00:15<27:09, 15.09s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
  2%|▏         | 2/109 [01:37<1:03:11, 35.43s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
  3%|▎         | 3/109 [01:58<54:36, 30.91s/it]  The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
  4%|▎         | 4/109 [02:53<1:06:56, 38.25s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
  5%|▍         | 5/109 [03:10<54:56, 31.70s/it]  The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
  6%|▌         | 6/109 [03:36<51:38, 30.08s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
  6%|▋         | 7/109 [18:17<8:05:05, 285.34s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
  7%|▋         | 8/109 [18:31<5:43:25, 204.02s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
  8%|▊         | 9/109 [18:43<4:03:54, 146.34s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
  9%|▉         | 10/109 [18:58<2:56:25, 106.92s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 10%|█         | 11/109 [19:16<2:10:53, 80.14s/it] The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 11%|█         | 12/109 [19:33<1:39:04, 61.28s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 12%|█▏        | 13/109 [19:48<1:15:48, 47.38s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 13%|█▎        | 14/109 [20:09<1:02:37, 39.55s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 14%|█▍        | 15/109 [20:36<56:15, 35.91s/it]  The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 15%|█▍        | 16/109 [20:51<45:54, 29.61s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
all critic write to :  ['eval_result/codellama_baseline/2018-12080/critic', 'eval_result/codellama_baseline/2018-13703/critic', 'eval_result/codellama_baseline/2018-13083/critic', 'eval_result/codellama_baseline/2018-12885/critic', 'eval_result/codellama_baseline/2018-13079/critic', 'eval_result/codellama_baseline/2018-14003/critic', 'eval_result/codellama_baseline/2018-12975/critic', 'eval_result/codellama_baseline/2018-13041/critic', 'eval_result/codellama_baseline/2018-13625/critic', 'eval_result/codellama_baseline/2018-13230/critic', 'eval_result/codellama_baseline/2018-13088/critic', 'eval_result/codellama_baseline/2018-13493/critic', 'eval_result/codellama_baseline/2018-12079/critic', 'eval_result/codellama_baseline/2018-17987/critic', 'eval_result/codellama_baseline/2018-17877/critic', 'eval_result/codellama_baseline/2018-13208/critic', 'eval_result/codellama_baseline/2018-12070/critic', 'eval_result/codellama_baseline/2018-12081/critic', 'eval_result/codellama_baseline/2018-12083/critic', 'eval_result/codellama_baseline/2018-14087/critic', 'eval_result/codellama_baseline/2018-14089/critic', 'eval_result/codellama_baseline/2018-13113/critic', 'eval_result/codellama_baseline/2018-13533/critic', 'eval_result/codellama_baseline/2018-17111/critic', 'eval_result/codellama_baseline/2018-10666/critic', 'eval_result/codellama_baseline/2018-19833/critic', 'eval_result/codellama_baseline/2018-11411/critic', 'eval_result/codellama_baseline/2018-12082/critic', 'eval_result/codellama_baseline/2018-12063/critic', 'eval_result/codellama_baseline/2019-15079/critic', 'eval_result/codellama_baseline/2018-12702/critic', 'eval_result/codellama_baseline/2018-12511/critic', 'eval_result/codellama_baseline/2020-17752/critic', 'eval_result/codellama_baseline/2018-13070/critic', 'eval_result/codellama_baseline/2018-13127/critic', 'eval_result/codellama_baseline/2018-11335/critic', 'eval_result/codellama_baseline/2018-14576/critic', 'eval_result/codellama_baseline/2018-12025/critic', 'eval_result/codellama_baseline/2018-17071/critic', 'eval_result/codellama_baseline/2018-13221/critic', 'eval_result/codellama_baseline/2019-15078/critic', 'eval_result/codellama_baseline/2018-14063/critic', 'eval_result/codellama_baseline/2018-13089/critic', 'eval_result/codellama_baseline/2018-13087/critic', 'eval_result/codellama_baseline/2018-10944/critic', 'eval_result/codellama_baseline/2018-12959/critic', 'eval_result/codellama_baseline/2018-13075/critic', 'eval_result/codellama_baseline/2018-19834/critic', 'eval_result/codellama_baseline/2018-13085/critic', 'eval_result/codellama_baseline/2018-17050/critic', 'eval_result/codellama_baseline/2018-13670/critic', 'eval_result/codellama_baseline/2018-13069/critic', 'eval_result/codellama_baseline/2018-13836/critic', 'eval_result/codellama_baseline/2018-13132/critic', 'eval_result/codellama_baseline/2018-13722/critic', 'eval_result/codellama_baseline/2019-15080/critic', 'eval_result/codellama_baseline/2018-13325/critic', 'eval_result/codellama_baseline/2018-13202/critic', 'eval_result/codellama_baseline/2018-11687/critic', 'eval_result/codellama_baseline/2018-13227/critic', 'eval_result/codellama_baseline/2018-14001/critic', 'eval_result/codellama_baseline/2018-13091/critic', 'eval_result/codellama_baseline/2018-13129/critic', 'eval_result/codellama_baseline/2018-10706/critic', 'eval_result/codellama_baseline/2021-34273/critic', 'eval_result/codellama_baseline/2018-14004/critic', 'eval_result/codellama_baseline/2020-17753/critic', 'eval_result/codellama_baseline/2018-13327/critic', 'eval_result/codellama_baseline/2018-11239/critic', 'eval_result/codellama_baseline/2018-12454/critic', 'eval_result/codellama_baseline/2018-13086/critic', 'eval_result/codellama_baseline/2018-14715/critic', 'eval_result/codellama_baseline/2021-3004/critic', 'eval_result/codellama_baseline/2018-13128/critic', 'eval_result/codellama_baseline/2018-14084/critic', 'eval_result/codellama_baseline/2018-14006/critic', 'eval_result/codellama_baseline/2018-12230/critic', 'eval_result/codellama_baseline/2018-13777/critic', 'eval_result/codellama_baseline/2018-14085/critic', 'eval_result/codellama_baseline/2018-13228/critic', 'eval_result/codellama_baseline/2018-13225/critic', 'eval_result/codellama_baseline/2018-13189/critic', 'eval_result/codellama_baseline/2018-13126/critic', 'eval_result/codellama_baseline/2018-14005/critic', 'eval_result/codellama_baseline/2021-3006/critic', 'eval_result/codellama_baseline/2021-34270/critic', 'eval_result/codellama_baseline/2020-35962/critic', 'eval_result/codellama_baseline/2018-10299/critic', 'eval_result/codellama_baseline/2018-12068/critic', 'eval_result/codellama_baseline/2018-12067/critic', 'eval_result/codellama_baseline/2018-13071/critic', 'eval_result/codellama_baseline/2018-19830/critic', 'eval_result/codellama_baseline/2018-13778/critic', 'eval_result/codellama_baseline/2018-11429/critic', 'eval_result/codellama_baseline/2018-18425/critic', 'eval_result/codellama_baseline/2018-17968/critic', 'eval_result/codellama_baseline/2018-13074/critic', 'eval_result/codellama_baseline/2018-13092/critic', 'eval_result/codellama_baseline/2018-12078/critic', 'eval_result/codellama_baseline/2018-12062/critic', 'eval_result/codellama_baseline/2018-13068/critic', 'eval_result/codellama_baseline/2018-10376/critic', 'eval_result/codellama_baseline/2018-13073/critic', 'eval_result/codellama_baseline/2018-13783/critic', 'eval_result/codellama_baseline/2018-13782/critic', 'eval_result/codellama_baseline/2021-34272/critic', 'eval_result/codellama_baseline/2018-12703/critic', 'eval_result/codellama_baseline/2018-13076/critic', 'eval_result/codellama_baseline/2018-19832/critic']
---- Running step: critic_summary ----
FILEPATH:  eval_result/codellama_baseline/2018-12080/critic/AlfredPros_CodeLlama-7b-Instruct-Solidity_critic_summarized.json
num tokens input:  1
FILEPATH:  eval_result/codellama_baseline/2018-13703/critic/AlfredPros_CodeLlama-7b-Instruct-Solidity_critic_summarized.json
num tokens input:  1
FILEPATH:  eval_result/codellama_baseline/2018-13083/critic/AlfredPros_CodeLlama-7b-Instruct-Solidity_critic_summarized.json
num tokens input:  1
FILEPATH:  eval_result/codellama_baseline/2018-12885/critic/AlfredPros_CodeLlama-7b-Instruct-Solidity_critic_summarized.json
num tokens input:  1
FILEPATH:  eval_result/codellama_baseline/2018-13079/critic/AlfredPros_CodeLlama-7b-Instruct-Solidity_critic_summarized.json
num tokens input:  1
FILEPATH:  eval_result/codellama_baseline/2018-14003/critic/AlfredPros_CodeLlama-7b-Instruct-Solidity_critic_summarized.json
num tokens input:  1
FILEPATH:  eval_result/codellama_baseline/2018-12975/critic/AlfredPros_CodeLlama-7b-Instruct-Solidity_critic_summarized.json
num tokens input:  1
FILEPATH:  eval_result/codellama_baseline/2018-13041/critic/AlfredPros_CodeLlama-7b-Instruct-Solidity_critic_summarized.json
num tokens input:  1
FILEPATH:  eval_result/codellama_baseline/2018-13625/critic/AlfredPros_CodeLlama-7b-Instruct-Solidity_critic_summarized.json
num tokens input:  1
FILEPATH:  eval_result/codellama_baseline/2018-13230/critic/AlfredPros_CodeLlama-7b-Instruct-Solidity_critic_summarized.json
num tokens input:  1
FILEPATH:  eval_result/codellama_baseline/2018-13088/critic/AlfredPros_CodeLlama-7b-Instruct-Solidity_critic_summarized.json
num tokens input:  1
FILEPATH:  eval_result/codellama_baseline/2018-13493/critic/AlfredPros_CodeLlama-7b-Instruct-Solidity_critic_summarized.json
num tokens input:  1
FILEPATH:  eval_result/codellama_baseline/2018-12079/critic/AlfredPros_CodeLlama-7b-Instruct-Solidity_critic_summarized.json
num tokens input:  1
FILEPATH:  eval_result/codellama_baseline/2018-17987/critic/AlfredPros_CodeLlama-7b-Instruct-Solidity_critic_summarized.json
num tokens input:  1
FILEPATH:  eval_result/codellama_baseline/2018-17877/critic/AlfredPros_CodeLlama-7b-Instruct-Solidity_critic_summarized.json
num tokens input:  1
FILEPATH:  eval_result/codellama_baseline/2018-13208/critic/AlfredPros_CodeLlama-7b-Instruct-Solidity_critic_summarized.json
num tokens input:  1
FILEPATH:  eval_result/codellama_baseline/2018-12070/critic/AlfredPros_CodeLlama-7b-Instruct-Solidity_critic_summarized.json
num tokens input:  1
 16%|█▌        | 17/109 [21:09<39:55, 26.04s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 17%|█▋        | 18/109 [21:24<34:27, 22.72s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 17%|█▋        | 19/109 [21:40<30:49, 20.55s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 18%|█▊        | 20/109 [21:57<29:09, 19.66s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 19%|█▉        | 21/109 [22:40<38:50, 26.49s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 20%|██        | 22/109 [23:00<35:34, 24.54s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 21%|██        | 23/109 [23:35<40:02, 27.94s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 22%|██▏       | 24/109 [23:46<32:09, 22.69s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 23%|██▎       | 25/109 [24:16<34:44, 24.82s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 24%|██▍       | 26/109 [44:16<8:41:59, 377.35s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 25%|██▍       | 27/109 [44:27<6:05:29, 267.43s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 26%|██▌       | 28/109 [44:38<4:17:30, 190.75s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 27%|██▋       | 29/109 [44:53<3:04:04, 138.06s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 28%|██▊       | 30/109 [45:06<2:12:05, 100.32s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 28%|██▊       | 31/109 [45:25<1:38:38, 75.88s/it] The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 29%|██▉       | 32/109 [45:40<1:13:58, 57.65s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 30%|███       | 33/109 [45:54<56:30, 44.61s/it]  The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 31%|███       | 34/109 [46:32<53:10, 42.54s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 32%|███▏      | 35/109 [46:56<45:44, 37.08s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 33%|███▎      | 36/109 [47:22<40:56, 33.65s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 34%|███▍      | 37/109 [47:50<38:36, 32.18s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 35%|███▍      | 38/109 [48:28<40:12, 33.98s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 36%|███▌      | 39/109 [48:36<30:31, 26.17s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 37%|███▋      | 40/109 [1:03:17<5:25:01, 282.63s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 38%|███▊      | 41/109 [1:18:37<8:56:55, 473.76s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 39%|███▊      | 42/109 [1:18:51<6:14:52, 335.72s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 39%|███▉      | 43/109 [1:19:08<4:24:20, 240.30s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 40%|████      | 44/109 [1:19:26<3:07:57, 173.50s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 41%|████▏     | 45/109 [1:19:35<2:12:23, 124.12s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 42%|████▏     | 46/109 [1:19:56<1:37:48, 93.16s/it] The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 43%|████▎     | 47/109 [1:20:14<1:12:51, 70.51s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 44%|████▍     | 48/109 [1:21:06<1:06:09, 65.08s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 45%|████▍     | 49/109 [1:21:36<54:25, 54.42s/it]  The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 46%|████▌     | 50/109 [1:21:44<39:55, 40.60s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 47%|████▋     | 51/109 [1:21:51<29:30, 30.53s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 48%|████▊     | 52/109 [1:22:08<25:18, 26.64s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 49%|████▊     | 53/109 [1:22:30<23:17, 24.95s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 50%|████▉     | 54/109 [1:22:42<19:33, 21.34s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 50%|█████     | 55/109 [1:22:59<17:59, 19.99s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 51%|█████▏    | 56/109 [1:23:18<17:17, 19.58s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 52%|█████▏    | 57/109 [1:23:44<18:40, 21.55s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 53%|█████▎    | 58/109 [1:24:02<17:19, 20.39s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 54%|█████▍    | 59/109 [1:24:07<13:10, 15.81s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 55%|█████▌    | 60/109 [1:24:22<12:42, 15.57s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 56%|█████▌    | 61/109 [1:24:44<14:00, 17.51s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 57%|█████▋    | 62/109 [1:25:11<15:59, 20.42s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 58%|█████▊    | 63/109 [1:25:37<16:49, 21.94s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 59%|█████▊    | 64/109 [1:26:11<19:17, 25.72s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 60%|█████▉    | 65/109 [1:26:32<17:47, 24.25s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 61%|██████    | 66/109 [1:26:54<16:50, 23.51s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 61%|██████▏   | 67/109 [1:27:39<21:01, 30.04s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 62%|██████▏   | 68/109 [1:28:07<20:11, 29.55s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 63%|██████▎   | 69/109 [1:28:19<16:08, 24.22s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 64%|██████▍   | 70/109 [1:28:24<11:56, 18.36s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 65%|██████▌   | 71/109 [1:28:41<11:29, 18.14s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 66%|██████▌   | 72/109 [1:29:19<14:48, 24.01s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
FILEPATH:  eval_result/codellama_baseline/2018-12081/critic/AlfredPros_CodeLlama-7b-Instruct-Solidity_critic_summarized.json
num tokens input:  1
FILEPATH:  eval_result/codellama_baseline/2018-12083/critic/AlfredPros_CodeLlama-7b-Instruct-Solidity_critic_summarized.json
num tokens input:  1
FILEPATH:  eval_result/codellama_baseline/2018-14087/critic/AlfredPros_CodeLlama-7b-Instruct-Solidity_critic_summarized.json
num tokens input:  1
FILEPATH:  eval_result/codellama_baseline/2018-14089/critic/AlfredPros_CodeLlama-7b-Instruct-Solidity_critic_summarized.json
num tokens input:  1
FILEPATH:  eval_result/codellama_baseline/2018-13113/critic/AlfredPros_CodeLlama-7b-Instruct-Solidity_critic_summarized.json
num tokens input:  1
FILEPATH:  eval_result/codellama_baseline/2018-13533/critic/AlfredPros_CodeLlama-7b-Instruct-Solidity_critic_summarized.json
num tokens input:  1
FILEPATH:  eval_result/codellama_baseline/2018-17111/critic/AlfredPros_CodeLlama-7b-Instruct-Solidity_critic_summarized.json
num tokens input:  1
FILEPATH:  eval_result/codellama_baseline/2018-10666/critic/AlfredPros_CodeLlama-7b-Instruct-Solidity_critic_summarized.json
num tokens input:  1
FILEPATH:  eval_result/codellama_baseline/2018-19833/critic/AlfredPros_CodeLlama-7b-Instruct-Solidity_critic_summarized.json
num tokens input:  1
FILEPATH:  eval_result/codellama_baseline/2018-11411/critic/AlfredPros_CodeLlama-7b-Instruct-Solidity_critic_summarized.json
num tokens input:  1
FILEPATH:  eval_result/codellama_baseline/2018-12082/critic/AlfredPros_CodeLlama-7b-Instruct-Solidity_critic_summarized.json
num tokens input:  1
FILEPATH:  eval_result/codellama_baseline/2018-12063/critic/AlfredPros_CodeLlama-7b-Instruct-Solidity_critic_summarized.json
num tokens input:  1
FILEPATH:  eval_result/codellama_baseline/2019-15079/critic/AlfredPros_CodeLlama-7b-Instruct-Solidity_critic_summarized.json
num tokens input:  1
FILEPATH:  eval_result/codellama_baseline/2018-12702/critic/AlfredPros_CodeLlama-7b-Instruct-Solidity_critic_summarized.json
num tokens input:  1
FILEPATH:  eval_result/codellama_baseline/2018-12511/critic/AlfredPros_CodeLlama-7b-Instruct-Solidity_critic_summarized.json
num tokens input:  1
FILEPATH:  eval_result/codellama_baseline/2020-17752/critic/AlfredPros_CodeLlama-7b-Instruct-Solidity_critic_summarized.json
num tokens input:  1
FILEPATH:  eval_result/codellama_baseline/2018-13070/critic/AlfredPros_CodeLlama-7b-Instruct-Solidity_critic_summarized.json
num tokens input:  1
FILEPATH:  eval_result/codellama_baseline/2018-13127/critic/AlfredPros_CodeLlama-7b-Instruct-Solidity_critic_summarized.json
num tokens input:  1
FILEPATH:  eval_result/codellama_baseline/2018-11335/critic/AlfredPros_CodeLlama-7b-Instruct-Solidity_critic_summarized.json
num tokens input:  1
FILEPATH:  eval_result/codellama_baseline/2018-14576/critic/AlfredPros_CodeLlama-7b-Instruct-Solidity_critic_summarized.json
num tokens input:  1
FILEPATH:  eval_result/codellama_baseline/2018-12025/critic/AlfredPros_CodeLlama-7b-Instruct-Solidity_critic_summarized.json
num tokens input:  1
FILEPATH:  eval_result/codellama_baseline/2018-17071/critic/AlfredPros_CodeLlama-7b-Instruct-Solidity_critic_summarized.json
num tokens input:  1
FILEPATH:  eval_result/codellama_baseline/2018-13221/critic/AlfredPros_CodeLlama-7b-Instruct-Solidity_critic_summarized.json
num tokens input:  1
FILEPATH:  eval_result/codellama_baseline/2019-15078/critic/AlfredPros_CodeLlama-7b-Instruct-Solidity_critic_summarized.json
num tokens input:  1
FILEPATH:  eval_result/codellama_baseline/2018-14063/critic/AlfredPros_CodeLlama-7b-Instruct-Solidity_critic_summarized.json
num tokens input:  1
FILEPATH:  eval_result/codellama_baseline/2018-13089/critic/AlfredPros_CodeLlama-7b-Instruct-Solidity_critic_summarized.json
num tokens input:  1
FILEPATH:  eval_result/codellama_baseline/2018-13087/critic/AlfredPros_CodeLlama-7b-Instruct-Solidity_critic_summarized.json
num tokens input:  1
FILEPATH:  eval_result/codellama_baseline/2018-10944/critic/AlfredPros_CodeLlama-7b-Instruct-Solidity_critic_summarized.json
num tokens input:  1
FILEPATH:  eval_result/codellama_baseline/2018-12959/critic/AlfredPros_CodeLlama-7b-Instruct-Solidity_critic_summarized.json
num tokens input:  1
FILEPATH:  eval_result/codellama_baseline/2018-13075/critic/AlfredPros_CodeLlama-7b-Instruct-Solidity_critic_summarized.json
num tokens input:  1
FILEPATH:  eval_result/codellama_baseline/2018-19834/critic/AlfredPros_CodeLlama-7b-Instruct-Solidity_critic_summarized.json
num tokens input:  1
FILEPATH:  eval_result/codellama_baseline/2018-13085/critic/AlfredPros_CodeLlama-7b-Instruct-Solidity_critic_summarized.json
num tokens input:  1
FILEPATH:  eval_result/codellama_baseline/2018-17050/critic/AlfredPros_CodeLlama-7b-Instruct-Solidity_critic_summarized.json
num tokens input:  1
FILEPATH:  eval_result/codellama_baseline/2018-13670/critic/AlfredPros_CodeLlama-7b-Instruct-Solidity_critic_summarized.json
num tokens input:  1
FILEPATH:  eval_result/codellama_baseline/2018-13069/critic/AlfredPros_CodeLlama-7b-Instruct-Solidity_critic_summarized.json
num tokens input:  1
FILEPATH:  eval_result/codellama_baseline/2018-13836/critic/AlfredPros_CodeLlama-7b-Instruct-Solidity_critic_summarized.json
num tokens input:  1
FILEPATH:  eval_result/codellama_baseline/2018-13132/critic/AlfredPros_CodeLlama-7b-Instruct-Solidity_critic_summarized.json
num tokens input:  1
FILEPATH:  eval_result/codellama_baseline/2018-13722/critic/AlfredPros_CodeLlama-7b-Instruct-Solidity_critic_summarized.json
num tokens input:  1
FILEPATH:  eval_result/codellama_baseline/2019-15080/critic/AlfredPros_CodeLlama-7b-Instruct-Solidity_critic_summarized.json
num tokens input:  1
FILEPATH:  eval_result/codellama_baseline/2018-13325/critic/AlfredPros_CodeLlama-7b-Instruct-Solidity_critic_summarized.json
num tokens input:  1
FILEPATH:  eval_result/codellama_baseline/2018-13202/critic/AlfredPros_CodeLlama-7b-Instruct-Solidity_critic_summarized.json
num tokens input:  1
FILEPATH:  eval_result/codellama_baseline/2018-11687/critic/AlfredPros_CodeLlama-7b-Instruct-Solidity_critic_summarized.json
num tokens input:  1
FILEPATH:  eval_result/codellama_baseline/2018-13227/critic/AlfredPros_CodeLlama-7b-Instruct-Solidity_critic_summarized.json
num tokens input:  1
FILEPATH:  eval_result/codellama_baseline/2018-14001/critic/AlfredPros_CodeLlama-7b-Instruct-Solidity_critic_summarized.json
num tokens input:  1
FILEPATH:  eval_result/codellama_baseline/2018-13091/critic/AlfredPros_CodeLlama-7b-Instruct-Solidity_critic_summarized.json
num tokens input:  1
FILEPATH:  eval_result/codellama_baseline/2018-13129/critic/AlfredPros_CodeLlama-7b-Instruct-Solidity_critic_summarized.json
num tokens input:  1
FILEPATH:  eval_result/codellama_baseline/2018-10706/critic/AlfredPros_CodeLlama-7b-Instruct-Solidity_critic_summarized.json
num tokens input:  1
FILEPATH:  eval_result/codellama_baseline/2021-34273/critic/AlfredPros_CodeLlama-7b-Instruct-Solidity_critic_summarized.json
num tokens input:  1
FILEPATH:  eval_result/codellama_baseline/2018-14004/critic/AlfredPros_CodeLlama-7b-Instruct-Solidity_critic_summarized.json
num tokens input:  1
FILEPATH:  eval_result/codellama_baseline/2020-17753/critic/AlfredPros_CodeLlama-7b-Instruct-Solidity_critic_summarized.json
num tokens input:  1
FILEPATH:  eval_result/codellama_baseline/2018-13327/critic/AlfredPros_CodeLlama-7b-Instruct-Solidity_critic_summarized.json
num tokens input:  1
FILEPATH:  eval_result/codellama_baseline/2018-11239/critic/AlfredPros_CodeLlama-7b-Instruct-Solidity_critic_summarized.json
num tokens input:  1
FILEPATH:  eval_result/codellama_baseline/2018-12454/critic/AlfredPros_CodeLlama-7b-Instruct-Solidity_critic_summarized.json
num tokens input:  1
FILEPATH:  eval_result/codellama_baseline/2018-13086/critic/AlfredPros_CodeLlama-7b-Instruct-Solidity_critic_summarized.json
num tokens input:  1
FILEPATH:  eval_result/codellama_baseline/2018-14715/critic/AlfredPros_CodeLlama-7b-Instruct-Solidity_critic_summarized.json
num tokens input:  1
FILEPATH:  eval_result/codellama_baseline/2021-3004/critic/AlfredPros_CodeLlama-7b-Instruct-Solidity_critic_summarized.json
num tokens input:  1
 67%|██████▋   | 73/109 [1:45:08<3:00:51, 301.44s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 68%|██████▊   | 74/109 [1:45:32<2:07:13, 218.09s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 69%|██████▉   | 75/109 [1:45:49<1:29:30, 157.94s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 70%|██████▉   | 76/109 [1:46:17<1:05:28, 119.05s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 71%|███████   | 77/109 [1:46:39<47:54, 89.83s/it]   The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 72%|███████▏  | 78/109 [1:47:09<37:04, 71.75s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 72%|███████▏  | 79/109 [1:47:22<27:03, 54.11s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 73%|███████▎  | 80/109 [1:47:37<20:29, 42.39s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 74%|███████▍  | 81/109 [1:48:01<17:14, 36.96s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 75%|███████▌  | 82/109 [1:48:08<12:36, 28.03s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 76%|███████▌  | 83/109 [1:48:17<09:40, 22.33s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 77%|███████▋  | 84/109 [1:48:36<08:49, 21.17s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 78%|███████▊  | 85/109 [1:48:40<06:26, 16.12s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 79%|███████▉  | 86/109 [1:49:10<07:46, 20.29s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 80%|███████▉  | 87/109 [1:49:14<05:41, 15.51s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 81%|████████  | 88/109 [1:49:30<05:26, 15.52s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 82%|████████▏ | 89/109 [1:49:45<05:07, 15.37s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 83%|████████▎ | 90/109 [1:50:00<04:50, 15.27s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 83%|████████▎ | 91/109 [1:50:28<05:46, 19.22s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 84%|████████▍ | 92/109 [1:50:38<04:39, 16.47s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 85%|████████▌ | 93/109 [1:50:59<04:42, 17.65s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 86%|████████▌ | 94/109 [1:51:14<04:15, 17.03s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 87%|████████▋ | 95/109 [1:51:47<05:01, 21.55s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 88%|████████▊ | 96/109 [1:52:21<05:29, 25.33s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 89%|████████▉ | 97/109 [1:52:37<04:30, 22.55s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 90%|████████▉ | 98/109 [1:52:48<03:29, 19.03s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 91%|█████████ | 99/109 [1:52:56<02:38, 15.82s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 92%|█████████▏| 100/109 [1:53:11<02:20, 15.56s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 93%|█████████▎| 101/109 [1:53:26<02:03, 15.45s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 94%|█████████▎| 102/109 [1:53:34<01:32, 13.17s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 94%|█████████▍| 103/109 [1:53:52<01:28, 14.69s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 95%|█████████▌| 104/109 [1:54:07<01:13, 14.73s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 96%|█████████▋| 105/109 [1:54:22<00:59, 14.81s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 97%|█████████▋| 106/109 [1:54:38<00:45, 15.13s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 98%|█████████▊| 107/109 [1:55:08<00:39, 19.69s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 99%|█████████▉| 108/109 [1:55:20<00:17, 17.40s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
100%|██████████| 109/109 [1:55:44<00:00, 19.29s/it]100%|██████████| 109/109 [1:55:44<00:00, 63.71s/it]
FILEPATH:  eval_result/codellama_baseline/2018-13128/critic/AlfredPros_CodeLlama-7b-Instruct-Solidity_critic_summarized.json
num tokens input:  1
FILEPATH:  eval_result/codellama_baseline/2018-14084/critic/AlfredPros_CodeLlama-7b-Instruct-Solidity_critic_summarized.json
num tokens input:  1
FILEPATH:  eval_result/codellama_baseline/2018-14006/critic/AlfredPros_CodeLlama-7b-Instruct-Solidity_critic_summarized.json
num tokens input:  1
FILEPATH:  eval_result/codellama_baseline/2018-12230/critic/AlfredPros_CodeLlama-7b-Instruct-Solidity_critic_summarized.json
num tokens input:  1
FILEPATH:  eval_result/codellama_baseline/2018-13777/critic/AlfredPros_CodeLlama-7b-Instruct-Solidity_critic_summarized.json
num tokens input:  1
FILEPATH:  eval_result/codellama_baseline/2018-14085/critic/AlfredPros_CodeLlama-7b-Instruct-Solidity_critic_summarized.json
num tokens input:  1
FILEPATH:  eval_result/codellama_baseline/2018-13228/critic/AlfredPros_CodeLlama-7b-Instruct-Solidity_critic_summarized.json
num tokens input:  1
FILEPATH:  eval_result/codellama_baseline/2018-13225/critic/AlfredPros_CodeLlama-7b-Instruct-Solidity_critic_summarized.json
num tokens input:  1
FILEPATH:  eval_result/codellama_baseline/2018-13189/critic/AlfredPros_CodeLlama-7b-Instruct-Solidity_critic_summarized.json
num tokens input:  1
FILEPATH:  eval_result/codellama_baseline/2018-13126/critic/AlfredPros_CodeLlama-7b-Instruct-Solidity_critic_summarized.json
num tokens input:  1
FILEPATH:  eval_result/codellama_baseline/2018-14005/critic/AlfredPros_CodeLlama-7b-Instruct-Solidity_critic_summarized.json
num tokens input:  1
FILEPATH:  eval_result/codellama_baseline/2021-3006/critic/AlfredPros_CodeLlama-7b-Instruct-Solidity_critic_summarized.json
num tokens input:  1
FILEPATH:  eval_result/codellama_baseline/2021-34270/critic/AlfredPros_CodeLlama-7b-Instruct-Solidity_critic_summarized.json
num tokens input:  1
FILEPATH:  eval_result/codellama_baseline/2020-35962/critic/AlfredPros_CodeLlama-7b-Instruct-Solidity_critic_summarized.json
num tokens input:  1
FILEPATH:  eval_result/codellama_baseline/2018-10299/critic/AlfredPros_CodeLlama-7b-Instruct-Solidity_critic_summarized.json
num tokens input:  1
FILEPATH:  eval_result/codellama_baseline/2018-12068/critic/AlfredPros_CodeLlama-7b-Instruct-Solidity_critic_summarized.json
num tokens input:  1
FILEPATH:  eval_result/codellama_baseline/2018-12067/critic/AlfredPros_CodeLlama-7b-Instruct-Solidity_critic_summarized.json
num tokens input:  1
FILEPATH:  eval_result/codellama_baseline/2018-13071/critic/AlfredPros_CodeLlama-7b-Instruct-Solidity_critic_summarized.json
num tokens input:  1
FILEPATH:  eval_result/codellama_baseline/2018-19830/critic/AlfredPros_CodeLlama-7b-Instruct-Solidity_critic_summarized.json
num tokens input:  1
FILEPATH:  eval_result/codellama_baseline/2018-13778/critic/AlfredPros_CodeLlama-7b-Instruct-Solidity_critic_summarized.json
num tokens input:  1
FILEPATH:  eval_result/codellama_baseline/2018-11429/critic/AlfredPros_CodeLlama-7b-Instruct-Solidity_critic_summarized.json
num tokens input:  1
FILEPATH:  eval_result/codellama_baseline/2018-18425/critic/AlfredPros_CodeLlama-7b-Instruct-Solidity_critic_summarized.json
num tokens input:  1
FILEPATH:  eval_result/codellama_baseline/2018-17968/critic/AlfredPros_CodeLlama-7b-Instruct-Solidity_critic_summarized.json
num tokens input:  1
FILEPATH:  eval_result/codellama_baseline/2018-13074/critic/AlfredPros_CodeLlama-7b-Instruct-Solidity_critic_summarized.json
num tokens input:  1
FILEPATH:  eval_result/codellama_baseline/2018-13092/critic/AlfredPros_CodeLlama-7b-Instruct-Solidity_critic_summarized.json
num tokens input:  1
FILEPATH:  eval_result/codellama_baseline/2018-12078/critic/AlfredPros_CodeLlama-7b-Instruct-Solidity_critic_summarized.json
num tokens input:  1
FILEPATH:  eval_result/codellama_baseline/2018-12062/critic/AlfredPros_CodeLlama-7b-Instruct-Solidity_critic_summarized.json
num tokens input:  1
FILEPATH:  eval_result/codellama_baseline/2018-13068/critic/AlfredPros_CodeLlama-7b-Instruct-Solidity_critic_summarized.json
num tokens input:  1
FILEPATH:  eval_result/codellama_baseline/2018-10376/critic/AlfredPros_CodeLlama-7b-Instruct-Solidity_critic_summarized.json
num tokens input:  1
FILEPATH:  eval_result/codellama_baseline/2018-13073/critic/AlfredPros_CodeLlama-7b-Instruct-Solidity_critic_summarized.json
num tokens input:  1
FILEPATH:  eval_result/codellama_baseline/2018-13783/critic/AlfredPros_CodeLlama-7b-Instruct-Solidity_critic_summarized.json
num tokens input:  1
FILEPATH:  eval_result/codellama_baseline/2018-13782/critic/AlfredPros_CodeLlama-7b-Instruct-Solidity_critic_summarized.json
num tokens input:  1
FILEPATH:  eval_result/codellama_baseline/2021-34272/critic/AlfredPros_CodeLlama-7b-Instruct-Solidity_critic_summarized.json
num tokens input:  1
FILEPATH:  eval_result/codellama_baseline/2018-12703/critic/AlfredPros_CodeLlama-7b-Instruct-Solidity_critic_summarized.json
num tokens input:  1
FILEPATH:  eval_result/codellama_baseline/2018-13076/critic/AlfredPros_CodeLlama-7b-Instruct-Solidity_critic_summarized.json
num tokens input:  1
FILEPATH:  eval_result/codellama_baseline/2018-19832/critic/AlfredPros_CodeLlama-7b-Instruct-Solidity_critic_summarized.json
num tokens input:  1
Unused kwargs: ['load_in_8bit_fp32_cpu_offload']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.
2024-11-22 22:31:02,201 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:43<00:43, 43.41s/it]Loading checkpoint shards: 100%|██████████| 2/2 [01:00<00:00, 35.44s/it]Loading checkpoint shards: 100%|██████████| 2/2 [01:00<00:00, 30.13s/it]
  0%|          | 0/109 [00:00<?, ?it/s]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
  1%|          | 1/109 [10:48<19:27:45, 648.75s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
  2%|▏         | 2/109 [12:36<14:27:33, 486.48s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
  3%|▎         | 3/109 [12:44<10:05:35, 342.79s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
  4%|▎         | 4/109 [14:04<7:42:03, 264.03s/it] The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
  5%|▍         | 5/109 [24:53<10:58:08, 379.69s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
  6%|▌         | 6/109 [25:27<7:53:27, 275.80s/it] The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
  6%|▋         | 7/109 [40:07<12:56:55, 457.02s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
  7%|▋         | 8/109 [50:55<14:26:08, 514.54s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
  8%|▊         | 9/109 [1:01:45<15:25:05, 555.05s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
  9%|▉         | 10/109 [1:12:32<16:01:12, 582.55s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 10%|█         | 11/109 [1:12:58<11:19:06, 415.78s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 11%|█         | 12/109 [1:23:45<13:04:17, 485.13s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 12%|█▏        | 13/109 [1:34:32<14:13:46, 533.60s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 13%|█▎        | 14/109 [1:34:59<10:04:26, 381.75s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 14%|█▍        | 15/109 [1:35:09<7:03:03, 270.04s/it] The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 15%|█▍        | 16/109 [1:45:56<9:53:57, 383.19s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 16%|█▌        | 17/109 [1:46:23<7:03:34, 276.25s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 17%|█▋        | 18/109 [1:57:06<9:46:05, 386.43s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 17%|█▋        | 19/109 [2:07:51<11:35:38, 463.77s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 18%|█▊        | 20/109 [2:08:17<8:13:24, 332.63s/it] The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 19%|█▉        | 21/109 [2:19:07<10:27:21, 427.74s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 20%|██        | 22/109 [2:20:04<7:38:58, 316.53s/it] The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 21%|██        | 23/109 [2:30:54<9:57:00, 416.52s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 22%|██▏       | 24/109 [2:31:15<7:02:06, 297.95s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 23%|██▎       | 25/109 [2:42:02<9:23:54, 402.79s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 24%|██▍       | 26/109 [2:42:13<6:34:34, 285.23s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 25%|██▍       | 27/109 [2:42:28<4:38:43, 203.94s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 26%|██▌       | 28/109 [2:42:50<3:21:54, 149.56s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 27%|██▋       | 29/109 [2:53:37<6:38:18, 298.73s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 28%|██▊       | 30/109 [3:04:23<8:50:23, 402.82s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 28%|██▊       | 31/109 [3:15:10<10:18:56, 476.10s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 29%|██▉       | 32/109 [3:25:57<11:16:54, 527.46s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 30%|███       | 33/109 [3:26:16<7:54:53, 374.91s/it] The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 31%|███       | 34/109 [3:27:03<5:45:31, 276.41s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 32%|███▏      | 35/109 [3:37:51<7:58:41, 388.12s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 33%|███▎      | 36/109 [3:38:25<5:42:42, 281.67s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 34%|███▍      | 37/109 [3:39:01<4:09:44, 208.12s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 35%|███▍      | 38/109 [3:39:57<3:12:12, 162.43s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 36%|███▌      | 39/109 [3:50:45<5:59:22, 308.03s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 37%|███▋      | 40/109 [4:05:23<9:10:49, 478.98s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 38%|███▊      | 41/109 [4:22:13<12:03:29, 638.38s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 39%|███▊      | 42/109 [4:24:16<9:00:03, 483.63s/it] The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 39%|███▉      | 43/109 [4:24:42<6:21:11, 346.53s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 40%|████      | 44/109 [4:25:09<4:31:27, 250.58s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 41%|████▏     | 45/109 [4:25:24<3:12:05, 180.08s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 42%|████▏     | 46/109 [4:36:09<5:35:28, 319.50s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 43%|████▎     | 47/109 [4:36:36<3:59:21, 231.64s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 44%|████▍     | 48/109 [4:47:23<6:02:08, 356.20s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 45%|████▍     | 49/109 [4:48:04<4:21:48, 261.81s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 46%|████▌     | 50/109 [4:48:22<3:05:18, 188.45s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 47%|████▋     | 51/109 [4:59:08<5:14:54, 325.77s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 48%|████▊     | 52/109 [4:59:34<3:44:13, 236.03s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 49%|████▊     | 53/109 [5:00:03<2:42:13, 173.82s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 50%|████▉     | 54/109 [5:10:47<4:48:36, 314.84s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 50%|█████     | 55/109 [5:21:31<6:12:20, 413.71s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 51%|█████▏    | 56/109 [5:21:38<4:17:40, 291.70s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 52%|█████▏    | 57/109 [5:22:15<3:06:27, 215.15s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 53%|█████▎    | 58/109 [5:22:51<2:17:16, 161.51s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 54%|█████▍    | 59/109 [5:33:36<4:15:25, 306.52s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 55%|█████▌    | 60/109 [5:44:21<5:33:08, 407.94s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 56%|█████▌    | 61/109 [5:45:02<3:58:13, 297.79s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 57%|█████▋    | 62/109 [5:45:39<2:52:09, 219.78s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 58%|█████▊    | 63/109 [5:46:12<2:05:33, 163.77s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 59%|█████▊    | 64/109 [5:48:19<1:54:27, 152.61s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 60%|█████▉    | 65/109 [5:48:53<1:25:47, 116.98s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 61%|██████    | 66/109 [5:51:58<1:38:33, 137.51s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 61%|██████▏   | 67/109 [5:52:57<1:19:40, 113.82s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 62%|██████▏   | 68/109 [5:54:29<1:13:24, 107.42s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 63%|██████▎   | 69/109 [5:54:58<55:56, 83.90s/it]   The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 64%|██████▍   | 70/109 [6:05:42<2:43:37, 251.73s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 65%|██████▌   | 71/109 [6:06:08<1:56:39, 184.20s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 66%|██████▌   | 72/109 [6:08:08<1:41:37, 164.79s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 67%|██████▋   | 73/109 [6:25:47<4:19:53, 433.16s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 68%|██████▊   | 74/109 [6:26:20<3:02:35, 313.01s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 69%|██████▉   | 75/109 [6:26:46<2:08:42, 227.12s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 70%|██████▉   | 76/109 [6:37:34<3:14:19, 353.31s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
all critic summary write to :  ['eval_result/codellama_baseline/2018-12080/critic_summary', 'eval_result/codellama_baseline/2018-13703/critic_summary', 'eval_result/codellama_baseline/2018-13083/critic_summary', 'eval_result/codellama_baseline/2018-12885/critic_summary', 'eval_result/codellama_baseline/2018-13079/critic_summary', 'eval_result/codellama_baseline/2018-14003/critic_summary', 'eval_result/codellama_baseline/2018-12975/critic_summary', 'eval_result/codellama_baseline/2018-13041/critic_summary', 'eval_result/codellama_baseline/2018-13625/critic_summary', 'eval_result/codellama_baseline/2018-13230/critic_summary', 'eval_result/codellama_baseline/2018-13088/critic_summary', 'eval_result/codellama_baseline/2018-13493/critic_summary', 'eval_result/codellama_baseline/2018-12079/critic_summary', 'eval_result/codellama_baseline/2018-17987/critic_summary', 'eval_result/codellama_baseline/2018-17877/critic_summary', 'eval_result/codellama_baseline/2018-13208/critic_summary', 'eval_result/codellama_baseline/2018-12070/critic_summary', 'eval_result/codellama_baseline/2018-12081/critic_summary', 'eval_result/codellama_baseline/2018-12083/critic_summary', 'eval_result/codellama_baseline/2018-14087/critic_summary', 'eval_result/codellama_baseline/2018-14089/critic_summary', 'eval_result/codellama_baseline/2018-13113/critic_summary', 'eval_result/codellama_baseline/2018-13533/critic_summary', 'eval_result/codellama_baseline/2018-17111/critic_summary', 'eval_result/codellama_baseline/2018-10666/critic_summary', 'eval_result/codellama_baseline/2018-19833/critic_summary', 'eval_result/codellama_baseline/2018-11411/critic_summary', 'eval_result/codellama_baseline/2018-12082/critic_summary', 'eval_result/codellama_baseline/2018-12063/critic_summary', 'eval_result/codellama_baseline/2019-15079/critic_summary', 'eval_result/codellama_baseline/2018-12702/critic_summary', 'eval_result/codellama_baseline/2018-12511/critic_summary', 'eval_result/codellama_baseline/2020-17752/critic_summary', 'eval_result/codellama_baseline/2018-13070/critic_summary', 'eval_result/codellama_baseline/2018-13127/critic_summary', 'eval_result/codellama_baseline/2018-11335/critic_summary', 'eval_result/codellama_baseline/2018-14576/critic_summary', 'eval_result/codellama_baseline/2018-12025/critic_summary', 'eval_result/codellama_baseline/2018-17071/critic_summary', 'eval_result/codellama_baseline/2018-13221/critic_summary', 'eval_result/codellama_baseline/2019-15078/critic_summary', 'eval_result/codellama_baseline/2018-14063/critic_summary', 'eval_result/codellama_baseline/2018-13089/critic_summary', 'eval_result/codellama_baseline/2018-13087/critic_summary', 'eval_result/codellama_baseline/2018-10944/critic_summary', 'eval_result/codellama_baseline/2018-12959/critic_summary', 'eval_result/codellama_baseline/2018-13075/critic_summary', 'eval_result/codellama_baseline/2018-19834/critic_summary', 'eval_result/codellama_baseline/2018-13085/critic_summary', 'eval_result/codellama_baseline/2018-17050/critic_summary', 'eval_result/codellama_baseline/2018-13670/critic_summary', 'eval_result/codellama_baseline/2018-13069/critic_summary', 'eval_result/codellama_baseline/2018-13836/critic_summary', 'eval_result/codellama_baseline/2018-13132/critic_summary', 'eval_result/codellama_baseline/2018-13722/critic_summary', 'eval_result/codellama_baseline/2019-15080/critic_summary', 'eval_result/codellama_baseline/2018-13325/critic_summary', 'eval_result/codellama_baseline/2018-13202/critic_summary', 'eval_result/codellama_baseline/2018-11687/critic_summary', 'eval_result/codellama_baseline/2018-13227/critic_summary', 'eval_result/codellama_baseline/2018-14001/critic_summary', 'eval_result/codellama_baseline/2018-13091/critic_summary', 'eval_result/codellama_baseline/2018-13129/critic_summary', 'eval_result/codellama_baseline/2018-10706/critic_summary', 'eval_result/codellama_baseline/2021-34273/critic_summary', 'eval_result/codellama_baseline/2018-14004/critic_summary', 'eval_result/codellama_baseline/2020-17753/critic_summary', 'eval_result/codellama_baseline/2018-13327/critic_summary', 'eval_result/codellama_baseline/2018-11239/critic_summary', 'eval_result/codellama_baseline/2018-12454/critic_summary', 'eval_result/codellama_baseline/2018-13086/critic_summary', 'eval_result/codellama_baseline/2018-14715/critic_summary', 'eval_result/codellama_baseline/2021-3004/critic_summary', 'eval_result/codellama_baseline/2018-13128/critic_summary', 'eval_result/codellama_baseline/2018-14084/critic_summary', 'eval_result/codellama_baseline/2018-14006/critic_summary', 'eval_result/codellama_baseline/2018-12230/critic_summary', 'eval_result/codellama_baseline/2018-13777/critic_summary', 'eval_result/codellama_baseline/2018-14085/critic_summary', 'eval_result/codellama_baseline/2018-13228/critic_summary', 'eval_result/codellama_baseline/2018-13225/critic_summary', 'eval_result/codellama_baseline/2018-13189/critic_summary', 'eval_result/codellama_baseline/2018-13126/critic_summary', 'eval_result/codellama_baseline/2018-14005/critic_summary', 'eval_result/codellama_baseline/2021-3006/critic_summary', 'eval_result/codellama_baseline/2021-34270/critic_summary', 'eval_result/codellama_baseline/2020-35962/critic_summary', 'eval_result/codellama_baseline/2018-10299/critic_summary', 'eval_result/codellama_baseline/2018-12068/critic_summary', 'eval_result/codellama_baseline/2018-12067/critic_summary', 'eval_result/codellama_baseline/2018-13071/critic_summary', 'eval_result/codellama_baseline/2018-19830/critic_summary', 'eval_result/codellama_baseline/2018-13778/critic_summary', 'eval_result/codellama_baseline/2018-11429/critic_summary', 'eval_result/codellama_baseline/2018-18425/critic_summary', 'eval_result/codellama_baseline/2018-17968/critic_summary', 'eval_result/codellama_baseline/2018-13074/critic_summary', 'eval_result/codellama_baseline/2018-13092/critic_summary', 'eval_result/codellama_baseline/2018-12078/critic_summary', 'eval_result/codellama_baseline/2018-12062/critic_summary', 'eval_result/codellama_baseline/2018-13068/critic_summary', 'eval_result/codellama_baseline/2018-10376/critic_summary', 'eval_result/codellama_baseline/2018-13073/critic_summary', 'eval_result/codellama_baseline/2018-13783/critic_summary', 'eval_result/codellama_baseline/2018-13782/critic_summary', 'eval_result/codellama_baseline/2021-34272/critic_summary', 'eval_result/codellama_baseline/2018-12703/critic_summary', 'eval_result/codellama_baseline/2018-13076/critic_summary', 'eval_result/codellama_baseline/2018-19832/critic_summary']
---- Running Ranker ----
num tokens input:  1
num tokens input:  1
num tokens input:  1
num tokens input:  1
num tokens input:  1
num tokens input:  1
num tokens input:  1
num tokens input:  1
num tokens input:  1
num tokens input:  1
num tokens input:  1
num tokens input:  1
num tokens input:  1
num tokens input:  1
num tokens input:  1
num tokens input:  1
num tokens input:  1
num tokens input:  1
num tokens input:  1
num tokens input:  1
num tokens input:  1
num tokens input:  1
num tokens input:  1
num tokens input:  1
num tokens input:  1
num tokens input:  1
num tokens input:  1
num tokens input:  1
num tokens input:  1
num tokens input:  1
num tokens input:  1
num tokens input:  1
num tokens input:  1
num tokens input:  1
num tokens input:  1
num tokens input:  1
num tokens input:  1
num tokens input:  1
num tokens input:  1
num tokens input:  1
num tokens input:  1
num tokens input:  1
num tokens input:  1
num tokens input:  1
num tokens input:  1
num tokens input:  1
num tokens input:  1
num tokens input:  1
num tokens input:  1
num tokens input:  1
num tokens input:  1
num tokens input:  1
num tokens input:  1
num tokens input:  1
num tokens input:  1
num tokens input:  1
num tokens input:  1
num tokens input:  1
num tokens input:  1
num tokens input:  1
num tokens input:  1
num tokens input:  1
num tokens input:  1
num tokens input:  1
num tokens input:  1
num tokens input:  1
num tokens input:  1
num tokens input:  1
num tokens input:  1
num tokens input:  1
num tokens input:  1
num tokens input:  1
num tokens input:  1
num tokens input:  1
num tokens input:  1
num tokens input:  1
num tokens input:  1
 71%|███████   | 77/109 [6:48:21<3:55:21, 441.28s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 72%|███████▏  | 78/109 [6:59:11<4:20:19, 503.85s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 72%|███████▏  | 79/109 [7:09:55<4:33:04, 546.13s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 73%|███████▎  | 80/109 [7:20:40<4:38:12, 575.62s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 74%|███████▍  | 81/109 [7:31:25<4:38:22, 596.51s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 75%|███████▌  | 82/109 [7:31:40<3:09:54, 422.00s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 76%|███████▌  | 83/109 [7:42:26<3:32:02, 489.35s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 77%|███████▋  | 84/109 [7:43:04<2:27:25, 353.80s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 78%|███████▊  | 85/109 [7:53:49<2:56:26, 441.12s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
 79%|███████▉  | 86/109 [8:04:39<3:13:07, 503.82s/it]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
slurmstepd: error: *** JOB 965238 ON atl1-1-03-013-3-0 CANCELLED AT 2024-11-23T06:37:56 DUE TO TIME LIMIT ***
srun: Job step aborted: Waiting up to 32 seconds for job step to finish.
slurmstepd: error: *** STEP 965238.0 ON atl1-1-03-013-3-0 CANCELLED AT 2024-11-23T06:37:56 DUE TO TIME LIMIT ***
---------------------------------------
Begin Slurm Epilog: Nov-23-2024 06:37:58
Job ID:        965238
Array Job ID:  _4294967294
User ID:       zyahn3
Account:       scs
Job name:      eval_bugscanner
Resources:     cpu=1,gres/gpu:h100=1,mem=80G,node=1
Rsrc Used:     cput=16:00:10,vmem=0,walltime=16:00:10,mem=6768K,energy_used=0
Partition:     ice-gpu
Nodes:         atl1-1-03-013-3-0
---------------------------------------
