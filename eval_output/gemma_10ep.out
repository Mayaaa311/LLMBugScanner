---------------------------------------
Begin Slurm Prolog: Nov-20-2024 10:23:50
Job ID:    958291
User ID:   zyahn3
Account:   scs
Job name:  eval_bugscanner
Partition: coc-gpu
---------------------------------------
/home/hice1/zyahn3/.conda/envs/BugScanner
2024-11-20 10:24:02,065 - INFO - Initializing auditor models...
2024-11-20 10:24:02,066 - INFO - Auditor models: finetune/model/gemma/gemma_messi_10ep/
2024-11-20 10:24:02,066 - INFO - Initializing critic model: finetune/model/gemma/gemma_messi_10ep/
2024-11-20 10:24:02,066 - INFO - Initializing ranker model: finetune/model/gemma/gemma_messi_10ep/
2024-11-20 10:24:02,066 - INFO - Initializing parser model: finetune/model/gemma/gemma_messi_10ep/
2024-11-20 10:24:02,066 - INFO - Initializing BugScanner with the given models...
2024-11-20 10:24:02,067 - INFO - BugScanner initialization completed.
2024-11-20 10:24:02,067 - INFO - Starting the bug scanning pipeline with Top-K = 5...
2024-11-20 10:24:02,068 - INFO - Found 136 .sol files in the folder: data_full/CVE_clean
2024-11-20 10:24:02,068 - INFO - Output will be saved in: eval_result/gemma_10ep
`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.
Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use
`config.hidden_activation` if you want to override this behaviour.
See https://github.com/huggingface/transformers/pull/29402 for more details.
using cuda
Attempted to get_model_instance for name finetune/model/gemma/gemma_messi_10ep/ and got model_class None
Attempted to get_model_instance for name finetune/model/gemma/gemma_messi_10ep/ and got model_class None
Attempted to get_model_instance for name finetune/model/gemma/gemma_messi_10ep/ and got model_class None
Attempted to get_model_instance for name finetune/model/gemma/gemma_messi_10ep/ and got model_class None
---- CHECK: model id is finetune/model/gemma/gemma_messi_10ep/ ----
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:07<00:21,  7.03s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:17<00:18,  9.20s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:28<00:09,  9.86s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:31<00:00,  7.37s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:31<00:00,  7.99s/it]
---- CHECK: model id is finetune/model/gemma/gemma_messi_10ep/ ----
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:12<00:38, 12.74s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:21<00:20, 10.41s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:29<00:09,  9.49s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:33<00:00,  7.18s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:33<00:00,  8.39s/it]
---- Running Auditor(s) ----
  0%|          | 0/136 [00:00<?, ?it/s]Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
This is a friendly reminder - the current text generation call will exceed the model's predefined maximum length (8192). Depending on the model, you may observe exceptions, performance degradation, or nothing at all.
srun: Job step aborted: Waiting up to 32 seconds for job step to finish.
slurmstepd: error: *** JOB 958291 ON atl1-1-01-002-7-0 CANCELLED AT 2024-11-20T13:13:13 ***
  1%|          | 1/136 [43:07<97:02:50, 2587.93s/it]  1%|▏         | 2/136 [1:26:08<96:09:41, 2583.44s/it]  2%|▏         | 3/136 [2:09:54<96:09:52, 2602.95s/it]slurmstepd: error: *** STEP 958291.0 ON atl1-1-01-002-7-0 CANCELLED AT 2024-11-20T13:13:13 ***
---------------------------------------
Begin Slurm Epilog: Nov-20-2024 13:13:17
Job ID:        958291
Array Job ID:  _4294967294
User ID:       zyahn3
Account:       scs
Job name:      eval_bugscanner
Resources:     cpu=1,gres/gpu:v100=4,mem=128G,node=1
Rsrc Used:     cput=02:49:23,vmem=0,walltime=02:49:23,mem=6548K,energy_used=0
Partition:     coc-gpu
Nodes:         atl1-1-01-002-7-0
---------------------------------------
